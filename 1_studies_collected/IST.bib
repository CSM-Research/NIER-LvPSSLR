@article{SHAHROKNI20131,
title = {A systematic review of software robustness},
journal = {Information and Software Technology},
volume = {55},
number = {1},
pages = {1-17},
year = {2013},
note = {Special section: Best papers from the 2nd International Symposium on Search Based Software Engineering 2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001048},
author = {Ali Shahrokni and Robert Feldt},
keywords = {Systematic review, Robustness, Software robustness},
abstract = {Context
With the increased use of software for running key functions in modern society it is of utmost importance to understand software robustness and how to support it. Although there have been many contributions to the field there is a lack of a coherent and summary view.
Objective
To address this issue, we have conducted a literature review in the field of robustness.
Method
This review has been conducted by following guidelines for systematic literature reviews. Systematic reviews are used to find and classify all existing and available literature in a certain field.
Results
From 9193 initial papers found in three well-known research databases, the 144 relevant papers were extracted through a multi-step filtering process with independent validation in each step. These papers were then further analyzed and categorized based on their development phase, domain, research, contribution and evaluation type. The results indicate that most existing results on software robustness focus on verification and validation of Commercial of the shelf (COTS) or operating systems or propose design solutions for robustness while there is a lack of results on how to elicit and specify robustness requirements. The research is typically solution proposals with little to no evaluation and when there is some evaluation it is primarily done with small, toy/academic example systems.
Conclusion
We conclude that there is a need for more software robustness research on real-world, industrial systems and on software development phases other than testing and design, in particular on requirements engineering.}
}
@article{DIKICI2018112,
title = {Factors influencing the understandability of process models: A systematic literature review},
journal = {Information and Software Technology},
volume = {93},
pages = {112-129},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916302889},
author = {Ahmet Dikici and Oktay Turetken and Onur Demirors},
keywords = {Business process model, Understandability, Comprehension, Process model understandability, Systematic literature review},
abstract = {Context
Process models are key in facilitating communication in organizations and in designing process-aware information systems. Organizations are facing increasingly larger and more complex processes, which pose difficulties to the understandability of process models. The literature reports several factors that are considered to influence the understandability of process models. However, these studies typically focus on testing of a limited set of factors. A work that collects, abstracts and synthesizes an in-depth summary of the current literature will help in developing the research in this field.
Objective
We conducted a systematic literature review (SLR) focusing on the empirical studies in the existing literature in order to better understand the state of the research on process model understandability, and identify the gaps and opportunities for future research.
Method
We searched the studies between the years 1995 and 2015 in established electronic libraries. Out of 1066 publications retrieved initially, we selected 45 publications for thorough analysis. We identified, analyzed and categorized factors that are considered to influence the understandability of process models as studied in the literature using empirical methods. We also analyzed the indicators that are used to quantify process model understandability.
Results
Our analysis identifies several gaps in the field, as well as issues of inconsistent findings regarding the effect of some factors, unbalanced emphasis on certain indicators, and methodological concerns.
Conclusions
The existing research calls for comprehensive empirical studies to contribute to a better understanding of the factors of process model understandability. Our study is a comprehensive source for researchers working on the understandability of process models and related fields, and a useful guide for practitioners aiming to generate understandable process models.}
}
@article{VANDINTER2022107008,
title = {Predictive maintenance using digital twins: A systematic literature review},
journal = {Information and Software Technology},
volume = {151},
pages = {107008},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001331},
author = {Raymon {van Dinter} and Bedir Tekinerdogan and Cagatay Catal},
keywords = {Systematic literature review, Active learning, Digital twin, Predictive maintenance},
abstract = {Context
Predictive maintenance is a technique for creating a more sustainable, safe, and profitable industry. One of the key challenges for creating predictive maintenance systems is the lack of failure data, as the machine is frequently repaired before failure. Digital Twins provide a real-time representation of the physical machine and generate data, such as asset degradation, which the predictive maintenance algorithm can use. Since 2018, scientific literature on the utilization of Digital Twins for predictive maintenance has accelerated, indicating the need for a thorough review.
Objective
This research aims to gather and synthesize the studies that focus on predictive maintenance using Digital Twins to pave the way for further research.
Method
A systematic literature review (SLR) using an active learning tool is conducted on published primary studies on predictive maintenance using Digital Twins, in which 42 primary studies have been analyzed.
Results
This SLR identifies several aspects of predictive maintenance using Digital Twins, including the objectives, application domains, Digital Twin platforms, Digital Twin representation types, approaches, abstraction levels, design patterns, communication protocols, twinning parameters, and challenges and solution directions. These results contribute to a Software Engineering approach for developing predictive maintenance using Digital Twins in academics and the industry.
Conclusion
This study is the first SLR in predictive maintenance using Digital Twins. We answer key questions for designing a successful predictive maintenance model leveraging Digital Twins. We found that to this day, computational burden, data variety, and complexity of models, assets, or components are the key challenges in designing these models.}
}
@article{MENDES201950,
title = {The relationship between personality and decision-making: A Systematic literature review},
journal = {Information and Software Technology},
volume = {111},
pages = {50-71},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300576},
author = {Fabiana Freitas Mendes and Emilia Mendes and Norsaremah Salleh},
keywords = {Decision-Making, Personality, Service or product development},
abstract = {Context
From a point of view, software development is a set of decisions that need to be made while the software is developed. Many alternatives should be considered, such as the technology to employ, or the most important features to implement. However, many factors can influence one’s decision-making, such as the decision maker’s personality.
Objective
This paper reports the state of the art with regard to the relationship between decision-makers’ personality and decision-making aspects.
Method
We conducted a Systematic Literature Review to search and analyze published primary studies that discuss the abovementioned relationship in the context of companies that develop any kind of product or service.
Results
Despite the recognized influence of personality in decision-making activities, we were not able to find any study in Software Engineering field that discusses this relationship. We included 15 studies and most of them are from Management field, excluding one from Information System field. From these studies, we identified 75 reported relationships between 28 different personality aspects and 30 different decision-making aspects.
Conclusion
The interest in this topic born on 80’s and it has grown after 2002. However, despite the number of reported relationships, and the number of personalities and decision-making aspects investigated, more research on this topic is necessary. In particular, it is important to verify how someone’s personality influences the decision-making considering the software development context. This can help in improving how a decision is made in software engineering context.}
}
@article{RATTAN20131165,
title = {Software clone detection: A systematic review},
journal = {Information and Software Technology},
volume = {55},
number = {7},
pages = {1165-1199},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913000323},
author = {Dhavleesh Rattan and Rajesh Bhatia and Maninder Singh},
keywords = {Software clone, Clone detection, Systematic literature review, Semantic clones, Model based clone},
abstract = {Context
Reusing software by means of copy and paste is a frequent activity in software development. The duplicated code is known as a software clone and the activity is known as code cloning. Software clones may lead to bug propagation and serious maintenance problems.
Objective
This study reports an extensive systematic literature review of software clones in general and software clone detection in particular.
Method
We used the standard systematic literature review method based on a comprehensive set of 213 articles from a total of 2039 articles published in 11 leading journals and 37 premier conferences and workshops.
Results
Existing literature about software clones is classified broadly into different categories. The importance of semantic clone detection and model based clone detection led to different classifications. Empirical evaluation of clone detection tools/techniques is presented. Clone management, its benefits and cross cutting nature is reported. Number of studies pertaining to nine different types of clones is reported. Thirteen intermediate representations and 24 match detection techniques are reported.
Conclusion
We call for an increased awareness of the potential benefits of software clone management, and identify the need to develop semantic and model clone detection techniques. Recommendations are given for future research.}
}
@article{UZUN201830,
title = {Model-driven architecture based testing: A systematic literature review},
journal = {Information and Software Technology},
volume = {102},
pages = {30-48},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918300880},
author = {Burak Uzun and Bedir Tekinerdogan},
keywords = {Model-based testing, Software architecture, Systematic review},
abstract = {Context
Model-driven architecture based testing (MDABT) adopts architectural models of a system under test and/or its environment to derive test artifacts. In the literature, different MDABT approaches have been provided together with the corresponding lessons results and lessons learned.
Objective
The overall objective of this paper is to identify the published concerns for applying MDABT, identify the proposed solutions, and describe the current research directions for MDABT.
Method
To this end we have provided a systematic literature review (SLR) that is conducted by a multi-phase study selection process using the published literature in major software engineering journals and conference proceedings.
Results
We reviewed 739 papers that are discovered using a well-planned review protocol, and 31 of them were assessed as primary studies related to our research questions. Based on the analysis of the data extraction process, we discuss the primary trends and approaches and present the identified obstacles.
Conclusion
This study shows that although a generic process the approaches different in various ways with different goals, modeling abstractions and results. Further, based on the synthesis process in the SLR we can state that the potential of MDABT has not been fully exploited yet.}
}
@article{BANO2015148,
title = {A systematic review on the relationship between user involvement and system success},
journal = {Information and Software Technology},
volume = {58},
pages = {148-169},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001505},
author = {Muneera Bano and Didar Zowghi},
keywords = {User involvement, Software development, Systematic Literature Review},
abstract = {Context
For more than four decades it has been intuitively accepted that user involvement (UI) during system development lifecycle leads to system success. However when the researchers have evaluated the user involvement and system success (UI-SS) relationship empirically, the results were not always positive.
Objective
Our objective was to explore the UI-SS relationship by synthesizing the results of all the studies that have empirically investigated this complex phenomenon.
Method
We performed a Systematic Literature Review (SLR) following the steps provided in the guidelines of Evidence Based Software Engineering. From the resulting studies we extracted data to answer our 9 research questions related to the UI-SS relationship, identification of users, perspectives of UI, benefits, problems and challenges of UI, degree and level of UI, relevance of stages of software development lifecycle (SDLC) and the research method employed on the UI-SS relationship.
Results
Our systematic review resulted in selecting 87 empirical studies published during the period 1980–2012. Among 87 studies reviewed, 52 reported that UI positively contributes to system success, 12 suggested a negative contribution and 23 were uncertain. The UI-SS relationship is neither direct nor binary, and there are various confounding factors that play their role. The identification of users, their degree/level of involvement, stage of SDLC for UI, and choice of research method have been claimed to have impact on the UI-SS relationship. However, there is not sufficient empirical evidence available to support these claims.
Conclusion
Our results have revealed that UI does contribute positively to system success. But it is a double edged sword and if not managed carefully it may cause more problems than benefits. Based on the analysis of 87 studies, we were able to identify factors for effective management of UI alluding to the causes for inconsistency in the results of published literature.}
}
@article{KITCHENHAM20097,
title = {Systematic literature reviews in software engineering – A systematic literature review},
journal = {Information and Software Technology},
volume = {51},
number = {1},
pages = {7-15},
year = {2009},
note = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2008.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584908001390},
author = {Barbara Kitchenham and O. {Pearl Brereton} and David Budgen and Mark Turner and John Bailey and Stephen Linkman},
keywords = {Systematic literature review, Evidence-based software engineering, Tertiary study, Systematic review quality, Cost estimation},
abstract = {Background
In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference.
Aims
This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence.
Method
We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings.
Results
Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4.
Conclusions
Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners.}
}
@article{LUCAS20091631,
title = {A systematic review of UML model consistency management},
journal = {Information and Software Technology},
volume = {51},
number = {12},
pages = {1631-1645},
year = {2009},
note = {Quality of UML Models},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909000433},
author = {Francisco J. Lucas and Fernando Molina and Ambrosio Toval},
keywords = {UML, Model consistency, Systematic literature review},
abstract = {Information System (IS) development has been beset by consistency problems since its infancy. These problems are greater still in UML software development, and are principally caused by the existence of multiple views (models) for the same system, and may involve potentially contradictory system specifications. Since a considerable amount of work takes place within the scope of model consistency management, this paper presents a systematic literature review (SLR) which was carried out to discover the various current model consistency conceptions, proposals, problems and solutions provided. To do this, a total of 907 papers related to UML model consistency published in literature and extracted from the most relevant scientific sources (IEEE Computer Society, ACM Digital Library, Google Scholar, ScienceDirect, and the SCOPUS Database) were considered, of which 42 papers were eventually analyzed. This systematic literature review resulted in the identification of the current state-of-the-art with regard to UML model consistency management research along with open issues, trends and future research within this scope. A formal approach for the handling of inconsistency problems which fulfils the identified limitations is also briefly presented.}
}
@article{KASOJU20131237,
title = {Analyzing an automotive testing process with evidence-based software engineering},
journal = {Information and Software Technology},
volume = {55},
number = {7},
pages = {1237-1259},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913000165},
author = {Abhinaya Kasoju and Kai Petersen and Mika V. Mäntylä},
keywords = {Evidence-based software engineering, Process assessment, Automotive software testing},
abstract = {Context
Evidence-based software engineering (EBSE) provides a process for solving practical problems based on a rigorous research approach. The primary focus so far was on mapping and aggregating evidence through systematic reviews.
Objectives
We extend existing work on evidence-based software engineering by using the EBSE process in an industrial case to help an organization to improve its automotive testing process. With this we contribute in (1) providing experiences on using evidence based processes to analyze a real world automotive test process and (2) provide evidence of challenges and related solutions for automotive software testing processes.
Methods
In this study we perform an in-depth investigation of an automotive test process using an extended EBSE process including case study research (gain an understanding of practical questions to define a research scope), systematic literature review (identify solutions through systematic literature), and value stream mapping (map out an improved automotive test process based on the current situation and improvement suggestions identified). These are followed by reflections on the EBSE process used.
Results
In the first step of the EBSE process we identified 10 challenge areas with a total of 26 individual challenges. For 15 out of those 26 challenges our domain specific systematic literature review identified solutions. Based on the input from the challenges and the solutions, we created a value stream map of the current and future process.
Conclusions
Overall, we found that the evidence-based process as presented in this study helps in technology transfer of research results to industry, but at the same time some challenges lie ahead (e.g. scoping systematic reviews to focus more on concrete industry problems, and understanding strategies of conducting EBSE with respect to effort and quality of the evidence).}
}
@article{ANU2018112,
title = {Development of a human error taxonomy for software requirements: A systematic literature review},
journal = {Information and Software Technology},
volume = {103},
pages = {112-124},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916302373},
author = {Vaibhav Anu and Wenhua Hu and Jeffrey C Carver and Gursimran S Walia and Gary Bradshaw},
keywords = {Systematic review, Requirements, Human errors, Taxonomy},
abstract = {Background
Human-centric software engineering activities, such as requirements engineering, are prone to error. These human errors manifest as faults. To improve software quality, developers need methods to prevent and detect faults and their sources.
Aims
Human error research from the field of cognitive psychology focuses on understanding and categorizing the fallibilities of human cognition. In this paper, we applied concepts from human error research to the problem of software quality.
Method
We performed a systematic literature review of the software engineering and psychology literature to identify and classify human errors that occur during requirements engineering.
Results
We developed the Human Error Taxonomy (HET) by adding detailed error classes to Reason's well-known human error taxonomy of Slips, Lapses, and Mistakes.
Conclusion
The process of identifying and classifying human error identification provides a structured way to understand and prevent the human errors (and resulting faults) that occur during human-centric software engineering activities like requirements engineering. Software engineering can benefit from closer collaboration with cognitive psychology researchers.}
}
@article{BEHUTIYE2017139,
title = {Analyzing the concept of technical debt in the context of agile software development: A systematic literature review},
journal = {Information and Software Technology},
volume = {82},
pages = {139-158},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916302890},
author = {Woubshet Nema Behutiye and Pilar Rodríguez and Markku Oivo and Ayşe Tosun},
keywords = {Technical debt, Agile software development, Technical debt management, Systematic literature review},
abstract = {Context
Technical debt (TD) is a metaphor that is used to communicate the consequences of poor software development practices to non-technical stakeholders. In recent years, it has gained significant attention in agile software development (ASD).
Objective
The purpose of this study is to analyze and synthesize the state of the art of TD, and its causes, consequences, and management strategies in the context of ASD.
Research Method
Using a systematic literature review (SLR), 38 primary studies, out of 346 studies, were identified and analyzed.
Results
We found five research areas of interest related to the literature of TD in ASD. Among those areas, “managing TD in ASD” received the highest attention, followed by “architecture in ASD and its relationship with TD”. In addition, eight categories regarding the causes and five categories regarding the consequences of incurring TD in ASD were identified. “Focus on quick delivery” and “architectural and design issues” were the most popular causes of incurring TD in ASD. “Reduced productivity”, “system degradation” and “increased maintenance cost” were identified as significant consequences of incurring TD in ASD. Additionally, we found 12 strategies for managing TD in the context of ASD, out of which “refactoring” and “enhancing the visibility of TD” were the most significant.
Conclusion
The results of this study provide a structured synthesis of TD and its management in the context of ASD as well as potential research areas for further investigation.}
}
@article{SANCHEZGORDON201923,
title = {Taking the emotional pulse of software engineering — A systematic literature review of empirical studies},
journal = {Information and Software Technology},
volume = {115},
pages = {23-43},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301661},
author = {Mary Sánchez-Gordón and Ricardo Colomo-Palacios},
keywords = {Systematic literature review, Social aspects of software development, Emotion, Affect, Mood, Behavioral software engineering},
abstract = {Context
Over the past 50 years of Software Engineering, numerous studies have acknowledged the importance of human factors. However, software developers’ emotions are still an area under investigation and debate that is gaining relevance in the software industry.
Objective
In this study, a systematic literature review (SLR) was carried out to identify, evaluate, and synthesize research published concerning software developers’ emotions as well as the measures used to assess its existence.
Method
By searching five major bibliographic databases, authors identified 7172 articles related to emotions in Software Engineering. We selected 66 of these papers as primary studies. Then, they were analyzed in order to find empirical evidence of the intersection of emotions and software engineering.
Results
Studies report a total of 40 discrete emotions but the most frequent were: anger, fear, disgust, sadness, joy, love, and happiness. There are also 2 different dimensional approaches and 10 datasets related to this topic which are publicly available on the Web. The findings also showed that self-reported mood instruments (e.g., SAM, PANAS), physiological measures (e.g., heart rate, perspiration) or behavioral measures (e.g., keyboard use) are the least reported tools, although, there is a recognized intrinsic problem with the accuracy of current state of the art sentiment analysis tools. Moreover, most of the studies used software practitioners and/or datasets from industrial context as subjects.
Conclusions
The study of emotions has received a growing attention from the research community in the recent years, but the management of emotions has always been challenging in practice. Although it can be said that this field is not mature enough yet, our results provide a holistic view that will benefit researchers by providing the latest trends in this area and identifying the corresponding research gaps.}
}
@article{ZAKARI2020106312,
title = {Multiple fault localization of software programs: A systematic literature review},
journal = {Information and Software Technology},
volume = {124},
pages = {106312},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106312},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300641},
author = {Abubakar Zakari and Sai Peck Lee and Rui Abreu and Babiker Hussien Ahmed and Rasheed Abubakar Rasheed},
keywords = {Program debugging, Parallel debugging, Fault localization, Multiple faults, One-bug-at-a-time (OBA)},
abstract = {Context
Multiple fault localization (MFL) is the act of identifying the locations of multiple faults (more than one fault) in a faulty software program. This is known to be more complicated, tedious, and costly in comparison to the traditional practice of presuming that a software contains a single fault. Due to the increasing interest in MFL by the research community, a broad spectrum of MFL debugging approaches and solutions have been proposed and developed.
Objective
The aim of this study is to systematically review existing research on MFL in the software fault localization (SFL) domain. This study also aims to identify, categorize, and synthesize relevant studies in the research domain.
Method
Consequently, using an evidence-based systematic methodology, we identified 55 studies relevant to four research questions. The methodology provides a systematic selection and evaluation process with rigorous and repeatable evidence-based studies selection process.
Result
The result of the systematic review shows that research on MFL is gaining momentum with stable growth in the last 5 years. Three prominent MFL debugging approaches were identified, i.e. One-bug-at-a-time debugging approach (OBA), parallel debugging approach, and multiple-bug-at-a-time debugging approach (MBA), with OBA debugging approach being utilized the most.
Conclusion
The study concludes with some identified research challenges and suggestions for future research. Although MFL is becoming of grave concern, existing solutions in the field are less mature. Studies utilizing real faults in their experiments are scarce. Concrete solutions to reduce MFL debugging time and cost by adopting an approach such as MBA debugging approach are also less, which require more attention from the research community.}
}
@article{YANG2021106397,
title = {Quality Assessment in Systematic Literature Reviews: A Software Engineering Perspective},
journal = {Information and Software Technology},
volume = {130},
pages = {106397},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106397},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301610},
author = {Lanxin Yang and He Zhang and Haifeng Shen and Xin Huang and Xin Zhou and Guoping Rong and Dong Shao},
keywords = {Quality assessment, Systematic (literature) review, Tertiary study, Empirical software engineering, Evidence-based software engineering},
abstract = {Context: Quality Assessment (QA) of reviewed literature is paramount to a Systematic Literature Review (SLR) as the quality of conclusions completely depends on the quality of selected literature. A number of researchers in Software Engineering (SE) have developed a variety of QA instruments and also reported their challenges. We previously conducted a tertiary study on SLRs with QA from 2004 to 2013, and reported the findings in 2015. Objective: With the widespread use of SLRs in SE and the increasing adoption of QA in these SLRs in recent years, it is necessary to empirically investigate whether the previous conclusions are still valid and whether there are new insights to the subject in question using a larger and a more up-to-date SLR set. More importantly, we aim to depict a clear picture of QA used in SLRs in SE by aggregating and distilling good practices, including the commonly used QA instruments as well as the major roles and aspects of QA in research. Method: An extended tertiary study was conducted with the newly collected SLRs from 2014 to 2018 and the original SLRs from 2004 to 2013 to systematically review the QA used by SLRs in SE during the 15-year period from 2004 to 2018. In addition, this extended study also compared and contrasted the findings of the previous study conducted in 2015. Results: A total of 241 SLRs between 2004 and 2018 were included, from which we identified a number of QA instruments. These instruments are generally designed to focus on the rationality of study design, the rigor of study execution and analysis, and the credibility and contribution of study findings and conclusions, with the emphasis largely placed on its rigor. The quality data is mainly used for literature selection or as evidence to support conclusions. Conclusions: QA has received much attention in SE in more recent years and the improvement is evident since the last study in 2015. New findings show that the aims are more concise, the instruments are more diverse and rigorous, and the criteria are more thoughtful.}
}
@article{TURNER2010463,
title = {Does the technology acceptance model predict actual use? A systematic literature review},
journal = {Information and Software Technology},
volume = {52},
number = {5},
pages = {463-479},
year = {2010},
note = {TAIC-PART 2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909002055},
author = {Mark Turner and Barbara Kitchenham and Pearl Brereton and Stuart Charters and David Budgen},
keywords = {Technology acceptance model (TAM), Systematic literature review, Evidence-based software engineering, Literature review, Actual usage},
abstract = {Context
The technology acceptance model (TAM) was proposed in 1989 as a means of predicting technology usage. However, it is usually validated by using a measure of behavioural intention to use (BI) rather than actual usage.
Objective
This review examines the evidence that the TAM predicts actual usage using both subjective and objective measures of actual usage.
Method
We performed a systematic literature review based on a search of six digital libraries, along with vote-counting meta-analysis to analyse the overall results.
Results
The search identified 79 relevant empirical studies in 73 articles. The results show that BI is likely to be correlated with actual usage. However, the TAM variables perceived ease of use (PEU) and perceived usefulness (PU) are less likely to be correlated with actual usage.
Conclusion
Care should be taken using the TAM outside the context in which it has been validated.}
}
@article{AHMAD2018130,
title = {Perspectives on usability guidelines for smartphone applications: An empirical investigation and systematic literature review},
journal = {Information and Software Technology},
volume = {94},
pages = {130-149},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301665},
author = {Naveed Ahmad and Aimal Rextin and Um E Kulsoom},
keywords = {Usability, Guidelines, Smartphones, Platform, Genre, Apps},
abstract = {Context
Several usability guidelines have been proposed to improve the usability of smartphone apps. These guidelines can be classified into three disjoint sets: platform specific guidelines, genre specific guidelines, and generic guidelines. However, smartphone applications are usually developed for multiple platforms and targeted for a variety of users. Hence the usefulness of existing guidelines is severally limited.
Objective
This study aims to develop a comprehensive list of usability guidelines suitable for multiple platforms and genres of smartphone applications.
Method
A controlled experiment was conducted, and it highlighted that even popular and established apps have usability problems. In order to identify different perspectives on usability a systematic literature review was conducted.
Results
Systematic literature review resulted in 148 studies that proposed a total of 359 usability guidelines. These guidelines were condensed into 25 guidelines in 7 categories by removing redundancy, repetition and similarity through a sequential and iterative process. Finally, usefulness of the proposed classification of guidelines is established by mapping these to usability issues identified earlier.}
}
@article{LEE2020106272,
title = {Test coverage criteria for software product line testing: Systematic literature review},
journal = {Information and Software Technology},
volume = {122},
pages = {106272},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106272},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300227},
author = {Jihyun Lee and Sungwon Kang and Pilsu Jung},
keywords = {Software product line, Software product line testing, Test coverage, Test coverage criteria},
abstract = {Context
In software product line testing (SPLT), test coverage criterion is an important concept, as it provides a means of measuring the extent to which domain testing has been performed and redundant application testing can be avoided based on the test coverage level achieved in domain testing. However, no previous literature reviews on SPLT have addressed test coverage criterion in SPLT.
Objective
The objectives of this paper are as follows: (1) to clarify the notions of test basis and test coverage criterion for SPLT; (2) to identify the test coverage criteria currently used for SPLT; (3) to investigate how various SPLT aspects, such as the SPLT method, variability implementation mechanism, and variability management approach, affect the choice of test coverage criterion for SPLT; and (4) to analyze the limitations of test coverage criteria currently used for SPLT.
Method
This paper conducts a systematic review of test coverage criteria in SPLT with 78 selected studies.
Results
We have several findings that can guide the future research on SPLT. One important finding is that choice of test coverage criterion in SPLT is independent from variability implementation mechanism, variability management, SPL approach, and binding time but is dependent on the variability representation used in development artifacts. Another that is easily overlooked is that SPL test coverage criteria with the same test coverage criterion names of single system testing neither adequately convey what should be covered by the test methods applying them, nor can they be more generally regarded as extensions or generalizations for SPLT of their corresponding test coverage criteria of single system testing.
Conclusion
This study showed that SPL test coverage criteria should be defined or redefined so that they can clearly deliver the target properties to be satisfied by SPLT.}
}
@article{HOLL2012828,
title = {A systematic review and an expert survey on capabilities supporting multi product lines},
journal = {Information and Software Technology},
volume = {54},
number = {8},
pages = {828-852},
year = {2012},
note = {Special Issue: Voice of the Editorial Board},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S095058491200033X},
author = {Gerald Holl and Paul Grünbacher and Rick Rabiser},
keywords = {Product line engineering, Large-scale systems, Multi product lines, Systematic literature review},
abstract = {Context
Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented.
Objective
The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs.
Method
Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry.
Results
The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized.
Conclusions
We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.}
}
@article{NEPOMUCENO201940,
title = {On the need to update systematic literature reviews},
journal = {Information and Software Technology},
volume = {109},
pages = {40-42},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300072},
author = {Vilmar Nepomuceno and Sergio Soares},
keywords = {Systematic literature review, Systematic mapping, Tertiary studies, Updates, Evidence based software engineering},
abstract = {Context
Many Systematic Literature Reviews (SLRs) were performed in the recent past, but just a few are being updated. Keeping SLRs updated is essential to prolong their lifespan.
Objective
To give a picture about how SLRs are being updated and what researchers think about SLRs updates.
Method
In this work, we present a Systematic Mapping (SM) study about SLRs updates and a survey with EBSE researchers that published their SLRs between 2011 and 2015.
Results
We included 22 studies in the SM, where 15 changed some artifact from the original study, including changes in research questions. We obtained 28 answers in our survey with SLRs authors that, in general, consolidate interpretations retrieved from the SM, but some answers did not.
Conclusion
SLRs may lose their impact over the years. Identifying actions to keep them updated is of great importance to SLR research field.}
}
@article{FELIZARDO20121079,
title = {A visual analysis approach to validate the selection review of primary studies in systematic reviews},
journal = {Information and Software Technology},
volume = {54},
number = {10},
pages = {1079-1091},
year = {2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912000742},
author = {Katia R. Felizardo and Gabriel F. Andery and Fernando V. Paulovich and Rosane Minghim and José C. Maldonado},
keywords = {Systematic Literature Review (SLR), Visual Text Mining (VTM), Information visualization, Content document map, Citation document map},
abstract = {Context
Systematic Literature Reviews (SLRs) are an important component to identify and aggregate research evidence from different empirical studies. Despite its relevance, most of the process is conducted manually, implying additional effort when the Selection Review task is performed and leading to reading all studies under analysis more than once.
Objective
We propose an approach based on Visual Text Mining (VTM) techniques to assist the Selection Review task in SLR. It is implemented into a VTM tool (Revis), which is freely available for use.
Method
We have selected and implemented appropriate visualization techniques into our approach and validated and demonstrated its usefulness in performing real SLRs.
Results
The results have shown that employment of VTM techniques can successfully assist in the Selection Review task, speeding up the entire SLR process in comparison to the conventional approach.
Conclusion
VTM techniques are valuable tools to be used in the context of selecting studies in the SLR process, prone to speed up some stages of SLRs.}
}
@article{VIDONI2022106791,
title = {A systematic process for Mining Software Repositories: Results from a systematic literature review},
journal = {Information and Software Technology},
volume = {144},
pages = {106791},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106791},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921002317},
author = {M. Vidoni},
keywords = {Mining Software Repositories, Systematic literature review, Evidence-based software engineering, Guidelines},
abstract = {Context:
Mining Software Repositories (MSR) is a growing area of Software Engineering (SE) research. Since their emergence in 2004, many investigations have analysed different aspects of these studies. However, there are no guidelines on how to conduct systematic MSR studies. There is a need to evaluate how MSR research is approached to provide a framework to do so systematically.
Objective:
To identify how MSR studies are conducted in terms of repository selection and data extraction. To uncover potential for improvement in directing systematic research and providing guidelines to do so.
Method:
A systematic literature review of MSR studies was conducted following the guidelines and template proposed by Mian et al. (which refines those provided by Kitchenham and Charters). These guidelines were extended and revised to provide a framework for systematic MSR studies.
Results:
MSR studies typically do not follow a systematic approach for repository selection, and many do not report selection or data extraction protocols. Furthermore, few manuscripts discuss threats to the study’s validity due to the selection or data extraction steps followed.
Conclusions:
Although MSR studies are evidence-based research, they seldom follow a systematic process. Hence, there is a need for guidelines on how to conduct systematic MSR studies. New guidelines and a template have been proposed, consolidating related studies in the MSR field and strategies for systematic literature reviews.}
}
@article{WOHLIN2020106366,
title = {Guidelines for the search strategy to update systematic literature reviews in software engineering},
journal = {Information and Software Technology},
volume = {127},
pages = {106366},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106366},
url = {https://www.sciencedirect.com/science/article/pii/S095058491930223X},
author = {Claes Wohlin and Emilia Mendes and Katia Romero Felizardo and Marcos Kalinowski},
keywords = {Systematic literature review update, Systematic literature reviews, Software engineering, Snowballing, Searching for evidence},
abstract = {Context
Systematic Literature Reviews (SLRs) have been adopted within Software Engineering (SE) for more than a decade to provide meaningful summaries of evidence on several topics. Many of these SLRs are now potentially not fully up-to-date, and there are no standard proposals on how to update SLRs in SE.
Objective
The objective of this paper is to propose guidelines on how to best search for evidence when updating SLRs in SE, and to evaluate these guidelines using an SLR that was not employed during the formulation of the guidelines.
Method
To propose our guidelines, we compare and discuss outcomes from applying different search strategies to identify primary studies in a published SLR, an SLR update, and two replications in the area of effort estimation. These guidelines are then evaluated using an SLR in the area of software ecosystems, its update and a replication.
Results
The use of a single iteration forward snowballing with Google Scholar, and employing as a seed set the original SLR and its primary studies is the most cost-effective way to search for new evidence when updating SLRs. Furthermore, the importance of having more than one researcher involved in the selection of papers when applying the inclusion and exclusion criteria is highlighted through the results.
Conclusions
Our proposed guidelines formulated based upon an effort estimation SLR, its update and two replications, were supported when using an SLR in the area of software ecosystems, its update and a replication. Therefore, we put forward that our guidelines ought to be adopted for updating SLRs in SE.}
}
@article{HENRICHS2022106940,
title = {A literature review on optimization techniques for adaptation planning in adaptive systems: State of the art and research directions},
journal = {Information and Software Technology},
volume = {149},
pages = {106940},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106940},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000891},
author = {Elia Henrichs and Veronika Lesch and Martin Straesser and Samuel Kounev and Christian Krupitzer},
keywords = {Self-adaptive systems, Adaptation planning, Optimization, Survey},
abstract = {Context:
Recent developments in modern IT systems including internet of things, edge/fog computing, or cyber–physical systems support intelligent and seamless interaction between users and systems. This requires a reaction to changes in their environment or the system. Adaptive systems provide mechanisms for these reactions.
Objective:
To implement this functionality, several approaches for the planning of adaptations exist that rely on rules, utility functions, or advanced techniques, such as machine learning. As the adaptation space with possible options is often extensively huge, optimization techniques might support efficient determination of the adaptation space and identify the system’s optimal configuration. With this paper, we provide a systematic review of adaptation planning as the optimization target.
Method:
In this paper, we review which optimization techniques are applied for adaptation planning in adaptive systems using a systematic literature review approach.
Results:
We reviewed 115 paper in detail out of an initial search set of 9,588 papers. Our analysis reveals that learning techniques and genetic algorithms are by far dominant; in total, heuristics (anytime learning) are more frequently applied as exact algorithms. We observed that around 57% of the approaches target multi-objectiveness and around 30% integrate distributed optimization. As last dimension, we focused on situation-awareness, which is only supported by two approaches.
Conclusion:
In this paper, we provide an overview of the current state of the art of approaches that rely on optimization techniques for planning adaptations in adaptive systems and further derive open research challenges, in particular regarding the integration of distributed optimization and situation-awareness.}
}
@article{NOGUEIRATEIXEIRA2019106175,
title = {Software process line as an approach to support software process reuse: A systematic literature review},
journal = {Information and Software Technology},
volume = {116},
pages = {106175},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301715},
author = {Eldânae {Nogueira Teixeira} and Fellipe Araújo Aleixo and Francisco Dione de Sousa Amâncio and Edson OliveiraJr and Uirá Kulesza and Cláudia Werner},
keywords = {Systematic review, Software process, Process reuse, Software process line, Process variability management},
abstract = {Context
Software Process Line (SPrL) aims at providing a systematic reuse technique to support reuse experiences and knowledge in the definition of software processes for new projects thus contributing to reduce effort and costs and to achieve improvements in quality. Although the research body in SPrL is expanding, it is still an immature area with results offering an overall view scattered with no consensus.
Objective
The goal of this work is to identify existing approaches for developing, using, managing and visualizing the evolution of SPrLs and to characterize their support, especially during the development of reusable process family artefacts, including an overview of existing SPrL supporting tools in their multiple stages; to analyse variability management and component-based aspects in SPrL; and, finally, to list practical examples and conducted evaluations. This research aims at reaching a broader and more consistent view of the research area and to provide perspectives and gaps for future research.
Method
We performed a systematic literature review according to well-established guidelines set. We used tools to partially support the process, which relies on a six-member research team.
Results
We report on 49 primary studies that deal mostly with conceptual or theoretical proposals and the domain engineering stage. Years 2014, 2015, and 2018 yielded the largest number of articles. This can indicate SPrL as a recent research theme and one that attracts ever-increasing interest.
Conclusion
Although this research area is growing, there is still a lack of practical experiences and approaches for actual applications or project-specific process derivations and decision-making support. The concept of an integrated reuse infrastructure is less discussed and explored; and the development of integrated tools to support all reuse stages is not fully addressed. Other topics for future research are discussed throughout the paper with gaps pointed as opportunities for improvements in the area.}
}
@article{RABISER2010324,
title = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
journal = {Information and Software Technology},
volume = {52},
number = {3},
pages = {324-346},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909001931},
author = {Rick Rabiser and Paul Grünbacher and Deepak Dhungana},
keywords = {Product derivation, Software product line, Product line engineering, Systematic literature review},
abstract = {Context
An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support.
Objective
Our aim is to identify and validate requirements for tool-supported product derivation.
Method
We identify the requirements through a systematic literature review and validate them with an expert survey.
Results
We discuss the resulting requirements and provide implementation examples from existing product derivation approaches.
Conclusions
We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.}
}
@article{ZHANG2020106296,
title = {Testing and verification of neural-network-based safety-critical control software: A systematic literature review},
journal = {Information and Software Technology},
volume = {123},
pages = {106296},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106296},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300471},
author = {Jin Zhang and Jingyue Li},
keywords = {Software testing and verification, Neural network, Safety-critical control software, Systematic literature review},
abstract = {Context: Neural Network (NN) algorithms have been successfully adopted in a number of Safety-Critical Cyber-Physical Systems (SCCPSs). Testing and Verification (T&V) of NN-based control software in safety-critical domains are gaining interest and attention from both software engineering and safety engineering researchers and practitioners. Objective: With the increase in studies on the T&V of NN-based control software in safety-critical domains, it is important to systematically review the state-of-the-art T&V methodologies, to classify approaches and tools that are invented, and to identify challenges and gaps for future studies. Method: By searching the six most relevant digital libraries, we retrieved 950 papers on the T&V of NN-based Safety-Critical Control Software (SCCS). Then we filtered the papers based on the predefined inclusion and exclusion criteria and applied snowballing to identify new relevant papers. Results: To reach our result, we selected 83 primary papers published between 2011 and 2018, applied the thematic analysis approach for analyzing the data extracted from the selected papers, presented the classification of approaches, and identified challenges. Conclusion: The approaches were categorized into five high-order themes, namely, assuring robustness of NNs, improving the failure resilience of NNs, measuring and ensuring test completeness, assuring safety properties of NN-based control software, and improving the interpretability of NNs. From the industry perspective, improving the interpretability of NNs is a crucial need in safety-critical applications. We also investigated nine safety integrity properties within four major safety lifecycle phases to investigate the achievement level of T&V goals in IEC 61508-3. Results show that correctness, completeness, freedom from intrinsic faults, and fault tolerance have drawn most attention from the research community. However, little effort has been invested in achieving repeatability, and no reviewed study focused on precisely defined testing configuration or defense against common cause failure.}
}
@article{MARQUES2019190,
title = {Software product line evolution: A systematic literature review},
journal = {Information and Software Technology},
volume = {105},
pages = {190-208},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301848},
author = {Maíra Marques and Jocelyn Simmonds and Pedro O. Rossel and María Cecilia Bastarrica},
keywords = {Evolution, Software reuse, Software product line, Systematic literature review},
abstract = {Context: Software Product Lines (SPL) evolve when there are changes in the requirements, product structure or the technology being used. Different approaches have been proposed for managing SPL assets and some also address how evolution affects these assets. Existing mapping studies have focused on specific aspects of SPL evolution, but there is no cohesive body of work that gives an overview of the area as a whole. Objective: The goals of this work are to review the characteristics of the approaches reported as supporting SPL evolution, and to synthesize the evidence provided by primary studies about the nature of their processes, as well as how they are reported and validated. Method: We conducted a systematic literature review, considering six research questions formulated to evaluate evolution approaches for SPL. We considered journal, conference and workshop papers published up until March 2017 in leading digital libraries for computer science. Results: After a thorough analysis of the papers retrieved from the digital libraries, we ended up with a set of 60 primary studies. Feature models are widely used to represent SPLs, so feature evolution is frequently addressed. Other assets are less frequently addressed. The area has matured over time: papers presenting more rigorous work are becoming more common. The processes used to support SPL evolution are systematic, but with a low level of automation. Conclusions: Our research shows that there is no consensus about SPL formalization, what assets can evolve, nor how and when these evolve. Case studies are quite popular, but few industrial-sized case studies are publicly available. Also, few of the proposed techniques offer tool support. We believe that the SPL community needs to work together to improve the state of the art, creating methods and tools that support SPL evolution in a more comparable manner.}
}
@article{DISSANAYAKE2022106771,
title = {Software security patch management - A systematic literature review of challenges, approaches, tools and practices},
journal = {Information and Software Technology},
volume = {144},
pages = {106771},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106771},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921002147},
author = {Nesara Dissanayake and Asangi Jayatilaka and Mansooreh Zahedi and M. Ali Babar},
keywords = {Security patch management, Vulnerability management, Systematic literature review},
abstract = {Context:
Software security patch management purports to support the process of patching known software security vulnerabilities. Patching security vulnerabilities in large and complex systems is a hugely challenging process that involves multiple stakeholders making several interdependent technological and socio-technical decisions. Given the increasing recognition of the importance of software security patch management, it is important and timely to systematically review and synthesise the relevant literature on this topic.
Objective:
This paper aims at systematically reviewing the state of the art of software security patch management to identify the socio-technical challenges in this regard, reported solutions (i.e., approaches, tools, and practices), the rigour of the evaluation and the industrial relevance of the reported solutions, and to identify the gaps for future research.
Method:
We conducted a systematic literature review of 72 studies published from 2002 to March 2020, with extended coverage until September 2020 through forward snowballing.
Results:
We identify 14 socio-technical challenges in software security patch management, 18 solution approaches, tools and practices mapped onto the software security patch management process. We provide a mapping between the solutions and challenges to enable a reader to obtain a holistic overview of the gap areas. The findings also reveal that only 20.8% of the reported solutions have been rigorously evaluated in industrial settings.
Conclusion:
Our results reveal that 50% of the common challenges have not been directly addressed in the solutions and that most of them (38.9%) address the challenges in one phase of the process, namely vulnerability scanning, assessment and prioritisation. Based on the results that highlight the important concerns in software security patch management and the lack of solutions, we recommend a list of future research directions. This study also provides useful insights about different opportunities for practitioners to adopt new solutions and understand the variations of their practical utility.}
}
@article{WONG2022106934,
title = {Self-adaptive systems: A systematic literature review across categories and domains},
journal = {Information and Software Technology},
volume = {148},
pages = {106934},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106934},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000854},
author = {Terence Wong and Markus Wagner and Christoph Treude},
keywords = {Self-adaptive systems, Literature review},
abstract = {Context:
Championed by IBM’s vision of autonomic computing paper in 2003, the autonomic computing research field has seen increased research activity over the last 20 years. Several conferences (SEAMS, SASO, ICAC) and workshops (SISSY) have been established and have contributed to the autonomic computing knowledge base in search of a new kind of system — a self-adaptive system (SAS). These systems are characterized by being context-aware and can act on that awareness. The actions carried out could be on the system or on the context (or environment). The underlying goal of a SAS is the sustained achievement of its goals despite changes in its environment.
Objective:
Despite a number of literature reviews on specific aspects of SASs ranging from their requirements to quality attributes, we lack a systematic understanding of the current state of the art.
Method:
This paper contributes a systematic literature review into self-adaptive systems using the dblp computer science bibliography as a database. We filtered the records systematically in successive steps to arrive at 293 relevant papers. Each paper was critically analyzed and categorized into an attribute matrix. This matrix consisted of five categories, with each category having multiple attributes. The attributes of each paper, along with the summary of its contents formed the basis of the literature review that spanned 30 years (1990–2020).
Results:
We characterize the maturation process of the research area from theoretical papers over practical implementations to more holistic and generic approaches, frameworks, and exemplars, applied to areas such as networking, web services, and robotics, with much of the recent work focusing on IoT and IaaS.
Conclusion:
While there is an ebb and flow of application domains, domains like bio-inspired approaches, security, and cyber–physical systems showed promise to grow heading into the 2020s.}
}
@article{HINDERKS2022106957,
title = {Approaches to manage the user experience process in Agile software development: A systematic literature review},
journal = {Information and Software Technology},
volume = {150},
pages = {106957},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106957},
url = {https://www.sciencedirect.com/science/article/pii/S095058492200101X},
author = {Andreas Hinderks and Francisco José {Domínguez Mayo} and Jörg Thomaschewski and María José Escalona},
keywords = {User experience management, UX process, User experience, UX, Usability, HCI, Agile methods, Agile, Systematic literature review},
abstract = {Context:
Software development companies use Agile methods to develop their products or services efficiently and in a goal-oriented way. But this alone is not enough to satisfy user demands today. It is much more important nowadays that a product or service should offer a great user experience — the user wants to have some positive user experience while interacting with the product or service.
Objective:
An essential requirement is the integration of user experience methods in Agile software development. Based on this, the development of positive user experience must be managed. We understand management in general as a combination of a goal, a strategy, and resources. When applied to UX, user experience management consists of a UX goal, a UX strategy, and UX resources.
Method:
We have conducted a systematic literature review (SLR) to analyse suitable approaches for managing user experience in the context of Agile software development.
Results:
We have identified 49 relevant studies in this regard. After analysing the studies in detail, we have identified different primary approaches that can be deemed suitable for UX management. Additionally, we have identified several UX methods that are used in combination with the primary approaches.
Conclusions:
However, we could not identify any approaches that directly address UX management. There is also no general definition or common understanding of UX management. To successfully implement UX management, it is important to know what UX management actually is and how to measure or determine successful UX management.}
}
@article{DASILVA2011899,
title = {Six years of systematic literature reviews in software engineering: An updated tertiary study},
journal = {Information and Software Technology},
volume = {53},
number = {9},
pages = {899-913},
year = {2011},
note = {Studying work practices in Global Software Engineering},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911001017},
author = {Fabio Q.B. {da Silva} and André L.M. Santos and Sérgio Soares and A. César C. França and Cleviton V.F. Monteiro and Felipe Farias Maciel},
keywords = {Systematic reviews, Mapping studies, Software engineering, Tertiary studies},
abstract = {Context
Since the introduction of evidence-based software engineering in 2004, systematic literature review (SLR) has been increasingly used as a method for conducting secondary studies in software engineering. Two tertiary studies, published in 2009 and 2010, identified and analysed 54 SLRs published in journals and conferences in the period between 1st January 2004 and 30th June 2008.
Objective
In this article, our goal was to extend and update the two previous tertiary studies to cover the period between 1st July 2008 and 31st December 2009. We analysed the quality, coverage of software engineering topics, and potential impact of published SLRs for education and practice.
Method
We performed automatic and manual searches for SLRs published in journals and conference proceedings, analysed the relevant studies, and compared and integrated our findings with the two previous tertiary studies.
Results
We found 67 new SLRs addressing 24 software engineering topics. Among these studies, 15 were considered relevant to the undergraduate educational curriculum, and 40 appeared of possible interest to practitioners. We found that the number of SLRs in software engineering is increasing, the overall quality of the studies is improving, and the number of researchers and research organisations worldwide that are conducting SLRs is also increasing and spreading.
Conclusion
Our findings suggest that the software engineering research community is starting to adopt SLRs consistently as a research method. However, the majority of the SLRs did not evaluate the quality of primary studies and fail to provide guidelines for practitioners, thus decreasing their potential impact on software engineering practice.}
}
@article{ALI2018133,
title = {Reliability of search in systematic reviews: Towards a quality assessment framework for the automated-search strategy},
journal = {Information and Software Technology},
volume = {99},
pages = {133-147},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304263},
author = {Nauman Bin Ali and Muhammad Usman},
keywords = {Secondary studies, Systematic literature reviews, Search strategies, Reliability, Credibility, Guidelines},
abstract = {Context
The trust in systematic literature reviews (SLRs) to provide credible recommendations is critical for establishing evidence-based software engineering (EBSE) practice. The reliability of SLR as a method is not a given and largely depends on the rigor of the attempt to identify, appraise and aggregate evidence. Previous research, by comparing SLRs on the same topic, has identified search as one of the reasons for discrepancies in the included primary studies. This affects the reliability of an SLR, as the papers identified and included in it are likely to influence its conclusions.
Objective
We aim to propose a comprehensive evaluation checklist to assess the reliability of an automated-search strategy used in an SLR.
Method
Using a literature review, we identified guidelines for designing and reporting automated-search as a primary search strategy. Using the aggregated design, reporting and evaluation guidelines, we formulated a comprehensive evaluation checklist. The value of this checklist was demonstrated by assessing the reliability of search in 27 recent SLRs.
Results
Using the proposed evaluation checklist, several additional issues (not captured by the current evaluation checklist) related to the reliability of search in recent SLRs were identified. These issues severely limit the coverage of literature by the search and also the possibility to replicate it.
Conclusion
Instead of solely relying on expensive replications to assess the reliability of SLRs, this work provides means to objectively assess the likely reliability of a search-strategy used in an SLR. It highlights the often-assumed aspect of repeatability of search when using automated-search. Furthermore, by explicitly considering repeatability and consistency as sub-characteristics of a reliable search, it provides a more comprehensive evaluation checklist than the ones currently used in EBSE.}
}
@article{NICOLAS20091291,
title = {On the generation of requirements specifications from software engineering models: A systematic literature review},
journal = {Information and Software Technology},
volume = {51},
number = {9},
pages = {1291-1307},
year = {2009},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909000378},
author = {Joaquín Nicolás and Ambrosio Toval},
keywords = {Specification generation from software engineering model, Textual requirements generation from software engineering model, Requirements document generation from software engineering model, Systematic literature review},
abstract = {System and software requirements documents play a crucial role in software engineering in that they must both communicate requirements to clients in an understandable manner and define requirements in precise detail for system developers. The benefits of both lists of textual requirements (usually written in natural language) and software engineering models (usually specified in graphical form) can be brought together by combining the two approaches in the specification of system and software requirements documents. If, moreover, textual requirements are generated from models in an automatic or closely monitored form, the effort of specifying those requirements is reduced and the completeness of the specification and the management of the requirements traceability are improved. This paper presents a systematic review of the literature related to the generation of textual requirements specifications from software engineering models.}
}
@article{WATANABE2020106395,
title = {Reducing efforts of software engineering systematic literature reviews updates using text classification},
journal = {Information and Software Technology},
volume = {128},
pages = {106395},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106395},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301592},
author = {Willian Massami Watanabe and Katia Romero Felizardo and Arnaldo Candido and Érica Ferreira {de Souza} and José Ede de Campos Neto and Nandamudi Lankalapalli Vijaykumar},
keywords = {Systematic literature review, SLR, Automatic selection, Review update, Text classification, Document classification, Text categorization},
abstract = {Context
Systematic Literature Reviews (SLRs) are frequently used to synthesize evidence in Software Engineering (SE), however replicating and keeping SLRs up-to-date is a major challenge. The activity of studies selection in SLR is labor intensive due to the large number of studies that must be analyzed. Different approaches have been investigated to support SLR processes, such as: Visual Text Mining or Text Classification. But acquiring the initial dataset is time-consuming and labor intensive.
Objective
In this work, we proposed and evaluated the use of Text Classification to support the studies selection activity of new evidences to update SLRs in SE.
Method
We applied Text Classification techniques to investigate how effective and how much effort could be spared during the studies selection phase of an SLR update. Considering the SLRs update scenario, the studies analyzed in the primary SLR could be used as a classified dataset to train Supervised Machine Learning algorithms. We conducted an experiment with 8 Software Engineering SLRs. In the experiments, we investigated the use of multiple preprocessing and feature extraction tasks such as tokenization, stop words removal, word lemmatization, TF-IDF (Term-Frequency/Inverse-Document-Frequency) with Decision Tree and Support Vector Machines as classification algorithms. Furthermore, we configured the classifier activation threshold for maximizing Recall, hence reducing the number of Missed selected studies.
Results
The techniques accuracies were measured and the results achieved on average a F-Score of 0.92 and 62% of exclusion rate when varying the activation threshold of the classifiers, with a 4% average number of Missed selected studies. Both the Exclusion rate and number of Missed selected studies were significantly different when compared to classifier which did not use the configuration of the activation threshold.
Conclusion
The results showed the potential of the techniques in reducing the effort required of SLRs updates.}
}
@article{ELMASRI2020106276,
title = {A systematic literature review on automated log abstraction techniques},
journal = {Information and Software Technology},
volume = {122},
pages = {106276},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106276},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300264},
author = {Diana El-Masri and Fabio Petrillo and Yann-Gaël Guéhéneuc and Abdelwahab Hamou-Lhadj and Anas Bouziane},
keywords = {Log Abstraction Techniques, Log Analysis, Log Mining, Log Parsing, Software Analysis, Software Log, Systematic literature review, Systematic survey, AIOps, Data mining, Log Management, Quality Model},
abstract = {Context: Logs are often the first and only information available to software engineers to understand and debug their systems. Automated log-analysis techniques help software engineers gain insights into large log data. These techniques have several steps, among which log abstraction is the most important because it transforms raw log-data into high-level information. Thus, log abstraction allows software engineers to perform further analyses. Existing log-abstraction techniques vary significantly in their designs and performances. To the best of our knowledge, there is no study that examines the performances of these techniques with respect to the following seven quality aspects concurrently: mode, coverage, delimiter independence, efficiency,scalability, system knowledge independence, and parameter tuning effort. Objectives: We want (1) to build a quality model for evaluating automated log-abstraction techniques and (2) to evaluate and recommend existing automated log-abstraction techniques using this quality model. Method: We perform a systematic literature review (SLR) of automated log-abstraction techniques. We review 89 research papers out of 2,864 initial papers. Results: Through this SLR, we (1) identify 17 automated log-abstraction techniques, (2) build a quality model composed of seven desirable aspects: mode, coverage, delimiter independence, efficiency, scalability, system knowledge independence, and parameter tuning effort, and (3) make recommendations for researchers on future research directions. Conclusion: Our quality model and recommendations help researchers learn about the state-of-the-art automated log-abstraction techniques, identify research gaps to enhance existing techniques, and develop new ones. We also support software engineers in understanding the advantages and limitations of existing techniques and in choosing the suitable technique to their unique use cases.}
}
@article{LI2021106449,
title = {Understanding and addressing quality attributes of microservices architecture: A Systematic literature review},
journal = {Information and Software Technology},
volume = {131},
pages = {106449},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106449},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301993},
author = {Shanshan Li and He Zhang and Zijia Jia and Chenxing Zhong and Cheng Zhang and Zhihao Shan and Jinfeng Shen and Muhammad Ali Babar},
keywords = {Microservices, Monolith, Quality attributes, Systematic literature review},
abstract = {Context: As a rapidly adopted architectural style in software engineering, Microservices Architecture (MSA) advocates implementing small-scale and independently distributed services, rather than binding all functions into one monolith. Although many initiatives have contributed to the quality improvement of microservices-based systems, there is still a lack of a systematic understanding of the Quality Attributes (QAs) associated with MSA. Objective: This study aims to investigate the evidence-based state-of-the-art of QAs of microservices-based systems. Method: We carried out a Systematic Literature Review (SLR) to identify and synthesize the relevant studies that report evidence related to QAs of MSA. Results: Based on the data extracted from the 72 selected primary studies, we portray an overview of the six identified QAs most concerned in MSA, scalability, performance, availability, monitorability, security, and testability. We identify 19 tactics that architecturally address the critical QAs in MSA, including two tactics for scalability, four for performance, four for availability, four for monitorability, three for security, and two for testability. Conclusion: This SLR concludes that for MSA-based systems: 1) Although scalability is the commonly acknowledged benefit of MSA, it is still an indispensable concern among the identified QAs, especially when trading-off with other QAs, e.g., performance. Apart from the six identified QAs in this study, other QAs for MSA like maintainability need more attention for effective improvement and evaluation in the future. 3) Practitioners need to carefully make the decision of migrating to MSA based on the return on investment, since this architectural style additionally cause some pains in practice.}
}
@article{RADJENOVIC20131397,
title = {Software fault prediction metrics: A systematic literature review},
journal = {Information and Software Technology},
volume = {55},
number = {8},
pages = {1397-1418},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913000426},
author = {Danijel Radjenović and Marjan Heričko and Richard Torkar and Aleš Živkovič},
keywords = {Software metric, Software fault prediction, Systematic literature review},
abstract = {Context
Software metrics may be used in fault prediction models to improve software quality by predicting fault location.
Objective
This paper aims to identify software metrics and to assess their applicability in software fault prediction. We investigated the influence of context on metrics’ selection and performance.
Method
This systematic literature review includes 106 papers published between 1991 and 2011. The selected papers are classified according to metrics and context properties.
Results
Object-oriented metrics (49%) were used nearly twice as often compared to traditional source code metrics (27%) or process metrics (24%). Chidamber and Kemerer’s (CK) object-oriented metrics were most frequently used. According to the selected studies there are significant differences between the metrics used in fault prediction performance. Object-oriented and process metrics have been reported to be more successful in finding faults compared to traditional size and complexity metrics. Process metrics seem to be better at predicting post-release faults compared to any static code metrics.
Conclusion
More studies should be performed on large industrial software systems to find metrics more relevant for the industry and to answer the question as to which metrics should be used in a given context.}
}
@article{GAROUSI2016106,
title = {Challenges and best practices in industry-academia collaborations in software engineering: A systematic literature review},
journal = {Information and Software Technology},
volume = {79},
pages = {106-127},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301203},
author = {Vahid Garousi and Kai Petersen and Baris Ozkan},
keywords = {Software engineering, Industry-academia collaborations, Industry, Universities, Challenges, Success patterns, Best practices, Systematic literature review},
abstract = {Context: The global software industry and the software engineering (SE) academia are two large communities. However, unfortunately, the level of joint industry-academia collaborations in SE is still relatively very low, compared to the amount of activity in each of the two communities. It seems that the two ’camps’ show only limited interest/motivation to collaborate with one other. Many researchers and practitioners have written about the challenges, success patterns (what to do, i.e., how to collaborate) and anti-patterns (what not do do) for industry-academia collaborations. Objective: To identify (a) the challenges to avoid risks to the collaboration by being aware of the challenges, (b) the best practices to provide an inventory of practices (patterns) allowing for an informed choice of practices to use when planning and conducting collaborative projects. Method: A systematic review has been conducted. Synthesis has been done using grounded-theory based coding procedures. Results: Through thematic analysis we identified 10 challenge themes and 17 best practice themes. A key outcome was the inventory of best practices, the most common ones recommended in different contexts were to hold regular workshops and seminars with industry, assure continuous learning from industry and academic sides, ensure management engagement, the need for a champion, basing research on real-world problems, showing explicit benefits to the industry partner, be agile during the collaboration, and the co-location of the researcher on the industry side. Conclusion: Given the importance of industry-academia collaboration to conduct research of high practical relevance we provide a synthesis of challenges and best practices, which can be used by researchers and practitioners to make informed decisions on how to structure their collaborations.}
}
@article{COPPOLA2022107062,
title = {A taxonomy of metrics for GUI-based testing research: A systematic literature review},
journal = {Information and Software Technology},
volume = {152},
pages = {107062},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107062},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001719},
author = {Riccardo Coppola and Emil Alégroth},
keywords = {Software testing, GUI-based testing, Coverage metrics, Taxonomies, Software verification and validation},
abstract = {Context:
GUI-based testing is a sub-field of software testing research that has emerged in the last three decades. GUI-based testing techniques focus on verifying the functional conformance of the system under test (SUT) through its graphical user interface. However, despite the research domains growth, studies in the field have low reproducibility and comparability. One observed cause of these phenomena is identified as a lack of research rigor and commonly used metrics, including coverage metrics.
Objective:
We aim to identify the most commonly used metrics in the field and formulate a taxonomy of coverage metrics for GUI-based testing research.
Method:
We adopt an evidence-based approach to build the taxonomy through a systematic literature review of studies in the GUI-based testing domain. Identified papers are then analyzed with Open and Axial Coding techniques to identify hierarchical and mutually exclusive categories of metrics with common characteristics, usages, and applications.
Results:
Through the analysis of 169 papers and 315 metric definitions, we obtained a taxonomy with 55 codes (common names for metrics), 17 metric categories, and 4 higher level categories: Functional Level, GUI Level, Model Level and Code Level. We measure a higher number of mentions of Model and Code level metrics over Functional and GUI level metrics.
Conclusions:
We propose a taxonomy for use in future GUI-based testing research to improve the general quality of studies in the domain. In addition, the taxonomy is perceived to help enable more replication studies as well as macro-analysis of the current body of research.}
}
@article{BARBOSA2022106902,
title = {A Systematic Literature Review on prioritizing software test cases using Markov chains},
journal = {Information and Software Technology},
volume = {147},
pages = {106902},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106902},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000623},
author = {Gerson Barbosa and Érica Ferreira {de Souza} and Luciana Brasil Rebelo {dos Santos} and Marlon {da Silva} and Juliana Marino Balera and Nandamudi Lankalapalli Vijaykumar},
keywords = {Systematic Literature Review, Markov Chains, Test case prioritization},
abstract = {Context:
Software Testing is a costly activity since the size of the test case set tends to increase as the construction of the software evolves. Test Case Prioritization (TCP) can reduce the effort and cost of software testing. TCP is an activity where a subset of the existing test cases is selected in order to maximize the possibility of finding defects. On the other hand, Markov Chains representing a reactive system, when solved, can present the occupation time of each of their states. The idea is to use such information and associate priority to those test cases that consist of states with the highest probabilities.
Objective:
The objective of this paper is to conduct a survey to identify and understand key initiatives for using Markov Chains in TCP. Aspects such as approaches, developed techniques, programming languages, analytical and simulation results, and validation tests are investigated.
Methods:
A Systematic Literature Review (SLR) was conducted considering studies published up to July 2021 from five different databases to answer the three research questions.
Results:
From SLR, we identified 480 studies addressing Markov Chains in TCP that have been reviewed in order to extract relevant information on a set of research questions.
Conclusion:
The final 12 studies analyzed use Markov Chains at some stage of test case prioritization in a distinct way, that is, we found that there is no strong relationship between any of the studies, not only on how the technique was used but also in the context of the application. Concerning the fields of application of this subject, 6 forms of approach were found: Controlled Markov Chain, Usage Model, Model-Based Test, Regression Test, Statistical Test, and Random Test. This demonstrates the versatility and robustness of the tool. A large part of the studies developed some prioritization tool, being its validation done in some cases analytically and in others numerically, such as: Measure of the software specification, Optimal Test Transition Probabilities, Adaptive Software Testing, Automatic Prioritization, Ant Colony Optimization, Model Driven approach, and Monte Carlo Random Testing.}
}
@article{KALEESWARAN2022106800,
title = {A systematic literature review on counterexample explanation},
journal = {Information and Software Technology},
volume = {145},
pages = {106800},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106800},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921002378},
author = {Arut Prakash Kaleeswaran and Arne Nordmann and Thomas Vogel and Lars Grunske},
keywords = {Formal methods, Model checking, Counterexample explanation},
abstract = {Context:
Safety is of paramount importance for cyber–physical systems in domains such as automotive, robotics, and avionics. Formal methods such as model checking are one way to ensure the safety of cyber–physical systems. However, adoption of formal methods in industry is hindered by usability issues, particularly the difficulty of understanding model checking results.
Objective:
We want to provide an overview of the state of the art for counterexample explanation by investigating the contexts, techniques, and evaluation of research approaches in this field. This overview shall provide an understanding of current and guide future research.
Method:
To provide this overview, we conducted a systematic literature review. The survey comprises 116 publications that address counterexample explanations for model checking.
Results:
Most primary studies provide counterexample explanations graphically or as traces, minimize counterexamples to reduce complexity, localize errors in the models expressed in the input formats of model checkers, support linear temporal logic or computation tree logic specifications, and use model checkers of the Symbolic Model Verifier family. Several studies evaluate their approaches in safety-critical domains with industrial applications.
Conclusion:
We notably see a lack of research on counterexample explanation that targets probabilistic and real-time systems, leverages the explanations to domain-specific models, and evaluates approaches in user studies. We conclude by discussing the adequacy of different types of explanations for users with varying domain and formal methods expertise, showing the need to support laypersons in understanding model checking results to increase adoption of formal methods in industry.}
}
@article{KUPIAINEN2015143,
title = {Using metrics in Agile and Lean Software Development – A systematic literature review of industrial studies},
journal = {Information and Software Technology},
volume = {62},
pages = {143-163},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S095058491500035X},
author = {Eetu Kupiainen and Mika V. Mäntylä and Juha Itkonen},
keywords = {Agile, Lean, Metrics, Measurement, Systematic literature review, Software engineering},
abstract = {Context
Software industry has widely adopted Agile software development methods. Agile literature proposes a few key metrics but little is known of the actual metrics use in Agile teams.
Objective
The objective of this paper is to increase knowledge of the reasons for and effects of using metrics in industrial Agile development. We focus on the metrics that Agile teams use, rather than the ones used from outside by software engineering researchers. In addition, we analyse the influence of the used metrics.
Method
This paper presents a systematic literature review (SLR) on using metrics in industrial Agile software development. We identified 774 papers, which we reduced to 30 primary studies through our paper selection process.
Results
The results indicate that the reasons for and the effects of using metrics are focused on the following areas: sprint planning, progress tracking, software quality measurement, fixing software process problems, and motivating people. Additionally, we show that although Agile teams use many metrics suggested in the Agile literature, they also use many custom metrics. Finally, the most influential metrics in the primary studies are Velocity and Effort estimate.
Conclusion
The use of metrics in Agile software development is similar to Traditional software development. Projects and sprints need to be planned and tracked. Quality needs to be measured. Problems in the process need to be identified and fixed. Future work should focus on metrics that had high importance but low prevalence in our study, as they can offer the largest impact to the software industry.}
}
@article{LEWOWSKI2022106783,
title = {How far are we from reproducible research on code smell detection? A systematic literature review},
journal = {Information and Software Technology},
volume = {144},
pages = {106783},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106783},
url = {https://www.sciencedirect.com/science/article/pii/S095058492100224X},
author = {Tomasz Lewowski and Lech Madeyski},
keywords = {Software engineering, Code smells, Reproducibility, Reproducible research},
abstract = {Context:
Code smells are symptoms of wrong design decisions or coding shortcuts that may increase defect rate and decrease maintainability. Research on code smells is accelerating, focusing on code smell detection and using code smells as defect predictors. Recent research shows that even between software developers, agreement on what constitutes a code smell is low, but several publications claim the high performance of detection algorithms—which seems counterintuitive, considering that algorithms should be taught on data labeled by developers.
Objective:
This paper aims to investigate the possible reasons for the inconsistencies between studies in the performance of applied machine learning algorithms compared to developers. It focuses on the reproducibility of existing studies.
Methods:
A systematic literature review was performed among conference and journal articles published between 1999 and 2020 to assess the state of reproducibility of the research performed in those papers. A quasi-gold standard procedure was used to validate the search. Modeling process descriptions, reproduction scripts, data sets, and techniques used for their creation were analyzed.
Results:
We obtained data from 46 publications. 22 of them contained a detailed description of the modeling process, 17 included any reproduction data (data set, results, or scripts) and 15 used existing data sets. In most of the publications, analyzed projects were hand-picked by the researchers.
Conclusion:
Most studies do not include any form of an online reproduction package, although this has started to change recently—8% of analyzed studies published before 2018 included a full reproduction package, compared to 22% in years 2018–2019. Ones that do include a package usually use a research group website or even a personal one. Dedicated archives are still rarely used for data packages. We recommend that researchers include complete reproduction packages for their studies and use well-established research data archives instead of their own websites.}
}
@article{PETERSEN20151,
title = {Guidelines for conducting systematic mapping studies in software engineering: An update},
journal = {Information and Software Technology},
volume = {64},
pages = {1-18},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000646},
author = {Kai Petersen and Sairam Vakkalanka and Ludwik Kuzniarz},
keywords = {Systematic mapping studies, Software engineering, Guidelines},
abstract = {Context
Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines.
Objective
To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly.
Method
We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment).
Results
In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given.
Conclusion
The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.}
}
@article{BEECHAM2008860,
title = {Motivation in Software Engineering: A systematic literature review},
journal = {Information and Software Technology},
volume = {50},
number = {9},
pages = {860-878},
year = {2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584907001097},
author = {Sarah Beecham and Nathan Baddoo and Tracy Hall and Hugh Robinson and Helen Sharp},
keywords = {Motivation, Software Engineering, Software Engineer, Characteristics, Personality, Systematic literature review},
abstract = {Objective
In this paper, we present a systematic literature review of motivation in Software Engineering. The objective of this review is to plot the landscape of current reported knowledge in terms of what motivates developers, what de-motivates them and how existing models address motivation.
Methods
We perform a systematic literature review of peer reviewed published studies that focus on motivation in Software Engineering. Systematic reviews are well established in medical research and are used to systematically analyse the literature addressing specific research questions.
Results
We found 92 papers related to motivation in Software Engineering. Fifty-six percent of the studies reported that Software Engineers are distinguishable from other occupational groups. Our findings suggest that Software Engineers are likely to be motivated according to three related factors: their ‘characteristics’ (for example, their need for variety); internal ‘controls’ (for example, their personality) and external ‘moderators’ (for example, their career stage). The literature indicates that de-motivated engineers may leave the organisation or take more sick-leave, while motivated engineers will increase their productivity and remain longer in the organisation. Aspects of the job that motivate Software Engineers include problem solving, working to benefit others and technical challenge. Our key finding is that the published models of motivation in Software Engineering are disparate and do not reflect the complex needs of Software Engineers in their career stages, cultural and environmental settings.
Conclusions
The literature on motivation in Software Engineering presents a conflicting and partial picture of the area. It is clear that motivation is context dependent and varies from one engineer to another. The most commonly cited motivator is the job itself, yet we found very little work on what it is about that job that Software Engineers find motivating. Furthermore, surveys are often aimed at how Software Engineers feel about ‘the organisation’, rather than ‘the profession’. Although models of motivation in Software Engineering are reported in the literature, they do not account for the changing roles and environment in which Software Engineers operate. Overall, our findings indicate that there is no clear understanding of the Software Engineers’ job, what motivates Software Engineers, how they are motivated, or the outcome and benefits of motivating Software Engineers.}
}
@article{ALSOLAI2020106214,
title = {A systematic literature review of machine learning techniques for software maintainability prediction},
journal = {Information and Software Technology},
volume = {119},
pages = {106214},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106214},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302228},
author = {Hadeel Alsolai and Marc Roper},
keywords = {Systematic literature review, Software maintainability prediction, Machine learning, Metric, Dataset},
abstract = {Context
Software maintainability is one of the fundamental quality attributes of software engineering. The accurate prediction of software maintainability is a significant challenge for the effective management of the software maintenance process.
Objective
The major aim of this paper is to present a systematic review of studies related to the prediction of maintainability of object-oriented software systems using machine learning techniques. This review identifies and investigates a number of research questions to comprehensively summarize, analyse and discuss various viewpoints concerning software maintainability measurements, metrics, datasets, evaluation measures, individual models and ensemble models.
Method
The review uses the standard systematic literature review method applied to the most common computer science digital database libraries from January 1991 to July 2018.
Results
We survey 56 relevant studies in 35 journals and 21 conference proceedings. The results indicate that there is relatively little activity in the area of software maintainability prediction compared with other software quality attributes. CHANGE maintenance effort and the maintainability index were the most commonly used software measurements (dependent variables) employed in the selected primary studies, and most made use of class-level product metrics as the independent variables. Several private datasets were used in the selected studies, and there is a growing demand to publish datasets publicly. Most studies focused on regression problems and performed k-fold cross-validation. Individual prediction models were employed in the majority of studies, while ensemble models relatively rarely.
Conclusion
Based on the findings obtained in this systematic literature review, ensemble models demonstrated increased accuracy prediction over individual models, and have been shown to be useful models in predicting software maintainability. However, their application is relatively rare and there is a need to apply these, and other models to an extensive variety of datasets with the aim of improving the accuracy and consistency of results.}
}
@article{RODRIGUEZPEREZ2018164,
title = {Reproducibility and credibility in empirical software engineering: A case study based on a systematic literature review of the use of the SZZ algorithm},
journal = {Information and Software Technology},
volume = {99},
pages = {164-176},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304275},
author = {Gema Rodríguez-Pérez and Gregorio Robles and Jesús M. González-Barahona},
keywords = {Credibility, Reproducibility, SZZ Algorithm, Systematic literature review},
abstract = {Context
Reproducibility of Empirical Software Engineering (ESE) studies is an essential part for improving their credibility, as it offers the opportunity to the research community to verify, evaluate and improve their research outcomes.
Objective
We aim to study reproducibility and credibility in ESE with a case study, by investigating how they have been addressed in studies where SZZ, a widely-used algorithm by Śliwerski, Zimmermann and Zeller to detect the origin of a bug, has been applied.
Methodology
We have performed a systematic literature review to evaluate publications that use SZZ. In total, 187 papers have been analyzed for reproducibility, reporting of limitations and use of improved versions of the algorithm.
Results
We have found a situation with a lot of room for improvement in ESE as reproducibility is not commonly found; factors that undermine the credibility of results are common. We offer some lessons learned and guidelines for researchers and reviewers to address this problem.
Conclusion
Reproducibility and other related aspects that ensure a high quality scientific process should be taken more into consideration by the ESE community in order to increase the credibility of the research results.}
}
@article{KAUR201956,
title = {Investigation on test effort estimation of mobile applications: Systematic literature review and survey},
journal = {Information and Software Technology},
volume = {110},
pages = {56-77},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S095058491930031X},
author = {Anureet Kaur and Kulwant Kaur},
keywords = {Mobile applications, Test effort estimation, Software engineering, Systematic literature review (SLR), Survey},
abstract = {Context
In the last few years, the exigency of mobile devices has proliferated to prodigious heights. The process of developing the mobile software/application proceeds amidst testing phase to verify the correctness of the mobile app. The estimation of testing plays a vital role in the effective completion of testing.
Objective
To identify how estimation of test effort for mobile applications is distinct from other software via published literature and from mobile software organizations. Second is to recognize different issues in adapting traditional test estimation methods to the mobile domain and if suggestions from survey results could be helpful in providing an improved test estimation model for mobile applications.
Method
A systematic literature review is conducted followed by a survey through an online questionnaire filled from experienced mobile application developers and testers.
Results
The results from SLR cover identification of mobile app specific characteristics and reports test effort estimation techniques in the mobile domain. Findings from survey corroborate that a) Function Point/Test Point Analysis is highly adapted traditional test estimation technique to mobile domain; b) Challenges like uncertain requirements, no tool support for test estimation, complexity in testing, client miscommunication etc. are reported; c)Suggestions to improve test estimation process include proper test planning, adoption of agile methodology, healthier communication among client, developer, and tester etc.; d) On the basis of responses, Analytical Hierarchical Process (AHP) identifies “Diverse Devices and OS” along with “Type of App” as highly influential mobile app characteristic on the test estimation process.
Conclusion
Results conclude that the importance of identified mobile app characteristics from SLR cannot be ignored in the estimation process of mobile software testing. There might be a possibility to improve existing test estimation techniques for mobile apps by giving weight to mobile app specific characteristics and by considering suggestions from experienced developers and testers.}
}
@article{SOOMRO201652,
title = {The effect of software engineers’ personality traits on team climate and performance: A Systematic Literature Review},
journal = {Information and Software Technology},
volume = {73},
pages = {52-65},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916000082},
author = {Arjumand Bano Soomro and Norsaremah Salleh and Emilia Mendes and John Grundy and Giles Burch and Azlin Nordin},
keywords = {Software team climate, Personality and software engineering, Systematic literature review, Team performance},
abstract = {Context
Over the past 50years numerous studies have investigated the possible effect that software engineers’ personalities may have upon their individual tasks and teamwork. These have led to an improved understanding of that relationship; however, the analysis of personality traits and their impact on the software development process is still an area under investigation and debate. Further, other than personality traits, “team climate” is also another factor that has also been investigated given its relationship with software teams’ performance.
Objective
The aim of this paper is to investigate how software professionals’ personality is associated with team climate and team performance.
Method
In this paper we detail a Systematic Literature Review (SLR) of the effect of software engineers’ personality traits and team climate on software team performance.
Results
Our main findings include 35 primary studies that have addressed the relationship between personality and team performance without considering team climate. The findings showed that team climate comprises a wide range of factors that fall within the fields of management and behavioral sciences. Most of the studies used undergraduate students as subjects and as surrogates of software professionals.
Conclusions
The findings from this SLR would be beneficial for understanding the personality assessment of software development team members by revealing the traits of personality taxonomy, along with the measurement of the software development team working environment. These measurements would be useful in examining the success and failure possibilities of software projects in development processes.
General terms
Human factors, performance.}
}
@article{TEBES2020106298,
title = {Analyzing and documenting the systematic review results of software testing ontologies},
journal = {Information and Software Technology},
volume = {123},
pages = {106298},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106298},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300495},
author = {Guido Tebes and Denis Peppino and Pablo Becker and Gerardo Matturro and Martin Solari and Luis Olsina},
keywords = {Software testing ontology, Systematic literature review, Systematic literature review process, Secondary study, Analysis, Testing strategy},
abstract = {Context
Software testing is a complex area since it has a large number of specific methods, processes and strategies, involving a lot of domain concepts. Therefore, it would be valuable to have a conceptualized software testing ontology that explicitly and unambiguously defines the concepts. Consequently, it is important to find out the available evidence in the literature on primary studies for software testing ontologies. In particular, we are looking for research that has a rich ontological coverage that includes Non-Functional Requirements (NFRs) and Functional Requirements (FRs) concepts in conjunction with static and dynamic testing concepts, which can be used in method and process specifications for a family of testing strategies.
Objective
The main goal for this secondary study is to identify, evaluate and synthesize the available primary studies on conceptualized software testing ontologies.
Method
To conduct this study, we use the Systematic Literature Review (SLR) approach, which follows our enhanced SLR process. We set three research questions. Additionally, to quantitatively evaluate the quality of the selected conceptualized ontologies, we designed a NFRs tree and its associated metrics and indicators.
Results
We obtained 12 primary studies documenting conceptualized testing ontologies by using three different retrieval methods. In general, we noted that most of them have a lack of NFRs and static testing terminological coverage. Finally, we observe that none of them is directly linked with FRs and NFRs conceptual components.
Conclusion
A general benefit of having the suitable software testing ontology is to minimize the current heterogeneity, ambiguity and incompleteness problems in terms, properties and relationships. We have confirmed that exists heterogeneity, ambiguity, and incompleteness for concepts dealing with testing artifacts, roles, activities, and methods. Moreover, we did not find the suitable ontology for our aim since none of the conceptualized ontologies are directly linked with NFRs and FRs components.}
}
@article{LI2020106287,
title = {A systematic review of unsupervised learning techniques for software defect prediction},
journal = {Information and Software Technology},
volume = {122},
pages = {106287},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106287},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300379},
author = {Ning Li and Martin Shepperd and Yuchen Guo},
keywords = {Unsupervised learning, Software defect prediction, Machine learning, Systematic review, Meta-analysis},
abstract = {Background
Unsupervised machine learners have been increasingly applied to software defect prediction. It is an approach that may be valuable for software practitioners because it reduces the need for labeled training data.
Objective
Investigate the use and performance of unsupervised learning techniques in software defect prediction.
Method
We conducted a systematic literature review that identified 49 studies containing 2456 individual experimental results, which satisfied our inclusion criteria published between January 2000 and March 2018. In order to compare prediction performance across these studies in a consistent way, we (re-)computed the confusion matrices and employed the Matthews Correlation Coefficient (MCC) as our main performance measure.
Results
Our meta-analysis shows that unsupervised models are comparable with supervised models for both within-project and cross-project prediction. Among the 14 families of unsupervised model, Fuzzy CMeans (FCM) and Fuzzy SOMs (FSOMs) perform best. In addition, where we were able to check, we found that almost 11% (262/2456) of published results (contained in 16 papers) were internally inconsistent and a further 33% (823/2456) provided insufficient details for us to check.
Conclusion
Although many factors impact the performance of a classifier, e.g., dataset characteristics, broadly speaking, unsupervised classifiers do not seem to perform worse than the supervised classifiers in our review. However, we note a worrying prevalence of (i) demonstrably erroneous experimental results, (ii) undemanding benchmarks and (iii) incomplete reporting. We therefore encourage researchers to be comprehensive in their reporting.}
}
@article{PERKUSICH2020106241,
title = {Intelligent software engineering in the context of agile software development: A systematic literature review},
journal = {Information and Software Technology},
volume = {119},
pages = {106241},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106241},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302587},
author = {Mirko Perkusich and Lenardo {Chaves e Silva} and Alexandre Costa and Felipe Ramos and Renata Saraiva and Arthur Freire and Ednaldo Dilorenzo and Emanuel Dantas and Danilo Santos and Kyller Gorgônio and Hyggo Almeida and Angelo Perkusich},
keywords = {Intelligent software engineering, Agile software development, Search-based software engineering, Machine learning, Bayesian networks, Artificial intelligence},
abstract = {CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.}
}
@article{TOSI201516,
title = {Supporting the semi-automatic semantic annotation of web services: A systematic literature review},
journal = {Information and Software Technology},
volume = {61},
pages = {16-32},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000154},
author = {Davide Tosi and Sandro Morasca},
keywords = {Ontologies, Semantic web services, Functional and non-functional aspects, Systematic literature review},
abstract = {Context
Semantically annotating web services is gaining more attention as an important aspect to support the automatic matchmaking and composition of web services. Therefore, the support of well-known and agreed ontologies and tools for the semantical annotation of web services is becoming a key concern to help the diffusion of semantic web services.
Objective
The objective of this systematic literature review is to summarize the current state-of-the-art for supporting the semantical annotation of web services by providing answers to a set of research questions.
Method
The review follows a predefined procedure that involves automatically searching well-known digital libraries. As a result, a total of 35 primary studies were identified as relevant. A manual search led to the identification of 9 additional primary studies that were not reported during the automatic search of the digital libraries. Required information was extracted from these 44 studies against the selected research questions and finally reported.
Results
Our systematic literature review identified some approaches available for semantically annotating functional and non-functional aspects of web services. However, many of the approaches are either not validated or the validation done lacks credibility.
Conclusion
We believe that a substantial amount of work remains to be done to improve the current state of research in the area of supporting semantic web services.}
}
@article{WILLIAMS201031,
title = {Characterizing software architecture changes: A systematic review},
journal = {Information and Software Technology},
volume = {52},
number = {1},
pages = {31-51},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909001207},
author = {Byron J. Williams and Jeffrey C. Carver},
keywords = {Software architecture, Software maintenance, Change characterization, Software evolution, Systematic review, Software changes},
abstract = {With today’s ever increasing demands on software, software developers must produce software that can be changed without the risk of degrading the software architecture. One way to address software changes is to characterize their causes and effects. A software change characterization mechanism allows developers to characterize the effects of a change using different criteria, e.g. the cause of the change, the type of change that needs to be made, and the part of the system where the change must take place. This information then can be used to illustrate the potential impact of the change. This paper presents a systematic literature review of software architecture change characteristics. The results of this systematic review were used to create the Software Architecture Change Characterization Scheme (SACCS). This report addresses key areas involved in making changes to software architecture. SACCS’s purpose is to identify the characteristics of a software change that will have an impact on the high-level software architecture.}
}
@article{CLARK2021106567,
title = {Test case generation for agent-based models: A systematic literature review},
journal = {Information and Software Technology},
volume = {135},
pages = {106567},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106567},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000501},
author = {Andrew G. Clark and Neil Walkinshaw and Robert M. Hierons},
keywords = {Agent-based modelling, Multi-agent systems, Software testing, Test case generation, Systematic literature review},
abstract = {Context:
Agent-based models play an important role in simulating complex emergent phenomena and supporting critical decisions. In this context, a software fault may result in poorly informed decisions that lead to disastrous consequences. The ability to rigorously test these models is therefore essential.
Objective:
Our objective is to summarise the state-of-the-art techniques for test case generation in agent-based models and identify future research directions.
Method:
We have conducted a systematic literature review in which we pose five research questions related to the key aspects of test case generation in agent-based models: What are the information artifacts used to generate tests? How are these tests generated? How is a verdict assigned to a generated test? How is the adequacy of a generated test suite measured? What level of abstraction of an agent-based model is targeted by a generated test?
Results:
Out of the 464 initial search results, we identified 24 primary publications. Based on these primary publications, we formed a taxonomy to summarise the state-of-the-art techniques for test case generation in agent-based models. Our results show that whilst the majority of techniques are effective for testing functional requirements at the agent and integration levels of abstraction, there are comparatively few techniques capable of testing society-level behaviour. Furthermore, the majority of techniques cannot test non-functional requirements or “soft goals”.
Conclusions:
This paper reports insights into the key developments and open challenges concerning test case generation in agent-based models that may be of interest to both researchers and practitioners. In particular, we identify the need for test case generation techniques that focus on societal and non-functional behaviour, and a more thorough evaluation using realistic case studies that feature challenging properties associated with a typical agent-based model.}
}
@article{WEN201241,
title = {Systematic literature review of machine learning based software development effort estimation models},
journal = {Information and Software Technology},
volume = {54},
number = {1},
pages = {41-59},
year = {2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911001832},
author = {Jianfeng Wen and Shixian Li and Zhiyong Lin and Yong Hu and Changqin Huang},
keywords = {Software effort estimation, Machine learning, Systematic literature review},
abstract = {Context
Software development effort estimation (SDEE) is the process of predicting the effort required to develop a software system. In order to improve estimation accuracy, many researchers have proposed machine learning (ML) based SDEE models (ML models) since 1990s. However, there has been no attempt to analyze the empirical evidence on ML models in a systematic way.
Objective
This research aims to systematically analyze ML models from four aspects: type of ML technique, estimation accuracy, model comparison, and estimation context.
Method
We performed a systematic literature review of empirical studies on ML model published in the last two decades (1991–2010).
Results
We have identified 84 primary studies relevant to the objective of this research. After investigating these studies, we found that eight types of ML techniques have been employed in SDEE models. Overall speaking, the estimation accuracy of these ML models is close to the acceptable level and is better than that of non-ML models. Furthermore, different ML models have different strengths and weaknesses and thus favor different estimation contexts.
Conclusion
ML models are promising in the field of SDEE. However, the application of ML models in industry is still limited, so that more effort and incentives are needed to facilitate the application of ML models. To this end, based on the findings of this review, we provide recommendations for researchers as well as guidelines for practitioners.}
}
@article{ALI2010871,
title = {A systematic review of comparative evidence of aspect-oriented programming},
journal = {Information and Software Technology},
volume = {52},
number = {9},
pages = {871-887},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910000819},
author = {Muhammad Sarmad Ali and Muhammad {Ali Babar} and Lianping Chen and Klaas-Jan Stol},
keywords = {Evidence-based software engineering, Systematic literature review, Aspect-oriented programming},
abstract = {Context
Aspect-oriented programming (AOP) promises to improve many facets of software quality by providing better modularization and separation of concerns, which may have system wide affect. There have been numerous claims in favor and against AOP compared with traditional programming languages such as Objective Oriented and Structured Programming Languages. However, there has been no attempt to systematically review and report the available evidence in the literature to support the claims made in favor or against AOP compared with non-AOP approaches.
Objective
This research aimed to systematically identify, analyze, and report the evidence published in the literature to support the claims made in favor or against AOP compared with non-AOP approaches.
Method
We performed a systematic literature review of empirical studies of AOP based development, published in major software engineering journals and conference proceedings.
Results
Our search strategy identified 3307 papers, of which 22 were identified as reporting empirical studies comparing AOP with non-AOP approaches. Based on the analysis of the data extracted from those 22 papers, our findings show that for performance, code size, modularity, and evolution related characteristics, a majority of the studies reported positive effects, a few studies reported insignificant effects, and no study reported negative effects; however, for cognition and language mechanism, negative effects were reported.
Conclusion
AOP is likely to have positive effect on performance, code size, modularity, and evolution. However its effect on cognition and language mechanism is less likely to be positive. Care should be taken using AOP outside the context in which it has been validated.}
}
@article{WALIA20091087,
title = {A systematic literature review to identify and classify software requirement errors},
journal = {Information and Software Technology},
volume = {51},
number = {7},
pages = {1087-1109},
year = {2009},
note = {Special Section: Software Engineering for Secure Systems},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909000111},
author = {Gursimran Singh Walia and Jeffrey C. Carver},
keywords = {Systematic literature review, Human errors, Software quality},
abstract = {Most software quality research has focused on identifying faults (i.e., information is incorrectly recorded in an artifact). Because software still exhibits incorrect behavior, a different approach is needed. This paper presents a systematic literature review to develop taxonomy of errors (i.e., the sources of faults) that may occur during the requirements phase of software lifecycle. This taxonomy is designed to aid developers during the requirement inspection process and to improve overall software quality. The review identified 149 papers from the software engineering, psychology and human cognition literature that provide information about the sources of requirements faults. A major result of this paper is a categorization of the sources of faults into a formal taxonomy that provides a starting point for future research into error-based approaches to improving software quality.}
}
@article{HYDARA2015170,
title = {Current state of research on cross-site scripting (XSS) – A systematic literature review},
journal = {Information and Software Technology},
volume = {58},
pages = {170-186},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001700},
author = {Isatou Hydara and Abu Bakar Md. Sultan and Hazura Zulzalil and Novia Admodisastro},
keywords = {Systematic literature review, Cross-site scripting, Security, Web applications},
abstract = {Context
Cross-site scripting (XSS) is a security vulnerability that affects web applications. It occurs due to improper or lack of sanitization of user inputs. The security vulnerability caused many problems for users and server applications.
Objective
To conduct a systematic literature review on the studies done on XSS vulnerabilities and attacks.
Method
We followed the standard guidelines for systematic literature review as documented by Barbara Kitchenham and reviewed a total of 115 studies related to cross-site scripting from various journals and conference proceedings.
Results
Research on XSS is still very active with publications across many conference proceedings and journals. Attack prevention and vulnerability detection are the areas focused on by most of the studies. Dynamic analysis techniques form the majority among the solutions proposed by the various studies. The type of XSS addressed the most is reflected XSS.
Conclusion
XSS still remains a big problem for web applications, despite the bulk of solutions provided so far. There is no single solution that can effectively mitigate XSS attacks. More research is needed in the area of vulnerability removal from the source code of the applications before deployment.}
}
@article{AMNA2022106824,
title = {Ambiguity in user stories: A systematic literature review},
journal = {Information and Software Technology},
volume = {145},
pages = {106824},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106824},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000040},
author = {Anis R. Amna and Geert Poels},
keywords = {Requirements engineering, Agile software development, User story, Ambiguity, Systematic Literature Review},
abstract = {Context
Ambiguity in user stories is a problem that has received little research attention. Due to the absence of review studies, it is not known how and to what extent this problem, which impacts the effectiveness of user stories in supporting systems development, has been solved.
Objectives
We review the studies that investigate or develop solutions for problems related to ambiguity in user stories. We investigate how these problems manifest themselves, what their causes and consequences are, what solutions have been proposed and what evidence of their effectiveness has been presented. Based on the insights we obtain from this review, we identify research gaps and suggest opportunities for future research.
Methods
We followed Systematic Literature Review guidelines to review problems investigated, solutions proposed, and validation/evaluation methods used. We classified the reviewed studies according to the four linguistic levels of ambiguity (i.e., lexical, syntactic, semantic, pragmatic) proposed by Berry and Kamsties to obtain insights from patterns that we observe in the classification of problems and solutions.
Results
A total of 36 studies published in 2001–2020 investigated ambiguity in user stories. Based on four patterns we discern, we identify three research gaps. First, we need more research on human behaviors and cognitive factors causing ambiguity. Second, ambiguity is seldom studied as a problem of a set of related user stories, like a theme or epic in Scrum. Third, there is a lack of holistic solution approaches that consider ambiguity at multiple linguistic levels.
Conclusion
Ambiguity in user stories is a known problem. However, a comprehensive solution for addressing ambiguity in a set of related user stories as it manifests itself at different linguistic levels as a cognitive problem is lacking.}
}
@article{QIN20191,
title = {Enactment of adaptation in data stream processing with latency implications—A systematic literature review},
journal = {Information and Software Technology},
volume = {111},
pages = {1-21},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300539},
author = {Cui Qin and Holger Eichelberger and Klaus Schmid},
keywords = {Stream processing, Big data, Runtime adaptation, Enactment, Latency, Systematic literature review},
abstract = {Context
Stream processing is a popular paradigm to continuously process huge amounts of data. Runtime adaptation plays a significant role in supporting the optimization of data processing tasks. In recent years runtime adaptation has received significant interest in scientific literature. However, so far no categorization of the enactment approaches for runtime adaptation in stream processing has been established.
Objective
This paper identifies and characterizes different approaches towards the enactment of runtime adaptation in stream processing with a main focus on latency as quality dimension.
Method
We performed a systematic literature review (SLR) targeting five main research questions. An automated search, resulting in 244 papers, was conducted. 75 papers published between 2006 and 2018 were finally included. From the selected papers, we extracted data like processing problems, adaptation goals, enactment approaches of adaptation, enactment techniques, evaluation metrics as well as evaluation parameters used to trigger the enactment of adaptation in their evaluation.
Results
We identified 17 different enactment approaches and categorized them into a taxonomy. For each, we extracted the underlying technique used to implement this enactment approach. Further, we identified 9 categories of processing problems, 6 adaptation goals, 9 evaluation metrics and 12 evaluation parameters according to the extracted data properties.
Conclusion
We observed that the research interest on enactment approaches to the adaptation of stream processing has significantly increased in recent years. The most commonly applied enactment approaches are parameter adaptation to tune parameters or settings of the processing, load balancing used to re-distribute workloads, and processing scaling to dynamically scale up and down the processing. In addition to latency, most adaptations also address resource fluctuation / bottleneck problems. For presenting a dynamic environment to evaluate enactment approaches, researchers often change input rates or processing workloads.}
}
@article{ALI201948,
title = {A critical appraisal tool for systematic literature reviews in software engineering},
journal = {Information and Software Technology},
volume = {112},
pages = {48-50},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300771},
author = {Nauman bin Ali and Muhammad Usman},
keywords = {Systematic literature reviews, Quality assessment, Software engineering, Critical appraisal tools, AMSTAR},
abstract = {Context: Methodological research on systematic literature reviews (SLRs) in Software Engineering (SE) has so far focused on developing and evaluating guidelines for conducting systematic reviews. However, the support for quality assessment of completed SLRs has not received the same level of attention. Objective: To raise awareness of the need for a critical appraisal tool (CAT) for assessing the quality of SLRs in SE. To initiate a community-based effort towards the development of such a tool. Method: We reviewed the literature on the quality assessment of SLRs to identify the frequently used CATs in SE and other fields. Results: We identified that the CATs currently used is SE were borrowed from medicine, but have not kept pace with substantial advancements in the field of medicine. Conclusion: In this paper, we have argued the need for a CAT for quality appraisal of SLRs in SE. We have also identified a tool that has the potential for application in SE. Furthermore, we have presented our approach for adapting this state-of-the-art CAT for assessing SLRs in SE.}
}
@article{TARHAN2016122,
title = {Business process maturity models: A systematic literature review},
journal = {Information and Software Technology},
volume = {75},
pages = {122-134},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300015},
author = {Ayca Tarhan and Oktay Turetken and Hajo A. Reijers},
keywords = {Business process management, Business process orientation, Maturity model, Systematic literature review},
abstract = {Context
The number of maturity models proposed in the area of Business Process Management (BPM) has increased considerably in the last decade. However, there are a number of challenges, such as the limited empirical studies on their validation and a limited extent of actionable properties of these models in guiding their application. These challenges hinder the widespread usage of the maturity models in the BPM field.
Objective
In order to better understand the state of the research on business process maturity models (BPMMs) and identify opportunities for future research, we conducted a systematic literature review.
Method
We searched the studies between the years 1990 and 2014 in established digital libraries to identify empirical studies of BPMMs by focusing on their development, validation, and application. We targeted studies on generic models proposed for business process maturity, business process management or orientation maturity, and selected 61 studies out of 2899 retrieved initially.
Results
We found that despite that many BPMMs were proposed in the last decade, the level of empirical evidence that reveals the validity and usefulness of these models is scarce.
Conclusion
The current state of research on BPM maturity is in its early phases, and academic literature lacks methodical applications of many mainstream BPMMs that have been proposed. Future research should be directed towards: (1) reconciling existing models with a strong emphasis on prescriptive properties, (2) conducting empirical studies to demonstrate the validity and usefulness of BPMMs, and (3) separating the assessment method used to evaluate the maturity level from the maturity model which acts as the reference framework for the assessment.}
}
@article{VELASQUEZ201830,
title = {Authentication schemes and methods: A systematic literature review},
journal = {Information and Software Technology},
volume = {94},
pages = {30-37},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301501},
author = {Ignacio Velásquez and Angélica Caro and Alfonso Rodríguez},
keywords = {Security, Authentication scheme, Multi-factor authentication method, Systematic literature review},
abstract = {Context
There is a great variety of techniques for performing authentication, like the use of text passwords or smart cards. Some techniques combine others into one, which is known as multi-factor authentication. There is an interest in knowing existing authentication techniques, including those aimed at multi-factor authentication, and the frameworks that can be found in literature that are used to compare and select these techniques according to different criteria.
Objective
This article aims to gather the existing knowledge on authentication techniques and ways to discern the most effective ones for different contexts.
Method
A systematic literature review is performed in order to gather existing authentication techniques proposed in literature and ways to compare and select them in different contexts. A total of 515 single-factor and 442 multi-factor authentication techniques have been found. Furthermore, 17 articles regarding comparison and selection criteria for authentication techniques and 8 frameworks that help in such a task are discussed.
Results
A great variety of single-factor techniques has been found and smart card-based authentication was shown to be the most researched technique. Similarly, multi-factor techniques combine the different single-factor techniques found and the combination of text-passwords and smart cards is the most researched technique. Usability, security and costs are the most used criteria for comparing and selecting authentication schemes, whereas the context is given an important remark as well. No framework among the ones found analyzed in detail both single-factor and multi-factor authentication techniques for the decision-making process.
Conclusion
The review shows that a vast research has been done for authentication techniques, although its use in some contexts has not been researched as much. The lack of works regarding the comparison and selection of authentication techniques is observed.}
}
@article{TAHIR2016101,
title = {A systematic literature review on software measurement programs},
journal = {Information and Software Technology},
volume = {73},
pages = {101-121},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300131},
author = {Touseef Tahir and Ghulam Rasool and Cigdem Gencel},
keywords = {Software measurement, Software measurement program, Software metrics, GQM, Systematic Literature Review},
abstract = {Context
Software measurement programs (MPs) are an important means for understanding, evaluating, managing, and improving software processes, products and resources. However, implementing successful MPs still remains a challenge.
Objectives
To make a comprehensive review of the studies on MPs for bringing into light the existing measurement planning models and tools used for implementing MPs,the accumulated knowledge on the success/failure factors of MPs and mitigation strategies to address their challenges.
Methods
A Systematic Literature Review (SLR) was conducted. In total, 65primary studies were reviewed and analyzed.
Results
We identified 35 measurement planning models and 11 associated tools, most of which either proposed extensions or improvements for goal based approaches. The identified success factors include (a) organizational adoption of MP, (b) integration of MP with SDLC, (c) synchronization of MP with SPI and (d) design of MP. The mostly mentioned mitigation strategies for addressing challenges are effective change management and measurement stakeholder management, automated tool support and incorporation of engineering mechanisms for designing sustainable, effective, scalable and extendible MPs, and measurement expertise and standards development.
Conclusion
Most of the success factors and mitigation strategies have interdependencies. Therefore, for successful MP implementation, software organizations should consider these factors in combination and make a feasibility study at the very beginning.}
}
@article{RAJAPAKSE2022106700,
title = {Challenges and solutions when adopting DevSecOps: A systematic review},
journal = {Information and Software Technology},
volume = {141},
pages = {106700},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106700},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001543},
author = {Roshan N. Rajapakse and Mansooreh Zahedi and M. Ali Babar and Haifeng Shen},
keywords = {DevOps, Security, DevSecOps, Continuous Software Engineering, Systematic Literature Review},
abstract = {Context:
DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge.
Objective:
This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future.
Method:
We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data.
Results:
We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied.
Conclusions:
We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.}
}
@article{HUJAINAH201885,
title = {Stakeholder quantification and prioritisation research: A systematic literature review},
journal = {Information and Software Technology},
volume = {102},
pages = {85-99},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302422},
author = {Fadhl Hujainah and Rohani Binti {Abu Bakar} and Basheer Al-haimi and Mansoor Abdullateef Abdulgabber},
keywords = {Stakeholders quantification, Stakeholders prioritisation, Systematic review},
abstract = {Context
Stakeholder quantification and prioritisation (SQP) is executed to quantify and prioritise stakeholders of the system based on their impacts. Selecting and involving the appropriate stakeholders are considered one of the major factors for producing a successful system.
Objective
The objectives of this paper is to provide precise investigation regarding the SQP domain with respect to its impact on prioritising requirements, identifying SQP attributes, critically investigating the existing techniques, and presenting the challenges and recommended future works.
Method
The systematic literature review (SLR) guidelines proposed by Kitchenham are adopted to guide the review process. The identified related studies underwent a rigorous study selection process. Thus, 31 out of 210 identified studies were selected as primary studies to address adequately the formulated research questions.
Results
Findings demonstrate that SQP is a crucial process in requirement prioritisation (RP) because of its ability to identify stakeholders’ impact on the systems requirements that lead to the production of a correctly prioritised list of requirements. Seventeen SQP attributes are revealed along with their description, usage impact, and degree of importance. Furthermore, nine techniques that focus on quantification and prioritisation of the stakeholders are identified and critically analysed in terms of their description, SQP process involved, SQP attributes used, types, and limitations. The findings reveal that these techniques face some challenges with respect to the lack of low-level implementation details, lack of automation and intelligence level, and heavy reliance on the involvement of experts.
Conclusion
SQP has been extensively discussed in stakeholder analysis and requirement prioritisation domains. Based on the findings, a new intelligent solution is suggested to minimise the need for expert participation in conducting the SQP process along with proposing measurement criteria for the attributes used to evaluate the stakeholders. The deficiency of research works regarding the selection of SQP techniques is also observed.}
}
@article{TORRECILLASALINAS201692,
title = {Agile, Web Engineering and Capability Maturity Model Integration: A systematic literature review.},
journal = {Information and Software Technology},
volume = {71},
pages = {92-107},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S095058491500186X},
author = {C.J. Torrecilla-Salinas and J. Sedeño and M.J. Escalona and M. Mejías},
keywords = {Agile, Scrum, Web Engineering, CMMI, Software Engineering},
abstract = {Context
Agile approaches are an alternative for organizations developing software, particularly for those who develop Web applications. Besides, CMMI (Capability Maturity Model Integration) models are well-established approaches focused on assessing the maturity of an organization that develops software. Web Engineering is the field of Software Engineering responsible for analyzing and studying the specific characteristics of the Web. The suitability of an Agile approach to help organizations reach a certain CMMI maturity level in Web environments will be very interesting, as they will be able to keep the ability to quickly react and adapt to changes as long as their development processes get mature.
Objective
This paper responds to whether it is feasible or not, for an organization developing Web systems, to achieve a certain maturity level of the CMMI-DEV model using Agile methods.
Method
The proposal is analyzed by means of a systematic literature review of the relevant approaches in the field, defining a characterization schema in order to compare them to introduce the current state-of-the-art.
Results
The results achieved after the systematic literature review are presented, analyzed and compared against the defined schema, extracting relevant conclusions for the different dimensions of the problem: compatibility, compliance, experience, maturity and Web.
Conclusion
It is concluded that although the definition of an Agile approach to meet the different CMMI maturity levels goals could be possible for an organization developing Web systems, there is still a lack of detailed studies and analysis on the field.}
}
@article{HOSSEINZADEH201872,
title = {Diversification and obfuscation techniques for software security: A systematic literature review},
journal = {Information and Software Technology},
volume = {104},
pages = {72-93},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301484},
author = {Shohreh Hosseinzadeh and Sampsa Rauti and Samuel Laurén and Jari-Matti Mäkelä and Johannes Holvitie and Sami Hyrynsalmi and Ville Leppänen},
keywords = {Diversification, Obfuscation, Software security, Systematic literature review},
abstract = {Context: Diversification and obfuscation are promising techniques for securing software and protecting computers from harmful malware. The goal of these techniques is not removing the security holes, but making it difficult for the attacker to exploit security vulnerabilities and perform successful attacks. Objective: There is an increasing body of research on the use of diversification and obfuscation techniques for improving software security; however, the overall view is scattered and the terminology is unstructured. Therefore, a coherent review gives a clear statement of state-of-the-art, normalizes the ongoing discussion and provides baselines for future research. Method: In this paper, systematic literature review is used as the method of the study to select the studies that discuss diversification/obfuscation techniques for improving software security. We present the process of data collection, analysis of data, and report the results. Results: As the result of the systematic search, we collected 357 articles relevant to the topic of our interest, published between the years 1993 and 2017. We studied the collected articles, analyzed the extracted data from them, presented classification of the data, and enlightened the research gaps. Conclusion: The two techniques have been extensively used for various security purposes and impeding various types of security attacks. There exist many different techniques to obfuscate/diversify programs, each of which targets different parts of the programs and is applied at different phases of software development life-cycle. Moreover, we pinpoint the research gaps in this field, for instance that there are still various execution environments that could benefit from these two techniques, including cloud computing, Internet of Things (IoT), and trusted computing. We also present some potential ideas on applying the techniques on the discussed environments.}
}
@article{HEEAGER201822,
title = {A conceptual model of agile software development in a safety-critical context: A systematic literature review},
journal = {Information and Software Technology},
volume = {103},
pages = {22-39},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301125},
author = {Lise Tordrup Heeager and Peter Axel Nielsen},
keywords = {Agile software development, Agile processes, Software development, Safety-critical software systems, Systematic literature review, Interpretive literature review},
abstract = {Context
Safety-critical software systems are increasingly being used in new application areas, such as personal medical devices, traffic control, and detection of pathogens. A current research debate is regarding whether safety-critical systems are better developed with traditional waterfall processes or agile processes that are purportedly faster and promise to lead to better products.
Objective
To identify the issues and disputes in agile development of safety-critical software and the key qualities as found in the extant research literature.
Method
We conducted a systematic literature review as an interpretive study following a research design to search, assess, extract, group, and understand the results of the found studies.
Results
There are key issues and propositions that we elicit from the literature and combine into a conceptual model for understanding the foundational challenges of agile software development of safety-critical systems. The conceptual model consists of four problematic practice areas and five relationships, which we find to be even more important than the problematic areas. From this review, we suggest that there are important research gaps that need to be investigated.
Conclusions
We suggest that future research should have a primary focus on the relationships in the resulting conceptual model and specifically on the dynamics of the field as a whole, on incremental versus iterative development, and on how to create value with minimal but sufficient effort.}
}
@article{MESQUIDA2012239,
title = {IT Service Management Process Improvement based on ISO/IEC 15504: A systematic review},
journal = {Information and Software Technology},
volume = {54},
number = {3},
pages = {239-247},
year = {2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911002266},
author = {Antoni Lluís Mesquida and Antonia Mas and Esperança Amengual and Jose A. Calvo-Manzano},
keywords = {Software Process Improvement (SPI), ISO/IEC 15504 (SPICE), IT Service Management (ITSM), Systematic review},
abstract = {Context
In recent years, many software companies have considered Software Process Improvement (SPI) as essential for successful software development. These companies have also shown special interest in IT Service Management (ITSM). SPI standards have evolved to incorporate ITSM best practices.
Objective
This paper presents a systematic literature review of ITSM Process Improvement initiatives based on the ISO/IEC 15504 standard for process assessment and improvement.
Method
A systematic literature review based on the guidelines proposed by Kitchenham and the review protocol template developed by Biolchini et al. is performed.
Results
Twenty-eight relevant studies related to ITSM Process Improvement have been found. From the analysis of these studies, nine different ITSM Process Improvement initiatives have been detected. Seven of these initiatives use ISO/IEC 15504 conformant process assessment methods.
Conclusion
During the last decade, in order to satisfy the on-going demand of mature software development companies for assessing and improving ITSM processes, different models which use the measurement framework of ISO/IEC 15504 have been developed. However, it is still necessary to define a method with the necessary guidelines to implement both software development processes and ITSM processes reducing the amount of effort, especially because some processes of both categories are overlapped.}
}
@article{GERALDI2020106293,
title = {Software product line applied to the internet of things: A systematic literature review},
journal = {Information and Software Technology},
volume = {124},
pages = {106293},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106293},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300434},
author = {Ricardo Theis Geraldi and Sheila Reinehr and Andreia Malucelli},
keywords = {Internet of things, Software product line, Variability management, Product family engineering, Families of systems},
abstract = {Context
Internet of Things (IoT) is a promising paradigm due to the growing number of devices that may be connected, defined as “things”. Managing these “things” is still considered a challenge. One way to overcome this challenge may be by adopting the software product line (SPL) paradigm and the variability management (VM) activity. SPL engineering consists of mechanisms that provide identification, representation, and traceability, which may be helpful to “things” management supported by VM organizational and technical activities.
Objective
This research aims to investigate how SPL engineering has been applied along with the IoT paradigm, as well as how VM is being carried out.
Method
A systematic literature review (SLR) was conducted considering papers available until March 2019. This systematic review identified 1039 papers. After eliminating the duplicated titles and the ones not related to the review, 112 papers remained. The number of papers was narrowed to 56 after applying the exclusion criteria.
Results
The results provide evidence on the diversity of proposed SPLs used to specify approaches for managing IoT systems. However, most SPLs and research developed for IoT lack a systematic and detailed specification to ensure their quality, as well as tailoring guidelines for further use.}
}
@article{JIA2021106478,
title = {A systematic review of scheduling approaches on multi-tenancy cloud platforms},
journal = {Information and Software Technology},
volume = {132},
pages = {106478},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106478},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920302214},
author = {Ru Jia and Yun Yang and John Grundy and Jacky Keung and Li Hao},
keywords = {Systematic review, Survey, Cloud computing, Multi-tenancy, Scheduling},
abstract = {Context:
Scheduling in cloud is complicated as a result of multi-tenancy. Diverse tenants have different requirements, including service functions, response time, QoS and throughput. Diverse tenants require different scheduling capabilities, resource consumption and competition. Multi-tenancy scheduling approaches have been developed for different service models, such as Software as a Service (SaaS), Platform as a service (PaaS), Infrastructure as a Service (IaaS), and Database as a Service (DBaaS).
Objective:
In this paper, we survey the current landscape of multi-tenancy scheduling, laying out the challenges and complexity of software engineering where multi-tenancy issues are involved. This study emphasises scheduling policies, cloud provisioning and deployment with regards to multi-tenancy issues. We conduct a systematic literature review of research studies related to multi-tenancy scheduling approaches on cloud platforms determine the primary scheduling approaches currently used and the challenges for addressing key multi-tenancy scheduling issues.
Method:
We adopted a systematic literature review method to search and review many major journal and conference papers on four major online electronic databases, which address our four predefined research questions. Defining inclusion and exclusion criteria was the initial step before extracting data from the selected papers and deriving answers addressing our enquiries.
Results:
Finally, 53 papers were selected, of which 62 approaches were identified. Most of these methods are developed without cloud layers’ limitation (43.40%) and on SaaS, most of scheduling approaches are oriented to framework design (43.75%).
Conclusion:
The results have demonstrated most of multi-tenancy scheduling solutions can work at any delivery layer. With the difference of tenants’ requirements and functionalities, the choice of cloud service delivery models is changed. Based on our study, designing a multi-tenancy scheduling framework should consider the following 3 factors: computing, QoS and storage resource. One of the potential research foci of multi-tenancy scheduling approaches is on GPU scheduling.}
}
@article{BANIJAMALI2020106271,
title = {Software architectures of the convergence of cloud computing and the Internet of Things: A systematic literature review},
journal = {Information and Software Technology},
volume = {122},
pages = {106271},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106271},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300215},
author = {Ahmad Banijamali and Olli-Pekka Pakanen and Pasi Kuvaja and Markku Oivo},
keywords = {Software architecture, Complex systems, Internet of Things (IoT), Cloud computing, Fog computing, Edge computing},
abstract = {Context
Over the last few years, there has been an increasing interest in the convergence of cloud computing and the Internet of Things (IoT). Although software systems in this domain have attracted researchers to develop a large body of knowledge on software architecture designs, there is no systematic analysis of this knowledge.
Objective
This study aims to identify and synthesise state-of-the-art architectural elements including the design patterns, styles, views, quality attributes, and evaluation methodologies in the convergence of cloud computing and IoT.
Method
We used systematic literature review (SLR) methodology for a detailed analysis of 82 primary studies of a total of 1618 studies.
Results
We extracted six architectural design patterns in this domain; among them, edge connectivity patterns stand out as the most popular choice. The service-oriented architecture is the most frequently applied style in this context. Among all applicable quality attributes, scalability, timeliness, and security were the most investigated quality attributes. In addition, we included nine cross analyses to address the relationship between architectural patterns, styles, views, and evaluation methodologies with respect to different quality attributes and application areas.
Conclusions
Our findings indicate that research on software architectures in this domain is increasing. Although few studies were found in which industrial evaluations were presented, industry requires more scientific and empirically validated design frameworks to guide software engineering in this domain. This work provides an overview of the field while identifying areas for future research.}
}
@article{ALABOOD2023107081,
title = {A systematic literature review of the Design Critique method},
journal = {Information and Software Technology},
volume = {153},
pages = {107081},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107081},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001902},
author = {Lorans Alabood and Zahra Aminolroaya and Dianna Yim and Omar Addam and Frank Maurer},
keywords = {Design Critique, Human-centered design, User-experience research, AgileUX, Systematic literature review},
abstract = {Context:
The Design Critique (DC) method is becoming more common in Human–Computer Interaction (HCI) and User Experience (UX) studies as the need for new evaluation methods of emerging technologies is increasing. However, there is an clear lack of guidelines on how to conduct DC studies in the UX context.
Objective:
The goal of this paper is to provide an overview of the DC method in the fields of UX. In addition, this paper aims to propose a generic process of running DC studies in the same context.
Methods:
We present a systematic literature review of the DC method. Moreover, we conduct a course of thematic analysis on the selected papers to identify the various DC processes and explore the following attributes: participant categories, data collection methods, and data analysis methods in each process.
Results:
We identified three different trends of DC processes: detailed, moderate and minimal. In addition, we proposed a generic DC process consisting of 10 steps divided into three main phases: preparation, conducting design critique, and pro-processing. We found that domain experts represent the majority of studies participants. Using interviews to collect qualitative data and using script coding analysis are the two most common methods of collecting and analyzing data.
Conclusion:
Conducting DC studies can improve overall systems usability by addressing design flaws at an early stage of development. The process of conducting a DC varies, depending on the project goals and states. The DC method aligns well with the small light-weight steps approach in Agile methods.}
}
@article{WOHLIN2022106908,
title = {Successful combination of database search and snowballing for identification of primary studies in systematic literature studies},
journal = {Information and Software Technology},
volume = {147},
pages = {106908},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106908},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000659},
author = {Claes Wohlin and Marcos Kalinowski and Katia {Romero Felizardo} and Emilia Mendes},
keywords = {Systematic literature reviews, Hybrid search, Snowballing, Scopus},
abstract = {Background:
A good search strategy is essential for a successful systematic literature study. Historically, database searches have been the norm, which was later complemented with snowball searches. Our conjecture is that we can perform even better searches if combining these two search approaches, referred to as a hybrid search strategy.
Objective:
Our main objective was to compare and evaluate a hybrid search strategy. Furthermore, we compared four alternative hybrid search strategies to assess whether we could identify more cost-efficient ways of searching for relevant primary studies.
Methods:
To compare and evaluate the hybrid search strategy, we replicated the search procedure in a systematic literature review (SLR) on industry–academia collaboration in software engineering. The SLR used a more “traditional” approach to searching for relevant articles for an SLR, while our replication was executed using a hybrid search strategy.
Results:
In our evaluation, the hybrid search strategy was superior in identifying relevant primary studies. It identified 30% more primary studies and even more studies when focusing only on peer-reviewed articles. To embrace individual viewpoints when assessing research articles and minimise the risk of missing primary studies, we introduced two new concepts, wild cards and borderline articles, when performing systematic literature studies.
Conclusions:
The hybrid search strategy is a strong contender for being used when performing systematic literature studies. Furthermore, alternative hybrid search strategies may be viable if selected wisely in relation to the start set for snowballing. Finally, the two new concepts were judged as essential to cater for different individual judgements and to minimise the risk of excluding primary studies that ought to be included.}
}
@article{SANTIAGO20121340,
title = {Model-Driven Engineering as a new landscape for traceability management: A systematic literature review},
journal = {Information and Software Technology},
volume = {54},
number = {12},
pages = {1340-1356},
year = {2012},
note = {Special Section on Software Reliability and Security},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001346},
author = {Iván Santiago and Álvaro Jiménez and Juan Manuel Vara and Valeria {De Castro} and Verónica A. Bollati and Esperanza Marcos},
keywords = {Traceability, Model-Driven Engineering, Systematic literature review},
abstract = {Context
Model-Driven Engineering provides a new landscape for dealing with traceability in software development.
Objective
Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering.
Method
We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments.
Results
Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals.
Conclusion
The evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information.}
}
@article{KITCHENHAM20132049,
title = {A systematic review of systematic review process research in software engineering},
journal = {Information and Software Technology},
volume = {55},
number = {12},
pages = {2049-2075},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913001560},
author = {Barbara Kitchenham and Pearl Brereton},
keywords = {Systematic review, Systematic literature review, Systematic review methodology, Mapping study},
abstract = {Context
Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research.
Objective
To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process.
Method
We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools.
Results
We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult.
Conclusion
We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem.}
}
@article{MARTINS201671,
title = {Requirements engineering for safety-critical systems: A systematic literature review},
journal = {Information and Software Technology},
volume = {75},
pages = {71-89},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300568},
author = {Luiz Eduardo G. Martins and Tony Gorschek},
keywords = {Safety requirements, Safety-critical systems, Hazard, Accident, Systematic literature review, Requirements engineering},
abstract = {Context
Safety-Critical Systems (SCS) are becoming increasingly present in our society. A considerable amount of research effort has been invested into improving the SCS requirements engineering process as it is critical to the successful development of SCS and, in particular, the engineering of safety aspects.
Objective
This article aims to investigate which approaches have been proposed to elicit, model, specify and validate safety requirements in the context of SCS, as well as to what extent such approaches have been validated in industrial settings. The paper will also investigate how the usability and usefulness of the reported approaches have been explored, and to what extent they enable requirements communication among the development project/team actors in the development of SCS.
Method
We conducted a systematic literature review by selecting 151 papers published between 1983 and 2014. The research methodology to conduct the SLR was based on the guidelines proposed by Kitchenham and Biolchini.
Results
The results of this systematic review should encourage further research into the design of studies to improve the requirements engineering for SCS, particularly to enable the communication of the safety requirements among the project team actors, and the adoption of other models for hazard and accident models. The presented results point to the need for more industry-oriented studies, particularly with more participation of practitioners in the validation of new approaches.
Conclusion
The most relevant findings from this review and their implications for further research are as follows: integration between requirements engineering and safety engineering areas; dominance of the traditional approaches; early mortality of new approaches; need for industry validation; lack of evidence for the usefulness and usability of most approaches; and the lack of studies that investigate how to improve the communication process throughout the lifecycle. Based on the findings, we suggest a research agenda to the community of researchers and advices to SCS practitioners.}
}
@article{NASS2021106625,
title = {Why many challenges with GUI test automation (will) remain},
journal = {Information and Software Technology},
volume = {138},
pages = {106625},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106625},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000963},
author = {Michel Nass and Emil Alégroth and Robert Feldt},
keywords = {System testing, GUI testing, Test automation, Systematic literature review},
abstract = {Context:
Automated testing is ubiquitous in modern software development and used to verify requirement conformance on all levels of system abstraction, including the system’s graphical user interface (GUI). GUI-based test automation, like other automation, aims to reduce the cost and time for testing compared to alternative, manual approaches. Automation has been successful in reducing costs for other forms of testing (like unit- or integration testing) in industrial practice. However, we have not yet seen the same convincing results for automated GUI-based testing, which has instead been associated with multiple technical challenges. Furthermore, the software industry has struggled with some of these challenges for more than a decade with what seems like only limited progress.
Objective:
This systematic literature review takes a longitudinal perspective on GUI test automation challenges by identifying them and then investigating why the field has been unable to mitigate them for so many years.
Method:
The review is based on a final set of 49 publications, all reporting empirical evidence from practice or industrial studies. Statements from the publications are synthesized, based on a thematic coding, into 24 challenges related to GUI test automation.
Results:
The most reported challenges were mapped chronologically and further analyzed to determine how they and their proposed solutions have evolved over time. This chronological mapping of reported challenges shows that four of them have existed for almost two decades.
Conclusion:
Based on the analysis, we discuss why the key challenges with GUI-based test automation are still present and why some will likely remain in the future. For others, we discuss possible ways of how the challenges can be addressed. Further research should focus on finding solutions to the identified technical challenges with GUI-based test automation that can be resolved or mitigated. However, in parallel, we also need to acknowledge and try to overcome non-technical challenges.}
}
@article{VANDINTER2021106589,
title = {Automation of systematic literature reviews: A systematic literature review},
journal = {Information and Software Technology},
volume = {136},
pages = {106589},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106589},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000690},
author = {Raymon {van Dinter} and Bedir Tekinerdogan and Cagatay Catal},
keywords = {Systematic literature review (SLR), Automation, Review, Text mining, Machine learning, Natural language processing},
abstract = {Context
Systematic Literature Review (SLR) studies aim to identify relevant primary papers, extract the required data, analyze, and synthesize results to gain further and broader insight into the investigated domain. Multiple SLR studies have been conducted in several domains, such as software engineering, medicine, and pharmacy. Conducting an SLR is a time-consuming, laborious, and costly effort. As such, several researchers developed different techniques to automate the SLR process. However, a systematic overview of the current state-of-the-art in SLR automation seems to be lacking.
Objective
This study aims to collect and synthesize the studies that focus on the automation of SLR to pave the way for further research.
Method
A systematic literature review is conducted on published primary studies on the automation of SLR studies, in which 41 primary studies have been analyzed.
Results
This SLR identifies the objectives of automation studies, application domains, automated steps of the SLR, automation techniques, and challenges and solution directions.
Conclusion
According to our study, the leading automated step is the Selection of Primary Studies. Although many studies have provided automation approaches for systematic literature reviews, no study has been found to apply automation techniques in the planning and reporting phase. Further research is needed to support the automation of the other activities of the SLR process.}
}
@article{ELSHARKAWY20191,
title = {Metrics for analyzing variability and its implementation in software product lines: A systematic literature review},
journal = {Information and Software Technology},
volume = {106},
pages = {1-30},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301873},
author = {Sascha El-Sharkawy and Nozomi Yamagishi-Eichler and Klaus Schmid},
keywords = {Software product lines, SPL, Metrics, Implementation, Systematic literature review},
abstract = {Context: Software Product Line (SPL) development requires at least concepts for variability implementation and variability modeling for deriving products from a product line. These variability implementation concepts are not required for the development of single systems and, thus, are not considered in traditional software engineering. Metrics are well established in traditional software engineering, but existing metrics are typically not applicable to SPLs as they do not address variability management. Over time, various specialized product line metrics have been described in literature, but no systematic description of these metrics and their characteristics is currently available. Objective: This paper describes and analyzes variability-aware metrics, designed for the needs of software product lines. More precisely we restrict the scope of our study explicitly to metrics designed for variability models, code artifacts, and metrics taking both kinds of artifacts into account. Further, we categorize the purpose for which these metrics were developed. We also analyze to what extent these metrics were evaluated to provide a basis for researchers for selecting adequate metrics. Method: We conducted a systematic literature review to identify variability-aware implementation metrics. We discovered 42 relevant papers reporting metrics intended to measure aspects of variability models or code artifacts. Results: We identified 57 variability model metrics, 34 annotation-based code metrics, 46 code metrics specific to composition-based implementation techniques, and 10 metrics integrating information from variability model and code artifacts. For only 31 metrics, an evaluation was performed assessing their suitability to draw any qualitative conclusions. Conclusions: We observed several problematic issues regarding the definition and the use of the metrics. Researchers and practitioners benefit from the catalog of variability-aware metrics, which is the first of its kind. Also, the research community benefits from the identified observations in order to avoid those problems when defining new metrics.}
}
@article{DURAN2019156,
title = {Reusability in goal modeling: A systematic literature review},
journal = {Information and Software Technology},
volume = {110},
pages = {156-173},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300515},
author = {Mustafa Berk Duran and Gunter Mussbacher},
keywords = {Goal model, Reuse, Context, Requirements reuse, Model-driven requirements engineering, Systematic literature review},
abstract = {Context: Goal modeling is an important instrument for the elicitation, specification, analysis, and validation of early requirements. Goal models capture hierarchical representations of stakeholder objectives, requirements, possible solutions, and their relationships to help requirements engineers understand stakeholder goals and explore solutions based on their impact on these goals. To reuse a goal model and benefit from the strengths of goal modeling, we argue that it is necessary (i) to make sure that analysis and validation of goal models is possible through reuse hierarchies, (ii) to provide the means to delay decision making to a later point in the reuse hierarchy, (iii) to take constraints imposed by other modeling notations into account during analysis, (iv) to allow context dependent information to be modeled so that the goal model can be used in various reuse contexts, and (v) to provide an interface for reuse. Objective: In this two-part systematic literature review, we (i) evaluate how well existing goal modeling approaches support reusability with our five desired characteristics of contextual and reusable goal models, (ii) categorize these approaches based on language constructs for context modeling and connection to other modeling formalisms, and then (iii) draw our conclusions on future research themes. Method: Following guidelines by Kitchenham, the review is conducted on seven major academic search engines. Research questions, inclusion criteria, and categorization criteria are specified, and threats to validity are discussed. A final list of 146 publications and 34 comparisons/assessments of goal modeling approaches is discussed in more detail. Results: Five major research themes are derived to realize reusable goal models with context dependent information. Conclusion: The results indicate that existing goal modeling approaches do not fully address the required capabilities for reusability in different contexts and that further research is needed to fill this gap in the landscape of goal modeling approaches.}
}
@article{MACHADO2019122,
title = {State of the art in hybrid strategies for context reasoning: A systematic literature review},
journal = {Information and Software Technology},
volume = {111},
pages = {122-130},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S095058491830106X},
author = {Roger S. Machado and Ricardo B. Almeida and Ana Marilza Pernas and Adenauer C. Yamin},
keywords = {Context awareness, Reasoning strategy, Hybrid context reasoning, Internet of Things},
abstract = {Context
Several strategies have been used to implement context reasoning, and a strategy that can be applied satisfactorily in different smart systems applications has not yet been found. Because of this, hybrid proposals for context reasoning are gaining prominence. These proposals allow the combination of two or more strategies.
Objective
This work aims to identify the state of the art in the context awareness field, considering papers that use hybrid strategies for context reasoning.
Method
A Systematic Literature Review was explored, contributing to the identification of relevant works in the field, as well as the specification of criteria for its selection. In this review, we analyzed papers published between 2004 and 2018.
Results
During the process, we identified 3241 papers. After applying filtering and conditioning processes, ten papers about hybrid strategies for context reasoning were selected. We described, discussed, and compared the selected papers.
Conclusion
The Systematic Literature Review showed that some researchers explore hybrid proposals, but these proposals do not offer flexibility regarding the reasoning strategies used. Thus, we noted that research efforts related to the topic are still necessary, mainly focusing on the development of dynamic approaches that allow the applications to choose how they want to use the different resources available.}
}
@article{THEISEN201894,
title = {Attack surface definitions: A systematic literature review},
journal = {Information and Software Technology},
volume = {104},
pages = {94-103},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301514},
author = {Christopher Theisen and Nuthan Munaiah and Mahran Al-Zyoud and Jeffrey C. Carver and Andrew Meneely and Laurie Williams},
keywords = {Attack surface, Vulnerabilities, Software engineering, Systematic literature review},
abstract = {Context
Michael Howard conceptualized the attack surface of a software system as a metaphor for risk assessment during the development and maintenance of software. While the phrase attack surface is used in a variety of contexts in cybersecurity, professionals have different conceptions of what the phrase means.
Objective
The goal of this systematic literature review is to aid researchers and practitioners in reasoning about security in terms of attack surface by exploring various definitions of the phrase attack surface.
Method
We reviewed 644 works from prior literature, including research papers, magazine articles, and technical reports, that use the phrase attack surface and categorized them into those that provided their own definition; cited another definition; or expected the reader to intuitively understand the phrase.
Results
In our study, 71% of the papers used the phrase without defining it or citing another paper. Additionally, we found six themes of definitions for the phrase attack surface.
Conclusion
Based on our analysis, we recommend practitioners choose a definition of attack surface appropriate for their domain based on the six themes we identified in our study.}
}
@article{IMTIAZ20191,
title = {A systematic literature review of test breakage prevention and repair techniques},
journal = {Information and Software Technology},
volume = {113},
pages = {1-19},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300990},
author = {Javaria Imtiaz and Salman Sherin and Muhammad Uzair Khan and Muhammad Zohaib Iqbal},
keywords = {Test case repair, Regression testing, Automated testing, Systematic literature review},
abstract = {Context
When an application evolves, some of the developed test cases break. Discarding broken test cases causes a significant waste of effort and leads to test suites that are less effective and have lower coverage. Test repair approaches evolve test suites along with applications by repairing the broken test cases.
Objective
Numerous studies are published on test repair approaches every year. It is important to summarise and consolidate the existing knowledge in the area to provide directions to researchers and practitioners. This research work provides a systematic literature review in the area of test case repair and breakage prevention, aiming to guide researchers and practitioners in the field of software testing.
Method
We followed the standard protocol for conducting a systematic literature review. First, research goals were defined using the Goal Question Metric (GQM). Then we formulate research questions corresponding to each goal. Finally, metrics are extracted from the included papers. Based on the defined selection criteria a final set of 41 primary studies are included for analysis.
Results
The selection process resulted in 5 journal papers, and 36 conference papers. We present a taxonomy that lists the causes of test case breakages extracted from the literature. We found that only four proposed test repair tools are publicly available. Most studies evaluated their approaches on open-source case studies.
Conclusion
There is significant room for future research on test repair techniques. Despite the positive trend of evaluating approaches on large scale open source studies, there is a clear lack of results from studies done in a real industrial context. Few tools are publicly available which lowers the potential of adaption by industry practitioners.}
}
@article{ALYAHYA2020106363,
title = {Crowdsourced software testing: A systematic literature review},
journal = {Information and Software Technology},
volume = {127},
pages = {106363},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106363},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301312},
author = {Sultan Alyahya},
keywords = {Crowdsourcing, Software testing, Crowdsourced software testing, Empirical software engineering, Systematic literature review},
abstract = {Context
Crowdsourced software testing (CST) refers to the use of crowdsourcing techniques in the domain of software testing. CST is an emerging area with its applications rapidly increasing in the last decade.
Objective
A comprehensive review on CST has been conducted to determine the current studies aiming to improve and assess the value of using CST as well as the challenges identified by these evaluation studies.
Method
We conducted a systematic literature review on CST by searching six popular databases. We identified 50 primary studies that passed our quality assessment criteria and defined two research questions covering the aim of the study.
Results
There are three main process activities that the current literature aims to improve, namely selection of suitable testers, reporting of defects, and validation of submitted defects. In addition, there are 23 CST evaluation studies and most of them involve a large group and real crowd. These studies have identified 27 different challenges encountered during the application of crowdsourcing in software testing.
Conclusions
The improvements achieved for the specific process activities in CST help explore other unexplored process activities. Similarly, knowing the characteristics of the evaluation studies can direct us on what other studies are worth investigating. Additionally, many of the challenges identified by the evaluation studies represent research problems that need better understanding and alternative solutions. This research also offers opportunities for practitioners to understand and apply new solutions proposed in the literature and the variations between them. Moreover, it provides awareness to the related parties regarding the challenges reported in the literature, which they may encounter during CST tasks.}
}
@article{CHEN2011344,
title = {A systematic review of evaluation of variability management approaches in software product lines},
journal = {Information and Software Technology},
volume = {53},
number = {4},
pages = {344-362},
year = {2011},
note = {Special section: Software Engineering track of the 24th Annual Symposium on Applied Computing},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002223},
author = {Lianping Chen and Muhammad {Ali Babar}},
keywords = {Software product line, Variability management, Systematic literature reviews, Empirical studies},
abstract = {Context
Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated.
Objective
The objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches.
Method
We carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007.
Results
We selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects.
Conclusion
The findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial.}
}
@article{CHACONLUNA2020106389,
title = {Empirical software product line engineering: A systematic literature review},
journal = {Information and Software Technology},
volume = {128},
pages = {106389},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106389},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301555},
author = {Ana Eva Chacón-Luna and Antonio Manuel Gutiérrez and José A. Galindo and David Benavides},
keywords = {Software product lines, Empirical strategies, Case study, Experiment, Systematic literature review},
abstract = {Context:
The adoption of Software Product Line Engineering (SPLE) is usually only based on its theoretical benefits instead of empirical evidences. In fact, there is no work that synthesizes the empirical studies on SPLE. This makes it difficult for researchers to base their contributions on previous works validated with an empirical strategy.
Objective:
The objective of this work is to discover and summarize the studies that have used empirical evidences in SPLE limited to those ones with the intervention of humans. This will allow evaluating the quality and to know the scope of these studies over time. Doing so, research opportunities can arise
Methods:
A systematic literature review was conducted. The scope of the work focuses on those studies in which there is human intervention and were published between 2000 and 2018. We considered peer-reviewed papers from journals and top software engineering conferences.
Results:
Out of a total of 1880 studies in the initial set, a total of 62 primary studies were selected after applying a series of inclusion and exclusion criteria. We found that, approximately 56% of the studies used the empirical case study strategy while the rest used experimental strategies. Around 86% of the case studies were performed in an industrial environment showing the penetration of SPLE in industry.
Conclusion:
The interest of empirical studies has been growing since 2008. Around 95.16% of the studies address aspects related to domain engineering while application engineering received less attention. Most of the experiments and case study evaluated showed an acceptable level of quality. The first study found dates from 2005 and since then, the interest in the empirical SPLE has increased.}
}
@article{ABOUZAHRA2020106316,
title = {Model composition in Model Driven Engineering: A systematic literature review},
journal = {Information and Software Technology},
volume = {125},
pages = {106316},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106316},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300689},
author = {Anas Abouzahra and Ayoub Sabraoui and Karim Afdel},
keywords = {Model composition, Model Driven Engineering, Systematic literature review},
abstract = {Context
Model Driven Engineering (MDE) aims to alleviate complexity and improve reusability in software development. The development of complex software implies to divide it into independent parts before then assembled. This is how the problem of model composition has become an interesting and stills an emerging topic in MDE.
Objective
Our goal is to analyze the current state of the art in model composition in the context of Model Driven Engineering.
Method
We use the systematic literature review based on the guidelines proposed by Biolchini et al., Brereton et al., and Kitchenham and Charters. We propose five research questions and six quality assessments.
Results
Of the 9270 search results, 56 have been considered relevant studies. These studies have resulted in 36 primary studies.
Conclusion
The evaluation shows that most of approaches allow more than two models as inputs of the composition, allow composing heterogeneous models and enable the tuning of the composition schema, while the important limitations are about the maturity of implementations and the lack on the management of future evolutions or backwards compatibility.}
}
@article{MOURAO2020106294,
title = {On the performance of hybrid search strategies for systematic literature reviews in software engineering},
journal = {Information and Software Technology},
volume = {123},
pages = {106294},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106294},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300446},
author = {Erica Mourão and João Felipe Pimentel and Leonardo Murta and Marcos Kalinowski and Emilia Mendes and Claes Wohlin},
keywords = {Systematic literature review, Search Strategy, Database Search, Snowballing, Software Engineering},
abstract = {Context
When conducting a Systematic Literature Review (SLR), researchers usually face the challenge of designing a search strategy that appropriately balances result quality and review effort. Using digital library (or database) searches or snowballing alone may not be enough to achieve high-quality results. On the other hand, using both digital library searches and snowballing together may increase the overall review effort.
Objective
The goal of this research is to propose and evaluate hybrid search strategies that selectively combine database searches with snowballing.
Method
We propose four hybrid search strategies combining database searches in digital libraries with iterative, parallel, or sequential backward and forward snowballing. We simulated the strategies over three existing SLRs in SE that adopted both database searches and snowballing. We compared the outcome of digital library searches, snowballing, and hybrid strategies using precision, recall, and F-measure to investigate the performance of each strategy.
Results
Our results show that, for the analyzed SLRs, combining database searches from the Scopus digital library with parallel or sequential snowballing achieved the most appropriate balance of precision and recall.
Conclusion
We put forward that, depending on the goals of the SLR and the available resources, using a hybrid search strategy involving a representative digital library and parallel or sequential snowballing tends to represent an appropriate alternative to be used when searching for evidence in SLRs.}
}
@article{SHAMSUJJOHA2021106693,
title = {Developing Mobile Applications Via Model Driven Development: A Systematic Literature Review},
journal = {Information and Software Technology},
volume = {140},
pages = {106693},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106693},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001488},
author = {Md. Shamsujjoha and John Grundy and Li Li and Hourieh Khalajzadeh and Qinghua Lu},
keywords = {Systematic Literature Review, Model Driven Development, Mobile App, Tools and Techniques},
abstract = {Context:
Mobile applications (known as “apps”) usage continues to rapidly increase, with many new apps being developed and deployed. However, developing a mobile app is challenging due to its dependencies on devices, technologies, platforms, and deadlines to reach the market. One potential approach is to use Model Driven Development (MDD) techniques that simplify the app development process, reduce complexity, increase abstraction level, help achieve scalable solutions and maximize cost-effectiveness and productivity.
Objective:
This paper systematically investigates what MDD techniques and methodologies have been used to date to support mobile app development and how these techniques have been employed, to identify key benefits, limitations, gaps and future research potential.
Method:
A Systematic Literature Review approach was used for this study based on a formal protocol. The rigorous search protocol identified a total of 1,042 peer-reviewed academic research papers from four major software engineering databases. These papers were subsequently filtered, and 55 high quality relevant studies were selected for analysis, synthesis, and reporting.
Results:
We identified the popularity of different applied MDD approaches, supporting tools, artifacts, and evaluation techniques. Our analysis found that architecture, domain model, and code generation are the most crucial purposes in MDD-based app development. Three qualities – productivity, scalability and reliability – can benefit from these modeling strategies. We then summarize the key collective strengths, limitations, gaps from the studies and made several future recommendations.
Conclusion:
There has been a steady interest in MDD approaches applied to mobile app development over the years. This paper guides future researchers, developers, and stakeholders to improve app development techniques, ultimately that will help end-users in having more effective apps, especially when some recommendations are addressed, e.g., taking into account more human-centric aspects in app development.}
}
@article{TAUSAN201782,
title = {Choreography in the embedded systems domain: A systematic literature review},
journal = {Information and Software Technology},
volume = {91},
pages = {82-101},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304469},
author = {Nebojša Taušan and Jouni Markkula and Pasi Kuvaja and Markku Oivo},
keywords = {Choreography, Service-oriented architecture, Embedded systems, Systematic literature review},
abstract = {Context
Software companies that develop their products on a basis of service-oriented architecture can expect various improvements as a result of choreography. Current choreography practices, however, are not yet used extensively in the embedded systems domain even though service-oriented architecture is increasingly used in this domain.
Objective
The objective of this study is to identify current features of the use of choreography in the embedded systems domain for practitioners and researchers by systematically analysing current developments in the scientific literature, strategies for choreography adaption, choreography specification and execution types, and implicit assumptions about choreography.
Method
To fulfil this objective, a systematic literature review of scientific publications that focus on the use of choreography in the embedded systems domain was carried out. After a systematic screening of 6823 publications, 48 were selected as primary studies and analysed using thematic synthesis.
Results
The main results of the study showed that there are differences in how choreography is used in embedded and non-embedded systems domain. In the embedded systems domain, it is used to capture the service interactions of a single organisation, while, for example, in the enterprise systems domain it captures the service interactions among multiple organisations. Additionally, the results indicate that the use of choreography can lead to improvements in system performance and that the languages that are used for choreography modelling in the embedded systems domain are insufficiently expressive to capture the complexities that are typical in this domain.
Conclusion
The selection of the key information resources and the identified gaps in the existing literature offer researchers a foundation for further investigations and contribute to the advancement of the use of choreography in the embedded systems domain. The study results facilitate the work of practitioners by allowing them to make informed decisions about the applicability of choreography in their organisations.}
}
@article{HASSLER2016122,
title = {Identification of SLR tool needs – results of a community workshop},
journal = {Information and Software Technology},
volume = {70},
pages = {122-129},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001779},
author = {Edgar Hassler and Jeffrey C. Carver and David Hale and Ahmed Al-Zubidy},
keywords = {Systematic literature review, Community workshops, Research infrastructure, Tool features},
abstract = {Context: With the increasing popularity of the Systematic Literature Review (SLR) process, there is also an increasing need for tool support. Objective:The goal of this work was to consult the software engineering researchers who conduct SLRs to identify and prioritize the necessary SLR tool features. Method: To gather information required to address this goal, we invited SLR authors to participate in an interactive 2 h workshop structured around the Nominal Group Technique. Results: The workshop outcomes indicated that Search & Selection and Collaboration are the two highest priority tool features. The results also showed that most of the high-priority features are not well-supported in current tools. Conclusion: These results support and extend the results of prior work. SLR tool authors can use these findings to guide future development efforts.}
}
@article{ZHANG20131341,
title = {Systematic reviews in software engineering: An empirical investigation},
journal = {Information and Software Technology},
volume = {55},
number = {7},
pages = {1341-1354},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912002029},
author = {He Zhang and Muhammad {Ali Babar}},
keywords = {Systematic (literature) reviews, Evidence-based software engineering, Research methodology, Methodology adoption, Mixed-methods research, Tertiary study},
abstract = {Background
Systematic Literature Reviews (SLRs) have gained significant popularity among Software Engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and methodological infrastructure to support SLRs in SE. We argue that there is also an apparent and essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE.
Objective
The main objective of this research is to empirically investigate the adoption, value, and use of SLRs in SE research from various perspectives.
Method
We used mixed-methods approach (systematically integrating tertiary literature review, semi-structured interviews and questionnaire-based survey) as it is based on a combination of complementary research methods which are expected to compensate each others’ limitations.
Results
A large majority of the participants are convinced of the value of using a rigourous and systematic methodology for literature reviews in SE research. However, there are concerns about the required time and resources for SLRs. One of the most important motivators for performing SLRs is new findings and inception of innovative ideas for further research. The reported SLRs are more influential compared to the traditional literature reviews in terms of number of citations. One of the main challenges of conducting SLRs is drawing a balance between methodological rigour and required effort.
Conclusions
SLR has become a popular research methodology for conducting literature review and evidence aggregation in SE. There is an overall positive perception about this relatively new methodology to SE research. The findings provide interesting insights into different aspects of SLRs. We expect that the findings can provide valuable information to readers about what can be expected from conducting SLRs and the potential impact of such reviews.}
}
@article{CABALLEROESPINOSA2022107078,
title = {Community smells—The sources of social debt: A systematic literature review},
journal = {Information and Software Technology},
pages = {107078},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107078},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001872},
author = {Eduardo Caballero-Espinosa and Jeffrey C. Carver and Kimberly Stowers},
keywords = {Community smells, Social debt, Software development teams, Systematic literature review, Teamwork, Team performance},
abstract = {Context:
Social debt describes the accumulation of unforeseen project costs (or potential costs) from sub-optimal software development processes. Community smells are sociotechnical anti-patterns and one source of social debt. Because community smells impact software teams, development processes, outcomes, and organizations, we to understand their impact on software engineering.
Objective:
To provide an overview of community smells in social debt, based on published literature, and describe future research.
Method:
We conducted a systematic literature review (SLR) to identify properties, understand origins and evolution, and describe the emergence of community smells. This SLR explains the impact of community smells on teamwork and team performance.
Results:
We include 25 studies. Social debt describes the impacts of poor socio-technical decisions on work environments, people, software products, and society. For each of the 30 community smells identified as sources of social debt, we provide a detailed description, management approaches, organizational strategies, and mitigation effectiveness. We identify five groups of management approaches: organizational strategies, frameworks, models, tools, and guidelines. We describe 11 common properties of community smells. We develop the Community Smell Stages Framework to concisely describe the origin and evolution of community smells. We then describe the causes and effects for each community smell. We identify and describe 8 types of causes and 11 types of effects related to the community smells. Finally, we provide 8 comprehensive Sankey diagrams that offer insights into threats the community smells pose to teamwork factors and team performance.
Conclusion:
Community smells explain the influence work conditions have on software developers. The literature is scarce and focuses on a small number of community smells. Thus, the community smells still need more research. This review helps by organizing the state of the art about community smells. Our contributions provide motivations for future research and provide educational material for software engineering professionals.}
}
@article{GONZALEZ2014821,
title = {Formal verification of static software models in MDE: A systematic review},
journal = {Information and Software Technology},
volume = {56},
number = {8},
pages = {821-838},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000627},
author = {Carlos A. González and Jordi Cabot},
keywords = {MDE, Formal verification, OCL, Systematic literature review},
abstract = {Context
Model-driven Engineering (MDE) promotes the utilization of models as primary artifacts in all software engineering activities. Therefore, mechanisms to ensure model correctness become crucial, specially when applying MDE to the development of software, where software is the result of a chain of (semi)automatic model transformations that refine initial abstract models to lower level ones from which the final code is eventually generated. Clearly, in this context, an error in the model/s is propagated to the code endangering the soundness of the resulting software. Formal verification of software models is a promising approach that advocates the employment of formal methods to achieve model correctness, and it has received a considerable amount of attention in the last few years.
Objective
The objective of this paper is to analyze the state of the art in the field of formal verification of models, restricting the analysis to those approaches applied over static software models complemented or not with constraints expressed in textual languages, typically the Object Constraint Language (OCL).
Method
We have conducted a Systematic Literature Review (SLR) of the published works in this field, describing their main characteristics.
Results
The study is based on a set of 48 resources that have been grouped in 18 different approaches according to their affinity. For each of them we have analyzed, among other issues, the formalism used, the support given to OCL, the correctness properties addressed or the feedback yielded by the verification process.
Conclusions
One of the most important conclusions obtained is that current model verification approaches are strongly influenced by the support given to OCL. Another important finding is that in general, current verification tools present important flaws like the lack of integration into the model designer tool chain or the lack of efficiency when verifying large, real-life models.}
}
@article{WOHLIN2013919,
title = {Systematic literature reviews in software engineering},
journal = {Information and Software Technology},
volume = {55},
number = {6},
pages = {919-920},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913000359},
author = {Claes Wohlin and Rafael Prikladnicki}
}
@article{LANE2011424,
title = {Process models for service-based applications: A systematic literature review},
journal = {Information and Software Technology},
volume = {53},
number = {5},
pages = {424-439},
year = {2011},
note = {Special Section on Best Papers from XP2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002211},
author = {Stephen Lane and Ita Richardson},
keywords = {SOA, Service-based application, Software process, Systematic literature review},
abstract = {Context
Service-Oriented Computing (SOC) is a promising computing paradigm which facilitates the development of adaptive and loosely coupled service-based applications (SBAs). Many of the technical challenges pertaining to the development of SBAs have been addressed, however, there are still outstanding questions relating to the processes required to develop them.
Objective
The objective of this study is to systematically identify process models for developing service-based applications (SBAs) and review the processes within them. This will provide a useful starting point for any further research in the area. A secondary objective of the study is to identify process models which facilitate the adaptation of SBAs.
Method
In order to achieve this objective a systematic literature review (SLR) of the existing software engineering literature is conducted.
Results
During this research 722 studies were identified using a predefined search strategy, this number was narrowed down to 57 studies based on a set of strict inclusion and exclusion criteria. The results are reported both quantitatively in the form of a mapping study, as well as qualitatively in the form of a narrative summary of the key processes identified.
Conclusion
There are many process models reported for the development of SBAs varying in detail and maturity, this review has identified and categorised the processes within those process models. The review has also identified and evaluated process models which facilitate the adaptation of SBAs.}
}

@article{HECKMAN2011363,
title = {A systematic literature review of actionable alert identification techniques for automated static code analysis},
journal = {Information and Software Technology},
volume = {53},
number = {4},
pages = {363-387},
year = {2011},
note = {Special section: Software Engineering track of the 24th Annual Symposium on Applied Computing},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002235},
author = {Sarah Heckman and Laurie Williams},
keywords = {Automated static analysis, Systematic literature review, Actionable alert identification, Unactionable alert mitigation, Warning prioritization, Actionable alert prediction},
abstract = {Context
Automated static analysis (ASA) identifies potential source code anomalies early in the software development lifecycle that could lead to field failures. Excessive alert generation and a large proportion of unimportant or incorrect alerts (unactionable alerts) may cause developers to reject the use of ASA. Techniques that identify anomalies important enough for developers to fix (actionable alerts) may increase the usefulness of ASA in practice.
Objective
The goal of this work is to synthesize available research results to inform evidence-based selection of actionable alert identification techniques (AAIT).
Method
Relevant studies about AAITs were gathered via a systematic literature review.
Results
We selected 21 peer-reviewed studies of AAITs. The techniques use alert type selection; contextual information; data fusion; graph theory; machine learning; mathematical and statistical models; or dynamic detection to classify and prioritize actionable alerts. All of the AAITs are evaluated via an example with a variety of evaluation metrics.
Conclusion
The selected studies support (with varying strength), the premise that the effective use of ASA is improved by supplementing ASA with an AAIT. Seven of the 21 selected studies reported the precision of the proposed AAITs. The two studies with the highest precision built models using the subject program’s history. Precision measures how well a technique identifies true actionable alerts out of all predicted actionable alerts. Precision does not measure the number of actionable alerts missed by an AAIT or how well an AAIT identifies unactionable alerts. Inconsistent use of evaluation metrics, subject programs, and ASAs in the selected studies preclude meta-analysis and prevent the current results from informing evidence-based selection of an AAIT. We propose building on an actionable alert identification benchmark for comparison and evaluation of AAIT from literature on a standard set of subjects and utilizing a common set of evaluation metrics.}
}
@article{KARVONEN201787,
title = {Systematic literature review on the impacts of agile release engineering practices},
journal = {Information and Software Technology},
volume = {86},
pages = {87-100},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917300678},
author = {Teemu Karvonen and Woubshet Behutiye and Markku Oivo and Pasi Kuvaja},
keywords = {Release engineering, Agile, Continuous integration, Rapid release, Continuous delivery, Continuous deployment},
abstract = {Context
Agile release engineering (ARE) practices are designed to deliver software faster and cheaper to end users; hence, claims of such impacts should be validated by rigorous and relevant empirical studies.
Objective
The study objective was to analyze both direct and indirect impacts of ARE practices as well as to determine how they have been empirically studied.
Method
The study applied the systematic literature review research method. ARE practices were identified in empirical studies by searching articles for “rapid release,” “continuous integration,” “continuous delivery,” and “continuous deployment.” We systematically analyzed 619 articles and selected 71 primary studies for deeper investigation. The impacts of ARE practices were analyzed from three viewpoints: impacts associated with adoption of the practice, prevalence of the practice, and success of software development.
Results
The results indicated that ARE practices can create shorter lead times and better communication within and between development teams. However, challenges and drawbacks were also found in change management, software quality assurance, and stakeholder acceptance. The analysis revealed that 33 out of 71 primary studies were casual experience reports that had neither an explicit research method nor a data collection approach specified, and 23 out of 38 empirical studies applied qualitative methods, such as interviews, among practitioners. Additionally, 12 studies applied quantitative methods, such as mining of software repositories. Only three empirical studies combined these research approaches.
Conclusion
ARE practices can contribute to improved efficiency of the development process. Moreover, release stakeholders can develop a better understanding of the software project's status. Future empirical studies should consider the comprehensive reporting of the context and how the practice is implemented instead of merely referring to usage of the practice. In addition, different stakeholder points of view, such as customer perceptions regarding ARE practices, still clearly require further research.}
}
@article{HAUGE20101133,
title = {Adoption of open source software in software-intensive organizations – A systematic literature review},
journal = {Information and Software Technology},
volume = {52},
number = {11},
pages = {1133-1154},
year = {2010},
note = {Special Section on Best Papers PROMISE 2009},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910000972},
author = {Øyvind Hauge and Claudia Ayala and Reidar Conradi},
keywords = {Open source software, Organizations, Software development, Systematic literature review},
abstract = {Context
Open source software (OSS) is changing the way organizations develop, acquire, use, and commercialize software.
Objective
This paper seeks to identify how organizations adopt OSS, classify the literature according to these ways of adopting OSS, and with a focus on software development evaluate the research on adoption of OSS in organizations.
Method
Based on the systematic literature review method we reviewed publications from 24 journals and seven conference and workshop proceedings, published between 1998 and 2008. From a population of 24,289 papers, we identified 112 papers that provide empirical evidence on how organizations actually adopt OSS.
Results
We show that adopting OSS involves more than simply using OSS products. We moreover provide a classification framework consisting of six distinctly different ways in which organizations adopt OSS. This framework is used to illustrate some of the opportunities and challenges organizations meet when approaching OSS, to show that OSS can be adopted successfully in different ways, and to organize and review existing research. We find that existing research on OSS adoption does not sufficiently describe the context of the organizations studied, and it fails to benefit fully from related research fields. While existing research covers a large number of topics, it contains very few closely related studies. To aid this situation, we offer directions for future research.
Conclusion
The implications of our findings are twofold. On the one hand, practitioners should embrace the many opportunities OSS offers, but consciously evaluate the consequences of adopting it in their own context. They may use our framework and the success stories provided by the literature in their own evaluations. On the other hand, researchers should align their work, and perform more empirical research on topics that are important to organizations. Our framework may be used to position this research and to describe the context of the organization they are studying.}
}
@article{HASELBERGER20161,
title = {A literature-based framework of performance-related leadership interactions in ICT project teams},
journal = {Information and Software Technology},
volume = {70},
pages = {1-17},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001561},
author = {David Haselberger},
keywords = {Leadership, Project team, Team performance, Interpersonal interaction, Classification, Clustering},
abstract = {Context
In Information and Communication Technology (ICT) projects, leadership interactions affect project team performance and so influence the outcomes of projects. While aspects of team performance have been vastly researched, leadership skills and interactions specific to the challenges of ICT project environments remain less investigated.
Objective
A taxonomy of leadership interactions is developed and descriptions of interactions are collected to form a framework of leadership interactions that support team performance in ICT projects.
Method
A systematic literature review was conducted to find resources on supportive leadership interactions in ICT project teams, including categorization schemas of leadership functions and behaviors. The review was limited by the selection of sources to be searched and comprises articles from 1977 until early 2014. The developed taxonomy was compared to benchmark work on team leadership. Risks and challenges of leading ICT project teams as well as descriptions of interactions were collected.
Results
Throughout the search process, 2780 articles were examined, 218 of which were included in the review. 62 articles qualified to be accepted, 156 were rejected. 20 (including 4 meta-studies) papers holding leadership interaction categories were selected as taxonomy papers. The developed taxonomy holds 18 key dimensions of leadership team interaction. 13 areas of risks and challenges were discerned. 262 leadership interaction descriptions relevant in ICT project teams were retrieved.
Conclusions
In collected studies, the categories “systems sensing”, “planning and scheduling”, “coaching”, as well as “monitoring and controlling” were highly developed. Many risks deal with project complexity and interpersonal processes. However, leadership interactions addressing these challenges are scarce, indicating a research gap. Furthermore, prominent leadership team interactions such as monitoring or consulting may need to be adapted to deal with challenges specific to ICT project teams. Few studies describe best-practice examples of leadership interaction in ICT teams.}
}
@article{CADAVID2020106202,
title = {Architecting systems of systems: A tertiary study},
journal = {Information and Software Technology},
volume = {118},
pages = {106202},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106202},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302083},
author = {Héctor Cadavid and Vasilios Andrikopoulos and Paris Avgeriou},
keywords = {Systems of Systems, SoS Architecting, Tertiary study, Systematic literature review},
abstract = {Context: The term System of Systems (SoS) has increasingly been used in a wide variety of domains to describe those systems composed of independent constituent systems that collaborate towards a mission that they could not accomplish on their own. There is a significant volume of research by the software architecture community that aims to overcome the challenges involved in architecting SoS, as evidenced by the number of secondary studies in the field published so far. However, the boundaries of such research do not seem to be well defined, at least partially, due to the emergence of SoS-adjacent areas of interest like the Internet of Things.Objective: This paper aims to investigate the current state of research on SoS architecting by synthesizing the demographic data, assessing the quality and the coverage of architecting activities and software quality attributes by the research, and distilling a concept map that reflects a community-wide understanding of the concept of SoS. Method: We conduct what is, to the best of our understanding, the first tertiary study on SoS architecting. Such tertiary study was based on five research questions, and was performed by following the guidelines of Kitchenham et al. In all, 19 secondary studies were evaluated, which is comparable to other tertiary studies. Results: The study illustrates a state of disconnection in the research community, with research gaps in the coverage of particular phases and quality attributes. Furthermore, a more effective approach in classifying systems as SoS is required, as the means of resolving conceptual and terminological overlaps with the related domains. Conclusions: Despite the amount of research in the area of SoS architecting, more coordinated and systematic targeted efforts are required in order to address the identified issues with the current state of research.}
}
@article{BISSI201645,
title = {The effects of test driven development on internal quality, external quality and productivity: A systematic review},
journal = {Information and Software Technology},
volume = {74},
pages = {45-54},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300222},
author = {Wilson Bissi and Adolfo Gustavo {Serra Seca Neto} and Maria Claudia Figueiredo Pereira Emer},
keywords = {Test-driven development, Productivity, Internal quality, External quality, Systematic review},
abstract = {Context: Test Driven Development (TDD) is an agile practice that has gained popularity when it was defined as a fundamental part in eXtreme Programming (XP). Objective: This study analyzed the conclusions of previously published articles on the effects of TDD on internal and external software quality and productivity, comparing TDD with Test Last Development (TLD). Method: In this study, a systematic literature review has been conducted considering articles published between 1999 and 2014. Results: In about 57% of the analyzed studies, the results were validated through experiments and in 32% of them, validation was performed through a case study. The results of this analysis show that 76% of the studies have identified a significant increase in internal software quality while 88% of the studies identified a meaningful increase in external software quality. There was an increase in productivity in the academic environment, while in the industrial scenario there was a decrease in productivity. Overall, about 44% of the studies indicated lower productivity when using TDD compared to TLD. Conclusion: According to our findings, TDD yields more benefits than TLD for internal and external software quality, but it results in lower developer productivity than TLD.}
}
@article{DASILVAESTACIO20151,
title = {Distributed Pair Programming: A Systematic Literature Review},
journal = {Information and Software Technology},
volume = {63},
pages = {1-10},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000476},
author = {Bernardo José {da Silva Estácio} and Rafael Prikladnicki},
keywords = {Extreme Programming, Distributed Software Development, Pair Programming, Distributed Pair Programming},
abstract = {Context
Geographically distributed teams have adopted agile practices as a work strategy. One of these practices is Distributed Pair Programming (DPP). DPP consists in two developers working remotely on the same design, algorithm or code.
Objective
In this paper we sought to identify and synthesize papers that describe and analyze DPP both from teaching and practice perspectives.
Method
We conducted a Systematic Literature Review to search for empirical evidence in eight digital libraries.
Results
Most of the 34 DPP primary studies identified explore DPP from a teaching perspective. We found that DPP requires a specific infrastructure, but the existing studies do not explore the impact of the distribution in the details. There are many tools proposed that support DPP practice, but few of them are evaluated within a software development team.
Conclusion
We need more studies that explore the effects of Pair Programming in the context of Distributed Software Development, such as coordination and communication. Most of the studies do not empirically evaluate DPP in industry. There is also a need to propose guidelines to use DPP in industry and as a teaching strategy.}
}
@article{FERNANDEZSAEZ20131119,
title = {Empirical studies concerning the maintenance of UML diagrams and their use in the maintenance of code: A systematic mapping study},
journal = {Information and Software Technology},
volume = {55},
number = {7},
pages = {1119-1142},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912002418},
author = {Ana M. Fernández-Sáez and Marcela Genero and Michel R.V. Chaudron},
keywords = {UML, Empirical studies, Software maintenance, Systematic mapping study, Systematic literature review},
abstract = {Context
The Unified Modelling Language (UML) has, after ten years, become established as the de facto standard for the modelling of object-oriented software systems. It is therefore relevant to investigate whether its use is important as regards the costs involved in its implantation in industry being worthwhile.
Method
We have carried out a systematic mapping study to collect the empirical studies published in order to discover “What is the current existing empirical evidence with regard to the use of UML diagrams in source code maintenance and the maintenance of the UML diagrams themselves?
Results
We found 38 papers, which contained 63 experiments and 3 case studies.
Conclusion
Although there is common belief that the use of UML is beneficial for source code maintenance, since the quality of the modifications is greater when UML diagrams are available, only 3 papers concerning this issue have been published. Most research (60 empirical studies) concerns the maintainability and comprehensibility of the UML diagrams themselves which form part of the system’s documentation, since it is assumed that they may influence source code maintainability, although this has not been empirically validated. Moreover, the generalizability of the majority of the experiments is questionable given the material, tasks and subjects used. There is thus a need for more experiments and case studies to be performed in industrial contexts, i.e., with real systems and using maintenance tasks conducted by practitioners under real conditions that truly show the utility of UML diagrams in maintaining code, and that the fact that a diagram is more comprehensible or modifiable influences the maintainability of the code itself. This utility should also be studied from the viewpoint of cost and productivity, and the consistent and simultaneous maintenance of diagrams and code must also be considered in future empirical studies.}
}
@article{VALLON2018161,
title = {Systematic literature review on agile practices in global software development},
journal = {Information and Software Technology},
volume = {96},
pages = {161-180},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302975},
author = {Raoul Vallon and Bernardo José {da Silva Estácio} and Rafael Prikladnicki and Thomas Grechenig},
keywords = {Global software development, Global software engineering, Distributed software development, Agile software development, Agile practices, Scrum, Extreme programming, XP, Systematic literature review},
abstract = {Context
Developing software in distributed development environments exhibits coordination, control and communication challenges. Agile practices, which demand frequent communication and self-organization between remote sites, are increasingly found in global software development (GSD) to mitigate said challenges.
Objective
We aim to provide detailed insight into what is reported on the successful application of agile practices in GSD from 1999 to 2016 and also identify the most frequently applied agile practices and reported distribution scenarios. We further strive to uncover research opportunities and gaps in the field of agile GSD.
Method
We build our systematic literature review on top of a previous review, which investigated studies published between 1999 and 2009, and extend the review by years 2010–2016, for which we conduct both a quantitative and a qualitative analysis.
Results
Our results show that the majority of the cases studied is global and involves complex distribution scenarios with Scrum or combined Scrum/Extreme Programming being the most used agile methods. Key results include that in contrast to 1999–2009, where four Extreme Programming practices were among the ten most frequently used agile practices, in 2010–2016 Scrum is in the center of agile GSD implementations with eight Scrum-based practices in the top ten agile practices used in GSD.
Conclusion
Agile GSD is a maturing research field with higher quality contributions and a greater variety of publication types and methods from 2010 to 2016 than before from 1999 to 2009. However, researchers need to report full empirical contextual details of their studied cases in order to improve the generalizability of results and allow the future creation of stronger frameworks to drive the implementation of agile practices in GSD.}
}
@article{IRSHAD2018223,
title = {A systematic literature review of software requirements reuse approaches},
journal = {Information and Software Technology},
volume = {93},
pages = {223-245},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916303615},
author = {Mohsin Irshad and Kai Petersen and Simon Poulding},
keywords = {Software requirements, Requirements reuse, Rigor, Relevance, Artefact reuse, Reusability},
abstract = {Context
Early software reuse is considered as the most beneficial form of software reuse. Hence, previous research has focused on supporting the reuse of software requirements.
Objective
This study aims to identify and investigate the current state of the art with respect to (a) what requirement reuse approaches have been proposed, (b) the methods used to evaluate the approaches, (c) the characteristics of the approaches, and (d) the quality of empirical studies on requirements reuse with respect to rigor and relevance.
Method
We conducted a systematic review and a combination of snowball sampling and database search have been used to identify the studies. The rigor and relevance scoring rubric has been used to assess the quality of the empirical studies. Multiple researchers have been involved in each step to increase the reliability of the study.
Results
Sixty-nine studies were identified that describe requirements reuse approaches. The majority of the approaches used structuring and matching of requirements as a method to support requirements reuse and text-based artefacts were commonly used as an input to these approaches. Further evaluation of the studies revealed that the majority of the approaches are not validated in the industry. The subset of empirical studies (22 in total) was analyzed for rigor and relevance and two studies achieved the maximum score for rigor and relevance based on the rubric. It was found that mostly text-based requirements reuse approaches were validated in the industry.
Conclusion
From the review, it was found that a number of approaches already exist in literature, but many approaches are not validated in industry. The evaluation of rigor and relevance of empirical studies show that these do not contain details of context, validity threats, and the industrial settings, thus highlighting the need for the industrial evaluation of the approaches.}
}
@article{ENGSTROM20112,
title = {Software product line testing – A systematic mapping study},
journal = {Information and Software Technology},
volume = {53},
number = {1},
pages = {2-13},
year = {2011},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910001709},
author = {Emelie Engström and Per Runeson},
keywords = {Software product line testing, Testing, Systematic mapping, Systematic literature review},
abstract = {Context
Software product lines (SPL) are used in industry to achieve more efficient software development. However, the testing side of SPL is underdeveloped.
Objective
This study aims at surveying existing research on SPL testing in order to identify useful approaches and needs for future research.
Method
A systematic mapping study is launched to find as much literature as possible, and the 64 papers found are classified with respect to focus, research type and contribution type.
Results
A majority of the papers are of proposal research types (64%). System testing is the largest group with respect to research focus (40%), followed by management (23%). Method contributions are in majority.
Conclusions
More validation and evaluation research is needed to provide a better foundation for SPL testing.}
}
@article{GAROUSI201716,
title = {Software test maturity assessment and test process improvement: A multivocal literature review},
journal = {Information and Software Technology},
volume = {85},
pages = {16-42},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917300162},
author = {Vahid Garousi and Michael Felderer and Tuna Hacaloğlu},
keywords = {Software testing, Test management, Test process, Test maturity, Test process assessment, Test process improvement, Multivocal literature review, Systematic literature review},
abstract = {Context
Software testing practices and processes in many companies are far from being mature and are usually conducted in ad-hoc fashions. Such immature practices lead to various negative outcomes, e.g., ineffectiveness of testing practices in detecting all the defects, and cost and schedule overruns of testing activities. To conduct test maturity assessment (TMA) and test process improvement (TPI) in a systematic manner, various TMA/TPI models and approaches have been proposed.
Objective
It is important to identify the state-of-the-art and the –practice in this area to consolidate the list of all various test maturity models proposed by practitioners and researchers, the drivers of TMA/TPI, the associated challenges and the benefits and results of TMA/TPI. Our article aims to benefit the readers (both practitioners and researchers) by providing the most comprehensive survey of the area, to this date, in assessing and improving the maturity of test processes.
Method
To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study to find out what we know about TMA/TPI. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine.
Results
Our MLR and its results are based on 181 sources, 51 (29%) of which were grey literature and 130 (71%) were formally published sources. By summarizing what we know about TMA/TPI, our review identified 58 different test maturity models and a large number of sources with varying degrees of empirical evidence on this topic. We also conducted qualitative analysis (coding) to synthesize the drivers, challenges and benefits of TMA/TPI from the primary sources.
Conclusion
We show that current maturity models and techniques in TMA/TPI provides reasonable advice for industry and the research community. We suggest directions for follow-up work, e.g., using the findings of this MLR in industry-academia collaborative projects and empirical evaluation of models and techniques in the area of TMA/TPI as reported in this article.}
}
@article{GARCIABORGONON2014103,
title = {Software process modeling languages: A systematic literature review},
journal = {Information and Software Technology},
volume = {56},
number = {2},
pages = {103-116},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913001894},
author = {L. García-Borgoñón and M.A. Barcelona and J.A. García-García and M. Alba and M.J. Escalona},
keywords = {Software process modeling, Software process language, Systematic literature review},
abstract = {Context
Organizations working in software development are aware that processes are very important assets as well as they are very conscious of the need to deploy well-defined processes with the goal of improving software product development and, particularly, quality. Software process modeling languages are an important support for describing and managing software processes in software-intensive organizations.
Objective
This paper seeks to identify what software process modeling languages have been defined in last decade, the relationships and dependencies among them and, starting from the current state, to define directions for future research.
Method
A systematic literature review was developed. 1929 papers were retrieved by a manual search in 9 databases and 46 primary studies were finally included.
Results
Since 2000 more than 40 languages have been first reported, each of which with a concrete purpose. We show that different base technologies have been used to define software process modeling languages. We provide a scheme where each language is registered together with the year it was created, the base technology used to define it and whether it is considered a starting point for later languages. This scheme is used to illustrate the trend in software process modeling languages. Finally, we present directions for future research.
Conclusion
This review presents the different software process modeling languages that have been developed in the last ten years, showing the relevant fact that model-based SPMLs (Software Process Modeling Languages) are being considered as a current trend. Each one of these languages has been designed with a particular motivation, to solve problems which had been detected. However, there are still several problems to face, which have become evident in this review. This let us provide researchers with some guidelines for future research on this topic.}
}
@article{TRIPATHI201956,
title = {Insights into startup ecosystems through exploration of multi-vocal literature},
journal = {Information and Software Technology},
volume = {105},
pages = {56-77},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S095058491830168X},
author = {Nirnaya Tripathi and Pertti Seppänen and Ganesh Boominathan and Markku Oivo and Kari Liukkunen},
keywords = {Startup, Ecosystem, Startup ecosystem, Software startup, Multi-vocal literature review, Systematic literature review},
abstract = {Context: Successful startup firms have the ability to create jobs and contribute to economic welfare. A suitable ecosystem developed around startups is important to form and support these firms. In this regard, it is crucial to understand the startup ecosystem, particularly from researchers’ and practitioners’ perspectives. However, a systematic literature research on the startup ecosystem is limited. Objective: In this study, our objective was to conduct a multi-vocal literature review and rigorously find existing studies on the startup ecosystem in order to organize and analyze them, know the definitions and major elements of this ecosystem, and determine the roles of such elements in startups’ product development. Method: We conducted a multi-vocal literature review to analyze relevant articles, which are published technical articles, white papers, and Internet articles that focused on the startup ecosystem. Our search generated 18,310 articles, of which 63 were considered primary candidates focusing on the startup ecosystem. Results: From our analysis of primary articles, we found four definitions of a startup ecosystem. These definitions used common terms, such as stakeholders, supporting organization, infrastructure, network, and region. Out of 63 articles, 34 belonged to the opinion type, with contributions in the form of reports, whereas over 50% had full relevance to the startup ecosystem. We identified eight major elements (finance, demography, market, education, human capital, technology, entrepreneur, and support factors) of a startup ecosystem, which directly or indirectly affected startups. Conclusions: This study aims to provide the state of the art on the startup ecosystem through a multi-vocal literature review. The results indicate that current knowledge on the startup ecosystem is mainly shared by non-peer-reviewed literature, thus signifying the need for more systematic and empirical literature on the topic. Our study also provides some recommendations for future work.}
}
@article{MUNIR2014375,
title = {Considering rigor and relevance when evaluating test driven development: A systematic review},
journal = {Information and Software Technology},
volume = {56},
number = {4},
pages = {375-394},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000135},
author = {Hussan Munir and Misagh Moayyed and Kai Petersen},
keywords = {Test-driven development (TDD), Test-last development (TLD), Internal code quality, External code quality, Productivity},
abstract = {Context
Test driven development (TDD) has been extensively researched and compared to traditional approaches (test last development, TLD). Existing literature reviews show varying results for TDD.
Objective
This study investigates how the conclusions of existing literature reviews change when taking two study quality dimension into account, namely rigor and relevance.
Method
In this study a systematic literature review has been conducted and the results of the identified primary studies have been analyzed with respect to rigor and relevance scores using the assessment rubric proposed by Ivarsson and Gorschek 2011. Rigor and relevance are rated on a scale, which is explained in this paper. Four categories of studies were defined based on high/low rigor and relevance.
Results
We found that studies in the four categories come to different conclusions. In particular, studies with a high rigor and relevance scores show clear results for improvement in external quality, which seem to come with a loss of productivity. At the same time high rigor and relevance studies only investigate a small set of variables. Other categories contain many studies showing no difference, hence biasing the results negatively for the overall set of primary studies. Given the classification differences to previous literature reviews could be highlighted.
Conclusion
Strong indications are obtained that external quality is positively influenced, which has to be further substantiated by industry experiments and longitudinal case studies. Future studies in the high rigor and relevance category would contribute largely by focusing on a wider set of outcome variables (e.g. internal code quality). We also conclude that considering rigor and relevance in TDD evaluation is important given the differences in results between categories and in comparison to previous reviews.}
}
@article{GAROUSI2020106321,
title = {NLP-assisted software testing: A systematic mapping of the literature},
journal = {Information and Software Technology},
volume = {126},
pages = {106321},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106321},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300744},
author = {Vahid Garousi and Sara Bauer and Michael Felderer},
keywords = {Software testing, Natural Language Processing (NLP), Systematic literature mapping, Systematic literature review},
abstract = {Context
To reduce manual effort of extracting test cases from natural-language requirements, many approaches based on Natural Language Processing (NLP) have been proposed in the literature. Given the large amount of approaches in this area, and since many practitioners are eager to utilize such techniques, it is important to synthesize and provide an overview of the state-of-the-art in this area.
Objective
Our objective is to summarize the state-of-the-art in NLP-assisted software testing which could benefit practitioners to potentially utilize those NLP-based techniques. Moreover, this can benefit researchers in providing an overview of the research landscape.
Method
To address the above need, we conducted a survey in the form of a systematic literature mapping (classification). After compiling an initial pool of 95 papers, we conducted a systematic voting, and our final pool included 67 technical papers.
Results
This review paper provides an overview of the contribution types presented in the papers, types of NLP approaches used to assist software testing, types of required input requirements, and a review of tool support in this area. Some key results we have detected are: (1) only four of the 38 tools (11%) presented in the papers are available for download; (2) a larger ratio of the papers (30 of 67) provided a shallow exposure to the NLP aspects (almost no details).
Conclusion
This paper would benefit both practitioners and researchers by serving as an “index” to the body of knowledge in this area. The results could help practitioners utilizing the existing NLP-based techniques; this in turn reduces the cost of test-case design and decreases the amount of human resources spent on test activities. After sharing this review with some of our industrial collaborators, initial insights show that this review can indeed be useful and beneficial to practitioners.}
}
@article{STEINMACHER201567,
title = {A systematic literature review on the barriers faced by newcomers to open source software projects},
journal = {Information and Software Technology},
volume = {59},
pages = {67-85},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914002390},
author = {Igor Steinmacher and Marco Aurelio {Graciotto Silva} and Marco Aurelio Gerosa and David F. Redmiles},
keywords = {Open source software, Newcomers, Joining, Barriers to entry, Onboarding, Systematic literature review},
abstract = {Context
Numerous open source software projects are based on volunteers collaboration and require a continuous influx of newcomers for their continuity. Newcomers face barriers that can lead them to give up. These barriers hinder both developers willing to make a single contribution and those willing to become a project member.
Objective
This study aims to identify and classify the barriers that newcomers face when contributing to open source software projects.
Method
We conducted a systematic literature review of papers reporting empirical evidence regarding the barriers that newcomers face when contributing to open source software (OSS) projects. We retrieved 291 studies by querying 4 digital libraries. Twenty studies were identified as primary. We performed a backward snowballing approach, and searched for other papers published by the authors of the selected papers to identify potential studies. Then, we used a coding approach inspired by open coding and axial coding procedures from Grounded Theory to categorize the barriers reported by the selected studies.
Results
We identified 20 studies providing empirical evidence of barriers faced by newcomers to OSS projects while making a contribution. From the analysis, we identified 15 different barriers, which we grouped into five categories: social interaction, newcomers’ previous knowledge, finding a way to start, documentation, and technical hurdles. We also classified the problems with regard to their origin: newcomers, community, or product.
Conclusion
The results are useful to researchers and OSS practitioners willing to investigate or to implement tools to support newcomers. We mapped technical and non-technical barriers that hinder newcomers’ first contributions. The most evidenced barriers are related to socialization, appearing in 75% (15 out of 20) of the studies analyzed, with a high focus on interactions in mailing lists (receiving answers and socialization with other members). There is a lack of in-depth studies on technical issues, such as code issues. We also noticed that the majority of the studies relied on historical data gathered from software repositories and that there was a lack of experiments and qualitative studies in this area.}
}
@article{MCCLEAN2021106442,
title = {Social network analysis of open source software: A review and categorisation},
journal = {Information and Software Technology},
volume = {130},
pages = {106442},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106442},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301956},
author = {Kelvin McClean and Des Greer and Anna Jurek-Loughrey},
keywords = {Review, Open source software, Social network analysis},
abstract = {Context: As companies have become large users of Open Source Software, it is important that they feel comfortable in their Open Source strategies. One of the critical differences between Open Source and Proprietary Software is the communication networks. Objective: This paper tries to set a base for understanding how open source teams are structured and how they change. This is vital to understanding Open Source Software Communities. Method: The paper looks into previous research on Social Network Analysis of Open Source Software, using a systematic literature review. Papers were gathered from Scopus, IEEEXplore and ACM Digital Library, and used or discarded based on predetermined inclusion and exclusion criteria. Research which focuses on the success factors of Open Source Software through Network Analysis is also examined. Results: A subjective categorisation is established for the papers: Structure, Lifecycle and Communication. It was found that the structure of a project has a large bearing on project success, with developers having previously worked together being indicative of project success. Other structure indicators of success are having a small but structured hierarchy, a diverse user and developer base, and project prominence. However, it was found that information on how these structures appear and evolve over time is lacking, and future research into temporal data models to determine project success information is suggested. Conclusions: A categorisation of existing research on Social Network Analysis is provided as a basis for further research. Further work into the lifecycle of OSS projects through Social Network Analysis of temporal project information is suggested.}
}
@article{LI201767,
title = {Static analysis of android apps: A systematic literature review},
journal = {Information and Software Technology},
volume = {88},
pages = {67-95},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302987},
author = {Li Li and Tegawendé F. Bissyandé and Mike Papadakis and Siegfried Rasthofer and Alexandre Bartel and Damien Octeau and Jacques Klein and Le Traon},
abstract = {Context
Static analysis exploits techniques that parse program source code or bytecode, often traversing program paths to check some program properties. Static analysis approaches have been proposed for different tasks, including for assessing the security of Android apps, detecting app clones, automating test cases generation, or for uncovering non-functional issues related to performance or energy. The literature thus has proposed a large body of works, each of which attempts to tackle one or more of the several challenges that program analyzers face when dealing with Android apps.
Objective
We aim to provide a clear view of the state-of-the-art works that statically analyze Android apps, from which we highlight the trends of static analysis approaches, pinpoint where the focus has been put, and enumerate the key aspects where future researches are still needed.
Method
We have performed a systematic literature review (SLR) which involves studying 124 research papers published in software engineering, programming languages and security venues in the last 5 years (January 2011–December 2015). This review is performed mainly in five dimensions: problems targeted by the approach, fundamental techniques used by authors, static analysis sensitivities considered, android characteristics taken into account and the scale of evaluation performed.
Results
Our in-depth examination has led to several key findings: 1) Static analysis is largely performed to uncover security and privacy issues; 2) The Soot framework and the Jimple intermediate representation are the most adopted basic support tool and format, respectively; 3) Taint analysis remains the most applied technique in research approaches; 4) Most approaches support several analysis sensitivities, but very few approaches consider path-sensitivity; 5) There is no single work that has been proposed to tackle all challenges of static analysis that are related to Android programming; and 6) Only a small portion of state-of-the-art works have made their artifacts publicly available.
Conclusion
The research community is still facing a number of challenges for building approaches that are aware altogether of implicit-Flows, dynamic code loading features, reflective calls, native code and multi-threading, in order to implement sound and highly precise static analyzers.}
}
@article{TRAN2021106620,
title = {Assessing test artifact quality—A tertiary study},
journal = {Information and Software Technology},
volume = {139},
pages = {106620},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106620},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000938},
author = {Huynh Khanh Vi Tran and Michael Unterkalmsteiner and Jürgen Börstler and Nauman bin Ali},
keywords = {Software testing, Test case quality, Test suite quality, Test artifact quality, Quality assurance},
abstract = {Context:
Modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software. This puts high demands on the quality of the central artifacts in software testing, test suites and test cases.
Objective:
We aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives.
Methods:
We have carried out a systematic literature review to identify and analyze existing secondary studies on quality aspects of software testing artifacts.
Results:
We identified 49 relevant secondary studies. Of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results. We present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated. We also provide a comprehensive model of test case/suite quality with definitions for the quality attributes and measurements based on findings in the literature and ISO/IEC 25010:2011.
Conclusion:
The test artifact quality model presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice. Furthermore, the model can also be used as a framework for documenting context characteristics to make research results more accessible for research and practice.}
}
@article{BUDGEN2018234,
title = {The contribution that empirical studies performed in industry make to the findings of systematic reviews: A tertiary study},
journal = {Information and Software Technology},
volume = {94},
pages = {234-244},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917303798},
author = {David Budgen and Pearl Brereton and Nikki Williams and Sarah Drummond},
keywords = {Systematic review, Primary study, Industry study, Case study},
abstract = {Context
Systematic reviews can provide useful knowledge for software engineering practice, by aggregating and synthesising empirical studies related to a specific topic.
Objective
We sought to assess how far the findings of systematic reviews addressing practice-oriented topics have been derived from empirical studies that were performed in industry or that used industry data.
Method
We drew upon and augmented the data obtained from a tertiary study that performed a systematic review of systematic reviews published in the period up to the end of 2015, seeking to identify those with findings that are relevant for teaching and practice. For the supplementary analysis reported here, we then examined the profiles of the primary studies as reported in each systematic review.
Results
We identified 48 systematic reviews as candidates for further analysis. The many differences that arise between systematic reviews, together with the incompleteness of reporting for these, mean that our counts should be treated as indicative rather than definitive. However, even when allowing for problems of classification, the findings from the majority of these systematic reviews were predominantly derived from using primary studies conducted in industry. There was also an emphasis upon the use of case studies, and a number of the systematic reviews also made some use of weaker ‘experience’ or even ‘opinion’ papers.
Conclusions
Primary studies from industry play an important role as inputs to systematic reviews. Using more rigorous industry-based primary studies can give greater authority to the findings of the systematic reviews, and should help with the creation of a corpus of sound empirical data to support evidence-informed decisions.}
}
@article{WEINREICH2016265,
title = {Software architecture knowledge management approaches and their support for knowledge management activities: A systematic literature review},
journal = {Information and Software Technology},
volume = {80},
pages = {265-286},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301707},
author = {Rainer Weinreich and Iris Groher},
keywords = {Software architecture, Software architecture knowledge management, Software architecture knowledge management activities, Software architecture knowledge management approaches, Systematic literature review},
abstract = {Context: Numerous approaches for Software Architecture Knowledge Management (SAKM) have been developed by the research community over the last decade. Still, these approaches have not yet found widespread use in practice. Objective: This work identifies existing approaches to SAKM and analyzes them in terms of their support for central architecture knowledge management activities, i.e., capturing, using, maintaining, sharing, and reuse of architectural knowledge, along with presenting the evidence provided for this support. Method: A systematic literature review has been conducted for identifying and analyzing SAKM approaches, covering work published between January 2004 and August 2015. We identified 56 different approaches to SAKM based on 115 studies. We analyzed each approach in terms of its focus and support for important architecture knowledge management activities and in terms of the provided level of evidence for each supported activity. Results: Most of the developed approaches focus on using already-captured knowledge. Using is also the best-validated activity. The problem of efficient capturing is still not sufficiently addressed, and only a few approaches specifically address reuse, sharing, and, especially, maintaining. Conclusions: Without adequate support for other core architecture knowledge management activities besides using, the adoption of SAKM in practice will remain an elusive target. The problem of efficient capturing is still unsolved, as is the problem of maintaining captured knowledge over the long term. We also need more case studies and replication studies providing evidence for the usefulness of developed support for SAKM activities, as well as better reporting on these case studies.}
}
@article{MAHDAVIHEZAVEHI20171,
title = {A systematic literature review on methods that handle multiple quality attributes in architecture-based self-adaptive systems},
journal = {Information and Software Technology},
volume = {90},
pages = {1-26},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302860},
author = {Sara Mahdavi-Hezavehi and Vinicius H.S. Durelli and Danny Weyns and Paris Avgeriou},
abstract = {Context
Handling multiple quality attributes (QAs) in the domain of self-adaptive systems is an understudied research area. One well-known approach to engineer adaptive software systems and fulfill QAs of the system is architecture-based self-adaptation. In order to develop models that capture the required knowledge of the QAs of interest, and to investigate how these models can be employed at runtime to handle multiple quality attributes, we need to first examine current architecture-based self-adaptive methods.
Objective
In this paper we review the state-of-the-art of architecture-based methods for handling multiple QAs in self-adaptive systems. We also provide a descriptive analysis of the collected data from the literature.
Method
We conducted a systematic literature review by performing an automatic search on 28 selected venues and books in the domain of self-adaptive systems. As a result, we selected 54 primary studies which we used for data extraction and analysis.
Results
Performance and cost are the most frequently addressed set of QAs. Current self-adaptive systems dealing with multiple QAs mostly belong to the domain of robotics and web-based systems paradigm. The most widely used mechanisms/models to measure and quantify QAs sets are QA data variables. After QA data variables, utility functions and Markov chain models are the most common models which are also used for decision making process and selection of the best solution in presence of many alternatives. The most widely used tools to deal with multiple QAs are PRISM and IBM's autonomic computing toolkit. KLAPER is the only language that has been specifically developed to deal with quality properties analysis.
Conclusions
Our results help researchers to understand the current state of research regarding architecture-based methods for handling multiple QAs in self-adaptive systems, and to identity areas for improvement in the future. To summarize, further research is required to improve existing methods performing tradeoff analysis and preemption, and in particular, new methods may be proposed to make use of models to handle multiple QAs and to enhance and facilitate the tradeoffs analysis and decision making mechanism at runtime.}
}
@article{HEATON2015207,
title = {Claims about the use of software engineering practices in science: A systematic literature review},
journal = {Information and Software Technology},
volume = {67},
pages = {207-219},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001342},
author = {Dustin Heaton and Jeffrey C. Carver},
keywords = {Computational science, Systematic literature review, Scientific software},
abstract = {Context: Scientists have become increasingly reliant on software in order to perform research that is too time-intensive, expensive, or dangerous to perform physically. Because the results produced by the software drive important decisions, the software must be correct and developed efficiently. Various software engineering practices have been shown to increase correctness and efficiency in the development of traditional software. It is unclear whether these observations will hold in a scientific context. Objective: This paper evaluates claims from software engineers and scientific software developers about 12 different software engineering practices and their use in developing scientific software. Method: We performed a systematic literature review examining claims about how scientists develop software. Of the 189 papers originally identified, 43 are included in the literature review. These 43 papers contain 33 different claims about 12 software engineering practices. Results: The majority of the claims indicated that software engineering practices are useful for scientific software development. Every claim was supported by evidence (i.e. personal experience, interview/survey, or case study) with slightly over half supported by multiple forms of evidence. For those claims supported by only one type of evidence, interviews/surveys were the most common. The claims that received the most support were: “The effectiveness of the testing practices currently used by scientific software developers is limited” and “Version control software is necessary for research groups with more than one developer.” Additionally, many scientific software developers have unconsciously adopted an agile-like development methodology. Conclusion: Use of software engineering practices could increase the correctness of scientific software and the efficiency of its development. While there is still potential for increased use of these practices, scientific software developers have begun to embrace software engineering practices to improve their software. Additionally, software engineering practices still need to be tailored to better fit the needs of scientific software development.}
}
@article{AUER2021106551,
title = {Controlled experimentation in continuous experimentation: Knowledge and challenges},
journal = {Information and Software Technology},
volume = {134},
pages = {106551},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106551},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000367},
author = {Florian Auer and Rasmus Ros and Lukas Kaltenbrunner and Per Runeson and Michael Felderer},
keywords = {Continuous experimentation, Online controlled experiments, A/B testing, Systematic literature review},
abstract = {Context:
Continuous experimentation and A/B testing is an established industry practice that has been researched for more than 10 years. Our aim is to synthesize the conducted research.
Objective:
We wanted to find the core constituents of a framework for continuous experimentation and the solutions that are applied within the field. Finally, we were interested in the challenges and benefits reported of continuous experimentation.
Methods:
We applied forward snowballing on a known set of papers and identified a total of 128 relevant papers. Based on this set of papers we performed two qualitative narrative syntheses and a thematic synthesis to answer the research questions.
Results:
The framework constituents for continuous experimentation include experimentation processes as well as supportive technical and organizational infrastructure. The solutions found in the literature were synthesized to nine themes, e.g. experiment design, automated experiments, or metric specification. Concerning the challenges of continuous experimentation, the analysis identified cultural, organizational, business, technical, statistical, ethical, and domain-specific challenges. Further, the study concludes that the benefits of experimentation are mostly implicit in the studies.
Conclusion:
The research on continuous experimentation has yielded a large body of knowledge on experimentation. The synthesis of published research presented within include recommended infrastructure and experimentation process models, guidelines to mitigate the identified challenges, and what problems the various published solutions solve.}
}
@article{SANTOS20171,
title = {Test case design for context-aware applications: Are we there yet?},
journal = {Information and Software Technology},
volume = {88},
pages = {1-16},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302513},
author = {Ismayle de Sousa Santos and Rossana Maria de Castro Andrade and Lincoln Souza Rocha and Santiago Matalonga and Káthia Marçal {de Oliveira} and Guilherme Horta Travassos},
keywords = {Context aware application, Systematic review, Software testing},
abstract = {Context
Current software systems have increasingly implemented context-aware adaptations to handle the diversity of conditions of their surrounding environment. Therefore, people are becoming used to a variety of context-aware software systems (CASS). This context-awareness brings challenges to the software construction and testing because the context is unpredictable and may change at any time. Therefore, software engineers need to consider the dynamic context changes while testing CASS. Different test case design techniques (TCDT) have been proposed to support the testing of CASS. However, to the best of our knowledge, there is no analysis of these proposals on the advantages, limitations and their effective support to context variation during testing.
Objective
To gather empirical evidence on TCDT concerned with CASS by identifying, evaluating and synthesizing knowledge available in the literature.
Method
To undertake a secondary study (quasi-Systematic Literature Review) on TCDT for CASS regarding their assessed quality characteristics, used coverage criteria, test type, and test technique.
Results
From 833 primary studies published between 2004 and 2014, just 17 studies regard the design of test cases for CASS. Most of them focus on functional suitability. Furthermore, some of them take into account the changes in the context by providing specific test cases for each context configuration (static perspective) during the test execution. These 17 studies revealed five challenges affecting the design of test cases and 20 challenges regarding the testing of CASS. Besides, seven TCDT are not empirically evaluated.
Conclusion
A few TCDT partially support the testing of CASS. However, it has not been observed evidence on any TCDT supporting the truly context-aware testing, which that can adapt the expected output based on the context variation (dynamic perspective) during the test execution. It is an open issue deserving greater attention from researchers to increase the testing coverage and ensure users confidence in CASS.}
}
@article{HOISL201749,
title = {Reusable and generic design decisions for developing UML-based domain-specific languages},
journal = {Information and Software Technology},
volume = {92},
pages = {49-74},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304536},
author = {Bernhard Hoisl and Stefan Sobernig and Mark Strembeck},
keywords = {Model-driven software development, Domain-specific language, Design decision, Design rationale, Unified modeling language, Survey},
abstract = {Context: In recent years, UML-based domain-specific model languages (DSMLs) have become a popular option in model-driven development projects. However, making informed design decisions for such DSMLs involves a large number of non-trivial and inter-related options. These options concern the language-model specification, UML extension techniques, concrete-syntax language design, and modeling-tool support. Objective: In order to make the corresponding knowledge on design decisions reusable, proven design rationale from existing DSML projects must be collected, systematized, and documented using an agreed upon documentation format. Method: We applied a sequential multi-method approach to identify and to document reusable design decisions for UML-based DSMLs. The approach included a Web-based survey with 80 participants. Moreover, 80 DSML projects11Note that it is pure coincidence that there were 80 participants in the survey and that 80 DSML projects were reviewed., which have been identified through a prior systematic literature review, were analyzed in detail in order to identify reusable design decisions for such DSMLs. Results: We present insights on the current state of practice in documenting UML-based DSMLs (e.g., perceived barriers, documentation techniques, reuse potential) and a publicly available collection of reusable design decisions, including 35 decision options on different DSML development concerns (especially concerning the language model, concrete-syntax language design, and modeling tools). The reusable design decisions are documented using a structured documentation format (decision record). Conclusion: Our results are both, scientifically relevant (e.g. for design-space analyses or for creating classification schemas for further research on UML-based DSML development) and important for actual software engineering projects (e.g. by providing best-practice guidelines and pointers to common pitfalls).}
}
@article{GAROUSI201935,
title = {A survey on software testability},
journal = {Information and Software Technology},
volume = {108},
pages = {35-64},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918302490},
author = {Vahid Garousi and Michael Felderer and Feyza Nur Kılıçaslan},
keywords = {Software testing, Software testability, Survey, Systematic literature mapping, Systematic literature review, Systematic mapping},
abstract = {Context
Software testability is the degree to which a software system or a unit under test supports its own testing. To predict and improve software testability, a large number of techniques and metrics have been proposed by both practitioners and researchers in the last several decades. Reviewing and getting an overview of the entire state-of-the-art and state-of-the-practice in this area is often challenging for a practitioner or a new researcher.
Objective
Our objective is to summarize the body of knowledge in this area and to benefit the readers (both practitioners and researchers) in preparing, measuring and improving software testability.
Method
To address the above need, the authors conducted a survey in the form of a systematic literature mapping (classification) to find out what we as a community know about this topic. After compiling an initial pool of 303 papers, and applying a set of inclusion/exclusion criteria, our final pool included 208 papers (published between 1982 and 2017).
Results
The area of software testability has been comprehensively studied by researchers and practitioners. Approaches for measurement of testability and improvement of testability are the most-frequently addressed in the papers. The two most often mentioned factors affecting testability are observability and controllability. Common ways to improve testability are testability transformation, improving observability, adding assertions, and improving controllability.
Conclusion
This paper serves for both researchers and practitioners as an “index” to the vast body of knowledge in the area of testability. The results could help practitioners measure and improve software testability in their projects. To assess potential benefits of this review paper, we shared its draft version with two of our industrial collaborators. They stated that they found the review useful and beneficial in their testing activities. Our results can also benefit researchers in observing the trends in this area and identify the topics that require further investigation.}
}
@article{MACHADO20141183,
title = {On strategies for testing software product lines: A systematic literature review},
journal = {Information and Software Technology},
volume = {56},
number = {10},
pages = {1183-1199},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000834},
author = {Ivan do Carmo Machado and John D. McGregor and Yguaratã Cerqueira Cavalcanti and Eduardo Santana {de Almeida}},
keywords = {Software product lines, Software testing, Software quality, Systematic literature review},
abstract = {Context
Testing plays an important role in the quality assurance process for software product line engineering. There are many opportunities for economies of scope and scale in the testing activities, but techniques that can take advantage of these opportunities are still needed.
Objective
The objective of this study is to identify testing strategies that have the potential to achieve these economies, and to provide a synthesis of available research on SPL testing strategies, to be applied towards reaching higher defect detection rates and reduced quality assurance effort.
Method
We performed a literature review of two hundred seventy-six studies published from the year 1998 up to the 1st semester of 2013. We used several filters to focus the review on the most relevant studies and we give detailed analyses of the core set of studies.
Results
The analysis of the reported strategies comprised two fundamental aspects for software product line testing: the selection of products for testing, and the actual test of products. Our findings indicate that the literature offers a large number of techniques to cope with such aspects. However, there is a lack of reports on realistic industrial experiences, which limits the inferences that can be drawn.
Conclusion
This study showed a number of leveraged strategies that can support both the selection of products, and the actual testing of products. Future research should also benefit from the problems and advantages identified in this study.}
}
@article{AWAN2022106896,
title = {Quantum computing challenges in the software industry. A fuzzy AHP-based approach},
journal = {Information and Software Technology},
volume = {147},
pages = {106896},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106896},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000581},
author = {Usama Awan and Lea Hannola and Anushree Tandon and Raman Kumar Goyal and Amandeep Dhir},
keywords = {Fuzzy analytic hierarchy process (F-AHP), Software process automation, Multiple-criteria decision-making (MCDM), Quantum software requirement, Quantum computing},
abstract = {Context
The current technology revolution has posed unexpected challenges for the software industry. In recent years, the field of quantum computing (QC) technologies has continued to grow in influence and maturity, and it is now poised to revolutionise software engineering. However, the evaluation and prioritisation of QC challenges in the software industry remain unexplored, relatively under-identified and fragmented.
Objective
The purpose of this study is to identify, examine and prioritise the most critical challenges in the software industry by implementing a fuzzy analytic hierarchy process (F-AHP).
Method
First, to identify the key challenges, we conducted a systematic literature review by drawing data from the four relevant digital libraries and supplementing these efforts with a forward and backward snowballing search. Second, we followed the F-AHP approach to evaluate and rank the identified challenges, or barriers.
Results
The results show that the key barriers to QC adoption are the lack of technical expertise, information accuracy and organisational interest in adopting the new process. Another critical barrier is the lack of standards of secure communication techniques for implementing QC.
Conclusion
By applying F-AHP, we identified institutional barriers as the highest and organisational barriers as the second highest global weight ranked categories among the main QC challenges facing the software industry. We observed that the highest-ranked local barriers facing the software technology industry are the lack of resources for design and initiative while the lack of organisational interest in adopting the new process is the most significant organisational barrier. Our findings, which entail implications for both academicians and practitioners, reveal the emergent nature of QC research and the increasing need for interdisciplinary research to address the identified challenges.}
}
@article{KHAN2017180,
title = {Systematic literature review and empirical investigation of barriers to process improvement in global software development: Client–vendor perspective},
journal = {Information and Software Technology},
volume = {87},
pages = {180-205},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302483},
author = {Arif Ali Khan and Jacky Keung and Mahmood Niazi and Shahid Hussain and Awais Ahmad},
keywords = {Software process improvement, Global software development, Systematic literature review, Barriers, Client, Vendor},
abstract = {Context
Increasingly, software development organizations are adopting global software development (GSD) strategies, mainly because of the significant return on investment they produce. However, there are many challenges associated with GSD, particularly with regards to software process improvement (SPI). SPI can play a significant role in the successful execution of GSD projects.
Objective
The aim of the present study was to identify barriers that can negatively affect SPI initiatives in GSD organizations from both client and vendor perspectives.
Method
A systematic literature review (SLR) and survey questionnaire were used to identify and validate the barriers.
Results
Twenty-two barriers to successful SPI programs were identified. Results illustrate that the barriers identified using SLR and survey approaches have more similarities However, there were significant differences between the ranking of these barriers in the SLR and survey approaches, as indicated by the results of t-tests (for instance, t = 2.28, p = 0.011 < 0.05). Our findings demonstrate that there is a moderate positive correlation between the ranks obtained from the SLR and the empirical study (rs (22)= 0.567, p = 0.006).
Conclusions
The identified barriers can assist both client and vendor GSD organizations during initiation of an SPI program. Client-vendor classification was used to provide a broad picture of SPI programs, and their respective barriers. The top-ranked barriers can be used as a guide for GSD organizations prior to the initiation of an SPI program. We believe that the results of this study can be useful in tackling the problems associated with the implementation of SPI, which is vital to the success and progression of GSD organizations.}
}
@article{SHAHRIVAR2018202,
title = {A business model for commercial open source software: A systematic literature review},
journal = {Information and Software Technology},
volume = {103},
pages = {202-214},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301277},
author = {Shahrokh Shahrivar and Shaban Elahi and Alireza Hassanzadeh and Gholamali Montazer},
keywords = {Business model, Commercial open source software (COSS), Systematic literature review},
abstract = {Context
Commercial open source software (COSS) and community open source software (OSS) are two types of open source software. The former is the newer concept with the grounds for research such as business model. However, in the literature of open source software, the revenue model has been studied as a business model, which is one component of the business model. Therefore, there is a need for a more complete review of the COSS business model with all components.
Objective
The purpose of this research is to describe and present the COSS business model with all its components.
Method
A systematic literature review of the COSS business model was conducted and 1157 studies were retrieved through search in six academic databases. The result of the process of selecting the primary studies was 21 studies. By backward snowballing, we discovered 10 other studies, and thus a total of 31 studies were found. Then, the grounded theory coding procedures were used to determine the characteristics and components of the COSS business model.
Results
The COSS business model was presented with value proposition, value creation & delivery, and value capture. This business model includes eight components: COSS products and complementarities, COSS clients and users, COSS competitive strategies, organizational aspects of COSS, position of COSS producers in the value network, resources and capabilities of COSS business, COSS revenue sources, and COSS cost-benefit.
Conclusion
This study provides a complete illustration of the COSS business model. Identifies COSS generic competitive strategies. By cost-benefit component, we have considered both tangible and intangible components. This business model is especially effective in developing countries. In future research, it is necessary to review the management of the COSS community, the organization, the new revenue models for disruptive ability of open source software, and the localization of open source software.}
}
@article{ZHANG2011625,
title = {Identifying relevant studies in software engineering},
journal = {Information and Software Technology},
volume = {53},
number = {6},
pages = {625-637},
year = {2011},
note = {Special Section: Best papers from the APSEC},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002260},
author = {He Zhang and Muhammad Ali Babar and Paolo Tell},
keywords = {Search strategy, Quasi-gold standard, Systematic literature review, Evidence-based software engineering},
abstract = {Context
Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries.
Objective
The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE.
Method
We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of ‘quasi-gold standard’ (QGS), which consists of collection of known studies, and corresponding ‘quasi-sensitivity’ into the search process for evaluating search performance.
Results
We conducted two participant–observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research.
Conclusion
We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE.}
}
@article{YOUNAS2018142,
title = {Agile development in the cloud computing environment: A systematic review},
journal = {Information and Software Technology},
volume = {103},
pages = {142-158},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301319},
author = {Muhammad Younas and Dayang N.A. Jawawi and Imran Ghani and Terrence Fries and Rafaqut Kazmi},
keywords = {Agile, Agile software development, Agile methodology, Cloud computing, Systematic review},
abstract = {Background: Agile software development is based on a set of values and principles. The twelve principles are inferred from agile values. Agile principles are composition of evolutionary requirement, simple design, continuous delivery, self-organizing team and face-to-face communication. Due to changing market demand, agile methodology faces problems such as scalability, more effort and cost required in setting up hardware and software infrastructure, availability of skilled resource and ability to build application from multiple locations. Twelve (12) principles may be practiced more appropriately with the support of cloud computing. This merger of agile and cloud computing may provide infrastructure optimization and automation benefits to agile practitioners. Objective: This Systematic Literature Review (SLR) identifies the techniques employed in cloud computing environment that are useful for agile development. In addition, SLR discusses the significance of cloud and its challenges. Method: By applying the SLR procedure, the authors select thirty-seven (37) studies out of six-hundred-forty-seven (647) from 2010 to 2017. Result: The result of SLR shows that the techniques using existing tools were reported in 35%, simulations in 20% and application developed in 15% of the studies. Evaluation of techniques was reported in 32% of the studies. The impact of cloud computing was measured by the classification of four major categories such as transparency 32%, collaboration 50%, development infrastructure 29% and cloud quality attributes in 39%. Furthermore, a large number of tools were reported in primary studies. The challenges posed by cloud adoption in agile was reported as interoperability 13%, security & privacy 18% and rest of the primary studies did not report any other research gaps. Conclusions: The study concludes that agile development in cloud computing environment is an important area in software engineering. There are many open challenges and gaps. In particular, more quality tools, evaluation research and empirical studies are required in this area.}
}
@article{KITCHENHAM2010792,
title = {Systematic literature reviews in software engineering – A tertiary study},
journal = {Information and Software Technology},
volume = {52},
number = {8},
pages = {792-805},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910000467},
author = {Barbara Kitchenham and Rialette Pretorius and David Budgen and O. {Pearl Brereton} and Mark Turner and Mahmood Niazi and Stephen Linkman},
keywords = {Systematic literature review, Mapping study, Software engineering, Tertiary study},
abstract = {Context
In a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007.
Objective
The aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search.
Method
We performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study.
Results
Our broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is increasing. The quality of papers in conferences and workshops has improved as more researchers use SLR guidelines.
Conclusion
SLRs appear to have gone past the stage of being used solely by innovators but cannot yet be considered a main stream software engineering research methodology. They are addressing a wide range of topics but still have limitations, such as often failing to assess primary study quality.}
}
@article{GAROUSI2016195,
title = {A systematic literature review of literature reviews in software testing},
journal = {Information and Software Technology},
volume = {80},
pages = {195-216},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301446},
author = {Vahid Garousi and Mika V. Mäntylä},
keywords = {Secondary studies, Tertiary study, Software testing, Systematic mapping, Systematic literature reviews, Surveys},
abstract = {Context
Any newcomer or industrial practitioner is likely to experience difficulties in digesting large volumes of knowledge in software testing. In an ideal world, all knowledge used in industry, education and research should be based on high-quality evidence. Since no decision should be made based on a single study, secondary studies become essential in presenting the evidence. According to our search, over 101 secondary studies have been published in the area of software testing since 1994. With this high number of secondary studies, it is important to conduct a review in this area to provide an overview of the research landscape in this area.
Objective
The goal of this study is to systematically map (classify) the secondary studies in software testing. We propose that tertiary studies can serve as summarizing indexes which facilitate finding the most relevant information from secondary studies and thus supporting evidence-based decision making in any given area of software engineering. Our research questions (RQs) investigate: (1) Software-testing-specific areas, (2) Types of RQs investigated, (3) Numbers and Trends, and (4) Citations of the secondary studies.
Method
To conduct the tertiary study, we use the systematic-mapping approach. Additionally, we contrast the testing topics to the number of Google hits to address a general popularity of a testing topic and study the most popular papers in terms of citations. We furthermore demonstrate the practicality and usefulness of our results by mapping them to ISTQB foundation syllabus and to SWEBOK to provide implications for practitioners, testing educators, and researchers.
Results
After a systematic search and voting process, our study pool included 101 secondary studies in the area of software testing between 1994 and 2015. Among our results are the following: (1) In terms of number of secondary studies, model-based approach is the most popular testing method, web services are the most popular system under test (SUT), while regression testing is the most popular testing phase; (2) The quality of secondary studies, as measured by a criteria set established in the community, is slowly increasing as the years go by; and (3) Analysis of research questions, raised and studied in the pool of secondary studies, showed that there is a lack of ‘causality’ and ‘relationship’ type of research questions, a situation which needs to be improved if we, as a community, want to advance as a scientific field. (4) Among secondary studies, we found that regular surveys receive significantly more citations than SMs (p=0.009) and SLRs (p=0.014).
Conclusion
Despite the large number of secondary studies, we found that many important areas of software testing currently lack secondary studies, e.g., test management, role of product risk in testing, human factors in software testing, beta-testing (A/B-testing), exploratory testing, testability, test stopping criteria, and test-environment development. Having secondary studies in those areas is important for satisfying industrial and educational needs in software testing. On the other hand, education material of ISTQB foundation syllabus and SWEBOK could benefit from the inclusion of the latest research topics, namely search-based testing, use of cloud-computing for testing and symbolic execution.}
}
@article{ALVES2010806,
title = {Requirements engineering for software product lines: A systematic literature review},
journal = {Information and Software Technology},
volume = {52},
number = {8},
pages = {806-820},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910000625},
author = {Vander Alves and Nan Niu and Carina Alves and George Valença},
keywords = {Software product lines, Requirements engineering, Systematic literature review},
abstract = {Context
Software product line engineering (SPLE) is a growing area showing promising results in research and practice. In order to foster its further development and acceptance in industry, it is necessary to assess the quality of the research so that proper evidence for adoption and validity are ensured. This holds in particular for requirements engineering (RE) within SPLE, where a growing number of approaches have been proposed.
Objective
This paper focuses on RE within SPLE and has the following goals: assess research quality, synthesize evidence to suggest important implications for practice, and identify research trends, open problems, and areas for improvement.
Method
A systematic literature review was conducted with three research questions and assessed 49 studies, dated from 1990 to 2009.
Results
The evidence for adoption of the methods is not mature, given the primary focus on toy examples. The proposed approaches still have serious limitations in terms of rigor, credibility, and validity of their findings. Additionally, most approaches still lack tool support addressing the heterogeneity and mostly textual nature of requirements formats as well as address only the proactive SPLE adoption strategy.
Conclusions
Further empirical studies should be performed with sufficient rigor to enhance the body of evidence in RE within SPLE. In this context, there is a clear need for conducting studies comparing alternative methods. In order to address scalability and popularization of the approaches, future research should be invested in tool support and in addressing combined SPLE adoption strategies.}
}
@article{AMPATZOGLOU201552,
title = {The financial aspect of managing technical debt: A systematic literature review},
journal = {Information and Software Technology},
volume = {64},
pages = {52-73},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000762},
author = {Areti Ampatzoglou and Apostolos Ampatzoglou and Alexander Chatzigeorgiou and Paris Avgeriou},
keywords = {Technical debt, Financial debt, Financial terms, Systematic literature review},
abstract = {Context
Technical debt is a software engineering metaphor, referring to the eventual financial consequences of trade-offs between shrinking product time to market and poorly specifying, or implementing a software product, throughout all development phases. Based on its inter-disciplinary nature, i.e. software engineering and economics, research on managing technical debt should be balanced between software engineering and economic theories.
Objective
The aim of this study is to analyze research efforts on technical debt, by focusing on their financial aspect. Specifically, the analysis is carried out with respect to: (a) how financial aspects are defined in the context of technical debt and (b) how they relate to the underlying software engineering concepts.
Method
In order to achieve the abovementioned goals, we employed a standard method for SLRs and applied it on studies retrieved from seven general-scope digital libraries. In total we selected 69 studies relevant to the financial aspect of technical debt.
Results
The most common financial terms that are used in technical debt research are principal and interest, whereas the financial approaches that have been more frequently applied for managing technical debt are real options, portfolio management, cost/benefit analysis and value-based analysis. However, the application of such approaches lacks consistency, i.e., the same approach is differently applied in different studies, and in some cases lacks a clear mapping between financial and software engineering concepts.
Conclusion
The results are expected to prove beneficial for the communication between technical managers and project managers, in the sense that they will provide a common vocabulary, and will help in setting up quality-related goals, during software development. To achieve this we introduce: (a) a glossary of terms and (b) a classification scheme for financial approaches used for managing technical debt. Based on these, we have been able to underline interesting implications for researchers and practitioners.}
}
@article{VIERHAUSER201689,
title = {Requirements monitoring frameworks: A systematic review},
journal = {Information and Software Technology},
volume = {80},
pages = {89-109},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301288},
author = {Michael Vierhauser and Rick Rabiser and Paul Grünbacher},
keywords = {Requirements monitoring, Systems of systems, Systematic literature review},
abstract = {Context
Software systems today often interoperate with each other, thus forming a system of systems (SoS). Due to the scale, complexity, and heterogeneity of SoS, determining compliance with their requirements is challenging, despite the range of existing monitoring approaches. The fragmented research landscape and the diversity of existing approaches, however, make it hard to understand and analyze existing research regarding its suitability for SoS.
Objective
The aims of this paper are thus to systematically identify, describe, and classify existing approaches for requirements-based monitoring of software systems at runtime. Specifically, we (i) analyze the characteristics and application areas of monitoring approaches proposed in different domains, we (ii) systematically identify frameworks supporting requirements monitoring, and finally (iii) analyze their support for requirements monitoring in SoS.
Method
We performed a systematic literature review (SLR) to identify existing monitoring approaches and to classify their key characteristics and application areas. Based on this analysis we selected requirements monitoring frameworks, following a definition by Robinson, and analyzed them regarding their support for requirements monitoring in SoS.
Results
We identified 330 publications, which we used to produce a comprehensive overview of the landscape of requirements monitoring approaches. We analyzed these publications regarding their support for Robinson’s requirements monitoring layers, resulting in 37 identified frameworks. We investigated how well these frameworks support requirements monitoring in SoS.
Conclusions
We conclude that most existing approaches are restricted to certain kinds of checks, particular types of events and data, and mostly also limited to one particular architectural style and technology. This lack of flexibility makes their application in an SoS context difficult. Also, systematic and automated variability management is still missing. Regarding their evaluation, many existing frameworks focus on measuring the performance overhead, while only few frameworks have been assessed in cases studies with real-world systems.}
}
@article{VELASQUEZ201827,
title = {Kontun: A Framework for recommendation of authentication schemes and methods},
journal = {Information and Software Technology},
volume = {96},
pages = {27-37},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917301714},
author = {Ignacio Velásquez and Angélica Caro and Alfonso Rodríguez},
keywords = {Security, Authentication scheme, Multi-factor authentication method, Recommendation Framework},
abstract = {Context
There are many techniques for performing authentication, such as text passwords and biometrics. Combining two factors into one technique is known as multi-factor authentication. The lack of a proper method for comparing and selecting these techniques for their implementation in software development processes is observed.
Objective
The article presents a recommendation Framework proposal for comparing and selecting authentication techniques in a software development process.
Method
Knowledge from academy is obtained through a systematic literature review and experience from industry is gathered using a survey and interviews. The results of these two techniques are used to generate a Framework proposal, which is validated afterwards, through an expert panel and the case study method.
Results
A recommendation Framework is generated, which recommends the most appropriate authentication schemes and methods for software applications based on criteria identified in literature and industry, categorized by usability, security and costs, plus the context for which the application is intended. The Framework's validity is ascertained by confirming that its recommendations are on line with those on industry, based in the results from the developed case studies. A tool prototype was created in order to help using the Framework in software development processes.
Conclusion
The proposed Framework helps to cover the observed gap in literature, helping software developers to compare and select the most appropriate authentication techniques for their applications.}
}
@article{RABISER201986,
title = {A domain analysis of resource and requirements monitoring: Towards a comprehensive model of the software monitoring domain},
journal = {Information and Software Technology},
volume = {111},
pages = {86-109},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300606},
author = {Rick Rabiser and Klaus Schmid and Holger Eichelberger and Michael Vierhauser and Sam Guinea and Paul Grünbacher},
keywords = {Software monitoring, Requirements monitoring, Resource monitoring, Domain model, Reference architecture},
abstract = {[Context] Complex and heterogeneous software systems need to be monitored as their full behavior often only emerges at runtime, e.g., when interacting with other systems or the environment. Software monitoring approaches observe and check properties or quality attributes of software systems during operation. Such approaches have been developed in diverse communities for various kinds of systems and purposes. For instance, requirements monitoring aims to check at runtime whether a software system adheres to its requirements, while resource or performance monitoring collects information about the consumption of computing resources by the monitored system. Many venues publish research on software monitoring, often using diverse terminology, and focusing on different monitoring aspects and phases. The lack of a comprehensive overview of existing research often leads to re-inventing the wheel. [Objective] We provide a domain model to structure and systematize the field of software monitoring, starting with requirements and resource monitoring. [Method] We developed an initial domain model based on (i) our extensive experiences with requirements and resource monitoring, (ii) earlier efforts to develop a comparison framework for monitoring approaches, and (iii) an earlier systematic literature review on requirements monitoring frameworks. We then systematically analyzed 47 existing requirements and resource monitoring approaches to iteratively refine the domain model and to develop a reference architecture for software monitoring approaches. [Results] Our domain model covers the key elements of monitoring approaches and allows analyzing their commonalities and differences. Together with the reference architecture, our domain model supports the development of integrated monitoring solutions. We provide details on 47 approaches we analyzed with the model to assess its coverage. We also evaluate the reference architecture by instantiating it for five different monitoring solutions. [Conclusions] We conclude that requirements and resource monitoring have more commonalities than differences, which is promising for the future integration of existing monitoring solutions.}
}
@article{ALDALLAL2015231,
title = {Identifying refactoring opportunities in object-oriented code: A systematic literature review},
journal = {Information and Software Technology},
volume = {58},
pages = {231-249},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001918},
author = {Jehad {Al Dallal}},
keywords = {Refactoring activity, Refactoring opportunity, Systematic literature review},
abstract = {Context
Identifying refactoring opportunities in object-oriented code is an important stage that precedes the actual refactoring process. Several techniques have been proposed in the literature to identify opportunities for various refactoring activities.
Objective
This paper provides a systematic literature review of existing studies identifying opportunities for code refactoring activities.
Method
We performed an automatic search of the relevant digital libraries for potentially relevant studies published through the end of 2013, performed pilot and author-based searches, and selected 47 primary studies (PSs) based on inclusion and exclusion criteria. The PSs were analyzed based on a number of criteria, including the refactoring activities, the approaches to refactoring opportunity identification, the empirical evaluation approaches, and the data sets used.
Results
The results indicate that research in the area of identifying refactoring opportunities is highly active. Most of the studies have been performed by academic researchers using nonindustrial data sets. Extract Class and Move Method were found to be the most frequently considered refactoring activities. The results show that researchers use six primary existing approaches to identify refactoring opportunities and six approaches to empirically evaluate the identification techniques. Most of the systems used in the evaluation process were open-source, which helps to make the studies repeatable. However, a relatively high percentage of the data sets used in the empirical evaluations were small, which limits the generality of the results.
Conclusions
It would be beneficial to perform further studies that consider more refactoring activities, involve researchers from industry, and use large-scale and industrial-based systems.}
}
@article{BRHEL2015163,
title = {Exploring principles of user-centered agile software development: A literature review},
journal = {Information and Software Technology},
volume = {61},
pages = {163-181},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000129},
author = {Manuel Brhel and Hendrik Meth and Alexander Maedche and Karl Werder},
keywords = {Agile software development, User-centered design, Systematic literature review},
abstract = {Context
In the last decade, software development has been characterized by two major approaches: agile software development, which aims to achieve increased velocity and flexibility during the development process, and user-centered design, which places the goals and needs of the system’s end-users at the center of software development in order to deliver software with appropriate usability. Hybrid development models, referred to as user-centered agile software development (UCASD) in this article, propose to combine the merits of both approaches in order to design software that is both useful and usable.
Objective
This paper aims to capture the current state of the art in UCASD approaches and to derive generic principles from these approaches. More specifically, we investigate the following research question: Which principles constitute a user-centered agile software development approach?
Method
We conduct a systematic review of the literature on UCASD. Identified works are analyzed using a coding scheme that differentiates four levels of UCASD: the process, practices, people/social and technology dimensions. Through subsequent synthesis, we derive generic principles of UCASD.
Results
We identified and analyzed 83 relevant publications. The analysis resulted in a comprehensive coding system and five principles for UCASD: (1) separate product discovery and product creation, (2) iterative and incremental design and development, (3) parallel interwoven creation tracks, (4) continuous stakeholder involvement, and (5) artifact-mediated communication.
Conclusion
Our paper contributes to the software development body of knowledge by (1) providing a broad overview of existing works in the area of UCASD, (2) deriving an analysis framework (in form a coding system) for works in this area, going beyond former classifications, and (3) identifying generic principles of UCASD and associating them with specific practices and processes.}
}
@article{MAHDAVIHEZAVEHI2013320,
title = {Variability in quality attributes of service-based software systems: A systematic literature review},
journal = {Information and Software Technology},
volume = {55},
number = {2},
pages = {320-343},
year = {2013},
note = {Special Section: Component-Based Software Engineering (CBSE), 2011},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001772},
author = {Sara Mahdavi-Hezavehi and Matthias Galster and Paris Avgeriou},
keywords = {Variability, Service-based systems, Quality attributes, Systematic literature review},
abstract = {Context
Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices.
Objective
We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement.
Method
A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies.
Results
Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments.
Conclusions
The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.}
}
@article{GAROUSI201814,
title = {Testing embedded software: A survey of the literature},
journal = {Information and Software Technology},
volume = {104},
pages = {14-45},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301265},
author = {Vahid Garousi and Michael Felderer and Çağrı Murat Karapıçak and Uğur Yılmaz},
keywords = {Software testing, Embedded systems, Embedded software, Systematic mapping, Systematic literature mapping, Systematic literature review},
abstract = {Context
Embedded systems have overwhelming penetration around the world. Innovations are increasingly triggered by software embedded in automotive, transportation, medical-equipment, communication, energy, and many other types of systems. To test embedded software in an effective and efficient manner, a large number of test techniques, approaches, tools and frameworks have been proposed by both practitioners and researchers in the last several decades.
Objective
However, reviewing and getting an overview of the entire state-of-the-art and the –practice in this area is challenging for a practitioner or a (new) researcher. Also unfortunately, as a result, we often see that many companies reinvent the wheel (by designing a test approach new to them, but existing in the domain) due to not having an adequate overview of what already exists in this area.
Method
To address the above need, we conducted and report in this paper a systematic literature review (SLR) in the form of a systematic literature mapping (SLM) in this area. After compiling an initial pool of 588 papers, a systematic voting about inclusion/exclusion of the papers was conducted among the authors, and our final pool included 312 technical papers.
Results
Among the various aspects that we aim at covering, our review covers the types of testing topics studied, types of testing activity, types of test artifacts generated (e.g., test inputs or test code), and the types of industries in which studies have focused on, e.g., automotive and home appliances. Furthermore, we assess the benefits of this review by asking several active test engineers in the Turkish embedded software industry to review its findings and provide feedbacks as to how this review has benefitted them.
Conclusion
The results of this review paper have already benefitted several of our industry partners in choosing the right test techniques / approaches for their embedded software testing challenges. We believe that it will also be useful for the large world-wide community of software engineers and testers in the embedded software industry, by serving as an “index” to the vast body of knowledge in this important area. Our results will also benefit researchers in observing the latest trends in this area and for identifying the topics which need further investigations.}
}
@article{LEITNER2014273,
title = {A systematic review on security in Process-Aware Information Systems – Constitution, challenges, and future directions},
journal = {Information and Software Technology},
volume = {56},
number = {3},
pages = {273-293},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913002334},
author = {Maria Leitner and Stefanie Rinderle-Ma},
keywords = {Business Process Management, Business process security, Process-Aware Information Systems, Security, Systematic literature review, Workflow security},
abstract = {Context
Security in Process-Aware Information Systems (PAIS) has gained increased attention in current research and practice. However, a common understanding and agreement on security is still missing. In addition, the proliferation of literature makes it cumbersome to overlook and determine state of the art and further to identify research challenges and gaps. In summary, a comprehensive and systematic overview of state of the art in research and practice in the area of security in PAIS is missing.
Objective
This paper investigates research on security in PAIS and aims at establishing a common understanding of terminology in this context. Further it investigates which security controls are currently applied in PAIS.
Method
A systematic literature review is conducted in order to classify and define security and security controls in PAIS. From initially 424 papers, we selected in total 275 publications that related to security and PAIS between 1993 and 2012. Furthermore, we analyzed and categorized the papers using a systematic mapping approach which resulted into 5 categories and 12 security controls.
Results
In literature, security in PAIS often centers on specific (security) aspects such as security policies, security requirements, authorization and access control mechanisms, or inter-organizational scenarios. In addition, we identified 12 security controls in the area of security concepts, authorization and access control, applications, verification, and failure handling in PAIS. Based on the results, open research challenges and gaps are identified and discussed with respect to possible solutions.
Conclusion
This survey provides a comprehensive review of current security practice in PAIS and shows that security in PAIS is a challenging interdisciplinary research field that assembles research methods and principles from security and PAIS. We show that state of the art provides a rich set of methods such as access control models but still several open research challenges remain.}
}
@article{KANEWALA20141219,
title = {Testing scientific software: A systematic literature review},
journal = {Information and Software Technology},
volume = {56},
number = {10},
pages = {1219-1232},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001232},
author = {Upulee Kanewala and James M. Bieman},
keywords = {Scientific software, Software testing, Systematic literature review, Software quality},
abstract = {Context
Scientific software plays an important role in critical decision making, for example making weather predictions based on climate models, and computation of evidence for research publications. Recently, scientists have had to retract publications due to errors caused by software faults. Systematic testing can identify such faults in code.
Objective
This study aims to identify specific challenges, proposed solutions, and unsolved problems faced when testing scientific software.
Method
We conducted a systematic literature survey to identify and analyze relevant literature. We identified 62 studies that provided relevant information about testing scientific software.
Results
We found that challenges faced when testing scientific software fall into two main categories: (1) testing challenges that occur due to characteristics of scientific software such as oracle problems and (2) testing challenges that occur due to cultural differences between scientists and the software engineering community such as viewing the code and the model that it implements as inseparable entities. In addition, we identified methods to potentially overcome these challenges and their limitations. Finally we describe unsolved challenges and how software engineering researchers and practitioners can help to overcome them.
Conclusions
Scientific software presents special challenges for testing. Specifically, cultural differences between scientist developers and software engineers, along with the characteristics of the scientific software make testing more difficult. Existing techniques such as code clone detection can help to improve the testing process. Software engineers should consider special challenges posed by scientific software such as oracle problems when developing testing techniques.}
}
@article{STOL20111319,
title = {A comparative study of challenges in integrating Open Source Software and Inner Source Software},
journal = {Information and Software Technology},
volume = {53},
number = {12},
pages = {1319-1336},
year = {2011},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S095058491100142X},
author = {Klaas-Jan Stol and Muhammad Ali Babar and Paris Avgeriou and Brian Fitzgerald},
keywords = {Open Source Software, Inner Source, Software development, Challenges, Case study, Empirical studies},
abstract = {Context
Several large software-developing organizations have adopted Open Source Software development (OSSD) practices to develop in-house components that are subsequently integrated into products. This phenomenon is also known as “Inner Source”. While there have been several reports of successful cases of this phenomenon, little is known about the challenges that practitioners face when integrating software that is developed in such a setting.
Objective
The objective of this study was to shed light on challenges related to building products with components that have been developed within an Inner Source development environment.
Method
Following an initial systematic literature review to generate seed category data constructs, we performed an in-depth exploratory case study in an organization that has a significant track record in the implementation of Inner Source. Data was gathered through semi-structured interviews with participants from a range of divisions across the organization. Interviews were transcribed and analyzed using qualitative data analysis techniques.
Results
We have identified a number of challenges and approaches to address them, and compared the findings to challenges related to development with OSS products reported in the literature. We found that many challenges identified in the case study could be mapped to challenges related to integration of OSS.
Conclusion
The results provide important insights into common challenges of developing with OSS and Inner Source and may help organizations to understand how to improve their software development practices by adopting certain OSSD practices. The findings also identify the areas that need further research.}
}
@article{NGUYEN201562,
title = {An extensive systematic review on the Model-Driven Development of secure systems},
journal = {Information and Software Technology},
volume = {68},
pages = {62-81},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001482},
author = {Phu H. Nguyen and Max Kramer and Jacques Klein and Yves Le Traon},
keywords = {Systematic review, Model-Driven Security, MDS, Model-Driven Engineering, MDE, Software security engineering},
abstract = {Context: Model-Driven Security (MDS) is as a specialised Model-Driven Engineering research area for supporting the development of secure systems. Over a decade of research on MDS has resulted in a large number of publications. Objective: To provide a detailed analysis of the state of the art in MDS, a systematic literature review (SLR ) is essential. Method: We conducted an extensive SLR on MDS. Derived from our research questions, we designed a rigorous, extensive search and selection process to identify a set of primary MDS studies that is as complete as possible. Our three-pronged search process consists of automatic searching, manual searching, and snowballing. After discovering and considering more than thousand relevant papers, we identified, strictly selected, and reviewed 108 MDS publications. Results: The results of our SLR show the overall status of the key artefacts of MDS, and the identified primary MDS studies. For example, regarding security modelling artefact, we found that developing domain-specific languages plays a key role in many MDS approaches. The current limitations in each MDS artefact are pointed out and corresponding potential research directions are suggested. Moreover, we categorise the identified primary MDS studies into 5 significant MDS studies, and other emerging or less common MDS studies. Finally, some trend analyses of MDS research are given. Conclusion: Our results suggest the need for addressing multiple security concerns more systematically and simultaneously, for tool chains supporting the MDS development cycle, and for more empirical studies on the application of MDS methodologies. To the best of our knowledge, this SLR is the first in the field of Software Engineering that combines a snowballing strategy with database searching. This combination has delivered an extensive literature study on MDS.}
}
@article{LAUKKANEN201755,
title = {Problems, causes and solutions when adopting continuous delivery—A systematic literature review},
journal = {Information and Software Technology},
volume = {82},
pages = {55-79},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916302324},
author = {Eero Laukkanen and Juha Itkonen and Casper Lassenius},
keywords = {Continuous integration, Continuous delivery, Continuous deployment, Systematic literature review},
abstract = {Context: Continuous delivery is a software development discipline in which software is always kept releasable. The literature contains instructions on how to adopt continuous delivery, but the adoption has been challenging in practice. Objective: In this study, a systematic literature review is conducted to survey the faced problems when adopting continuous delivery. In addition, we identify causes for and solutions to the problems. Method: By searching five major bibliographic databases, we identified 293 articles related to continuous delivery. We selected 30 of them for further analysis based on them containing empirical evidence of adoption of continuous delivery, and focus on practice instead of only tooling. We analyzed the selected articles qualitatively and extracted problems, causes and solutions. The problems and solutions were thematically synthesized into seven themes: build design, system design, integration, testing, release, human and organizational and resource. Results: We identified a total of 40 problems, 28 causal relationships and 29 solutions related to adoption of continuous delivery. Testing and integration problems were reported most often, while the most critical reported problems were related to testing and system design. Causally, system design and testing were most connected to other themes. Solutions in the system design, resource and human and organizational themes had the most significant impact on the other themes. The system design and build design themes had the least reported solutions. Conclusions: When adopting continuous delivery, problems related to system design are common, critical and little studied. The found problems, causes and solutions can be used to solve problems when adopting continuous delivery in practice.}
}
@article{GAROUSI2019101,
title = {Guidelines for including grey literature and conducting multivocal literature reviews in software engineering},
journal = {Information and Software Technology},
volume = {106},
pages = {101-121},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301939},
author = {Vahid Garousi and Michael Felderer and Mika V. Mäntylä},
keywords = {Multivocal literature review, Grey literature, Guidelines, Systematic literature review, Systematic mapping study, Literature study, Evidence-based software engineering},
abstract = {Context
A Multivocal Literature Review (MLR) is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts, videos and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). MLRs are useful for both researchers and practitioners since they provide summaries both the state-of-the art and –practice in a given area. MLRs are popular in other fields and have recently started to appear in software engineering (SE). As more MLR studies are conducted and reported, it is important to have a set of guidelines to ensure high quality of MLR processes and their results.
Objective
There are several guidelines to conduct SLR studies in SE. However, several phases of MLRs differ from those of traditional SLRs, for instance with respect to the search process and source quality assessment. Therefore, SLR guidelines are only partially useful for conducting MLR studies. Our goal in this paper is to present guidelines on how to conduct MLR studies in SE.
Method
To develop the MLR guidelines, we benefit from several inputs: (1) existing SLR guidelines in SE, (2), a literature survey of MLR guidelines and experience papers in other fields, and (3) our own experiences in conducting several MLRs in SE. We took the popular SLR guidelines of Kitchenham and Charters as the baseline and extended/adopted them to conduct MLR studies in SE. All derived guidelines are discussed in the context of an already-published MLR in SE as the running example.
Results
The resulting guidelines cover all phases of conducting and reporting MLRs in SE from the planning phase, over conducting the review to the final reporting of the review. In particular, we believe that incorporating and adopting a vast set of experience-based recommendations from MLR guidelines and experience papers in other fields have enabled us to propose a set of guidelines with solid foundations.
Conclusion
Having been developed on the basis of several types of experience and evidence, the provided MLR guidelines will support researchers to effectively and efficiently conduct new MLRs in any area of SE. The authors recommend the researchers to utilize these guidelines in their MLR studies and then share their lessons learned and experiences.}
}
@article{SHARP2009219,
title = {Models of motivation in software engineering},
journal = {Information and Software Technology},
volume = {51},
number = {1},
pages = {219-233},
year = {2009},
note = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2008.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584908000827},
author = {Helen Sharp and Nathan Baddoo and Sarah Beecham and Tracy Hall and Hugh Robinson},
keywords = {Motivation, Project management, Human factors, Systematic literature review},
abstract = {Motivation in software engineering is recognized as a key success factor for software projects, but although there are many papers written about motivation in software engineering, the field lacks a comprehensive overview of the area. In particular, several models of motivation have been proposed, but they either rely heavily on one particular model (the job characteristics model), or are quite disparate and difficult to combine. Using the results from our previous systematic literature review (SLR), we constructed a new model of motivation in software engineering. We then compared this new model with existing models and refined it based on this comparison. This paper summarises the SLR results, presents the important existing models found in the literature and explains the development of our new model of motivation in software engineering.}
}
@article{KAMEI2021106609,
title = {Grey Literature in Software Engineering: A critical review},
journal = {Information and Software Technology},
volume = {138},
pages = {106609},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106609},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000860},
author = {Fernando Kamei and Igor Wiese and Crescencio Lima and Ivanilton Polato and Vilmar Nepomuceno and Waldemar Ferreira and Márcio Ribeiro and Carolline Pena and Bruno Cartaxo and Gustavo Pinto and Sérgio Soares},
keywords = {Grey Literature, Tertiary study, Secondary Study, Software Engineering, Multivocal Literature Review, Grey Literature Review, Systematic Literature Review, Mapping Study},
abstract = {Context:
Grey Literature (GL) recently has grown in Software Engineering (SE) research since the increased use of online communication channels by software engineers. However, there is still a limited understanding of how SE research is taking advantage of GL.
Objective:
This research aimed to understand how SE researchers use GL in their secondary studies.
Methods:
We conducted a tertiary study of studies published between 2011 and 2018 in high-quality software engineering conferences and journals. We then applied qualitative and quantitative analysis to investigate 446 potential studies.
Results:
From the 446 selected studies, 126 studies cited GL but only 95 of those used GL to answer a specific research question representing almost 21% of all the 446 secondary studies. Interestingly, we identified that few studies employed specific search mechanisms and used additional criteria for assessing GL. Moreover, by the time we conducted this research, 49% of the GL URLs are not working anymore. Based on our findings, we discuss some challenges in using GL and potential mitigation plans.
Conclusion:
In this paper, we summarized the last 10 years of software engineering research that uses GL, showing that GL has been essential for bringing practical new perspectives that are scarce in traditional literature. By drawing the current landscape of use, we also raise some awareness of related challenges (and strategies to deal with them).}
}
@article{HASSLER201859,
title = {A comparison of automated training-by-example selection algorithms for Evidence Based Software Engineering},
journal = {Information and Software Technology},
volume = {98},
pages = {59-73},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917302896},
author = {Edgar E. Hassler and David P. Hale and Joanne E. Hale},
keywords = {Research infrastructure, Evidence Based Software Engineering, Systematic Literature Review, Systematic Mapping Studies, Culling, VSM, LSA, Recall, Precision, Document selection},
abstract = {Context
Study search and selection is central to conducting Evidence Based Software Engineering (EBSE) research, including Systematic Literature Reviews and Systematic Mapping Studies. Thus, selecting relevant studies and excluding irrelevant studies, is critical. Prior research argues that study selection is subject to researcher bias, and the time required to review and select relevant articles is a target for optimization.
Objective
This research proposes two training-by-example classifiers that are computationally simple, do not require extensive training or tuning, ensure inclusion/exclusion consistency, and reduce researcher study selection time: one based on Vector Space Models (VSM), and a second based on Latent Semantic Analysis (LSA).
Method
Algorithm evaluation is accomplished through Monte-Carlo Cross-Validation simulations, in which study subsets are randomly chosen from the corpus for training, with the remainder classified by the algorithm. The classification results are then assessed for recall (a measure of completeness), precision (a measure of exactness) and researcher efficiency savings (reduced proportion of corpus studies requiring manual review as a result of algorithm use). A second smaller simulation is conducted for external validation.
Results and conclusions
VSM algorithms perform better in recall; LSA algorithms perform better in precision. Recall improves with larger training sets with a higher proportion of truly relevant studies. Precision improves with training sets with a higher portion of irrelevant studies, without a significant impact from the training set size. The algorithms reduce the influence of researcher bias and are found to significantly improve researcher efficiency. To improve recall, the findings recommend VSM and a large training set including as many truly relevant studies as possible. If precision and efficiency are most critical, the findings suggest LSA and a training set including a large proportion of truly irrelevant studies.}
}
@article{GAROUSI201692,
title = {When and what to automate in software testing? A multi-vocal literature review},
journal = {Information and Software Technology},
volume = {76},
pages = {92-117},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.04.015},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300702},
author = {Vahid Garousi and Mika V. Mäntylä},
keywords = {Software test automation, Decision support, When to automate, What to automate, Multivocal literature review, Systematic literature review, Systematic Mapping study},
abstract = {Context
Many organizations see software test automation as a solution to decrease testing costs and to reduce cycle time in software development. However, establishment of automated testing may fail if test automation is not applied in the right time, right context and with the appropriate approach.
Objective
The decisions on when and what to automate is important since wrong decisions can lead to disappointments and major wrong expenditures (resources and efforts). To support decision making on when and what to automate, researchers and practitioners have proposed various guidelines, heuristics and factors since the early days of test automation technologies. As the number of such sources has increased, it is important to systematically categorize the current state-of-the-art and -practice, and to provide a synthesized overview.
Method
To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study on when and what to automate in software testing. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine.
Results
Our MLR and its results are based on 78 sources, 52 of which were grey literature and 26 were formally published sources. We used the qualitative analysis (coding) to classify the factors affecting the when- and what-to-automate questions to five groups: (1) Software Under Test (SUT)-related factors, (2) test-related factors, (3) test-tool-related factors, (4) human and organizational factors, and (5) cross-cutting and other factors. The most frequent individual factors were: need for regression testing (44 sources), economic factors (43), and maturity of SUT (39).
Conclusion
We show that current decision-support in software test automation provides reasonable advice for industry, and as a practical outcome of this research we have summarized it as a checklist that can be used by practitioners. However, we recommend developing systematic empirically-validated decision-support approaches as the existing advice is often unsystematic and based on weak empirical evidence.}
}
@article{SANTOSROCHA20131355,
title = {The use of software product lines for business process management: A systematic literature review},
journal = {Information and Software Technology},
volume = {55},
number = {8},
pages = {1355-1373},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913000402},
author = {Roberto dos {Santos Rocha} and Marcelo Fantinato},
keywords = {Software product line, PL, Business process management, BPM},
abstract = {Context
Business Process Management (BPM) is a potential domain in which Software Product Line (PL) can be successfully applied. Including the support of Service-oriented Architecture (SOA), BPM and PL may help companies achieve strategic alignment between business and IT.
Objective
Presenting the results of a study undertaken to seek and assess PL approaches for BPM through a Systematic Literature Review (SLR). Moreover, identifying the existence of dynamic PL approaches for BPM.
Method
A SLR was conducted with four research questions formulated to evaluate PL approaches for BPM.
Results
63 papers were selected as primary studies according to the criteria established. From these primary studies, only 15 papers address the specific dynamic aspects in the context evaluated. Moreover, it was found that PLs only partially address the BPM lifecycle since the last business process phase is not a current concern on the found approaches.
Conclusions
The found PL approaches for BPM only cover partially the BPM lifecycle, not taking into account the last phase which restarts the lifecycle. Moreover, no wide dynamic PL proposal was found for BPM, but only the treatment of specific dynamic aspects. The results indicate that PL approaches for BPM are still at an early stage and gaining maturity.}
}
@article{TUZUN2015136,
title = {Analyzing impact of experience curve on ROI in the software product line adoption process},
journal = {Information and Software Technology},
volume = {59},
pages = {136-148},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914002079},
author = {Eray Tüzün and Bedir Tekinerdogan},
keywords = {Experience curve, Learning curve, Software product line engineering, Cost models, Productivity, Software reuse},
abstract = {Context
Experience curve is a well-known concept in management and education science, which explains the phenomenon of increased worker efficiency with repetitive production of a good or service.
Objective
We aim to analyze the impact of the experience curve effect on the Return on Investment (ROI) in the software product line engineering (SPLE) process.
Method
We first present the results of a systematic literature review (SLR) to explicitly depict the studies that have considered the impact of experience curve effect on software development in general. Subsequently, based on the results of the SLR, the experience curve effect models in the literature, and the SPLE cost models, we define an approach for extending the cost models with the experience curve effect. Finally, we discuss the application of the refined cost models in a real industrial context.
Results
The SLR resulted in 15 primary studies which confirm the impact of experience curve effect on software development in general but the experience curve effect in the adoption of SPLE got less attention. The analytical discussion of the cost models and the application of the refined SPLE cost models in the industrial context showed a clear impact of the experience curve effect on the time-to-market, cost of development and ROI in the SPLE adoption process.
Conclusions
The proposed analysis with the newly defined cost models for SPLE adoption provides a more precise analysis tool for the management, and as such helps to support a better decision making.}
}
@article{LWAKATARE2020106368,
title = {Large-scale machine learning systems in real-world industrial settings: A review of challenges and solutions},
journal = {Information and Software Technology},
volume = {127},
pages = {106368},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106368},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301373},
author = {Lucy Ellen Lwakatare and Aiswarya Raj and Ivica Crnkovic and Jan Bosch and Helena Holmström Olsson},
keywords = {Machine learning systems, Software engineering, Industrial settings, Challenges, Solutions, SLR},
abstract = {Background: Developing and maintaining large scale machine learning (ML) based software systems in an industrial setting is challenging. There are no well-established development guidelines, but the literature contains reports on how companies develop and maintain deployed ML-based software systems. Objective: This study aims to survey the literature related to development and maintenance of large scale ML-based systems in industrial settings in order to provide a synthesis of the challenges that practitioners face. In addition, we identify solutions used to address some of these challenges. Method: A systematic literature review was conducted and we identified 72 papers related to development and maintenance of large scale ML-based software systems in industrial settings. The selected articles were qualitatively analyzed by extracting challenges and solutions. The challenges and solutions were thematically synthesized into four quality attributes: adaptability, scalability, safety and privacy. The analysis was done in relation to ML workflow, i.e. data acquisition, training, evaluation, and deployment. Results: We identified a total of 23 challenges and 8 solutions related to development and maintenance of large scale ML-based software systems in industrial settings including six different domains. Challenges were most often reported in relation to adaptability and scalability. Safety and privacy challenges had the least reported solutions. Conclusion: The development and maintenance on large-scale ML-based systems in industrial settings introduce new challenges specific for ML, and for the known challenges characteristic for these types of systems, require new methods in overcoming the challenges. The identified challenges highlight important concerns in ML system development practice and the lack of solutions point to directions for future research.}
}
@article{AZEEM2019115,
title = {Machine learning techniques for code smell detection: A systematic literature review and meta-analysis},
journal = {Information and Software Technology},
volume = {108},
pages = {115-138},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918302623},
author = {Muhammad Ilyas Azeem and Fabio Palomba and Lin Shi and Qing Wang},
keywords = {Code smells, Machine learning, Systematic literature review},
abstract = {Background: Code smells indicate suboptimal design or implementation choices in the source code that often lead it to be more change- and fault-prone. Researchers defined dozens of code smell detectors, which exploit different sources of information to support developers when diagnosing design flaws. Despite their good accuracy, previous work pointed out three important limitations that might preclude the use of code smell detectors in practice: (i) subjectiveness of developers with respect to code smells detected by such tools, (ii) scarce agreement between different detectors, and (iii) difficulties in finding good thresholds to be used for detection. To overcome these limitations, the use of machine learning techniques represents an ever increasing research area. Objective: While the research community carefully studied the methodologies applied by researchers when defining heuristic-based code smell detectors, there is still a noticeable lack of knowledge on how machine learning approaches have been adopted for code smell detection and whether there are points of improvement to allow a better detection of code smells. Our goal is to provide an overview and discuss the usage of machine learning approaches in the field of code smells. Method: This paper presents a Systematic Literature Review (SLR) on Machine Learning Techniques for Code Smell Detection. Our work considers papers published between 2000 and 2017. Starting from an initial set of 2456 papers, we found that 15 of them actually adopted machine learning approaches. We studied them under four different perspectives: (i) code smells considered, (ii) setup of machine learning approaches, (iii) design of the evaluation strategies, and (iv) a meta-analysis on the performance achieved by the models proposed so far. Results: The analyses performed show that God Class, Long Method, Functional Decomposition, and Spaghetti Code have been heavily considered in the literature. Decision Trees and Support Vector Machines are the most commonly used machine learning algorithms for code smell detection. Models based on a large set of independent variables have performed well. JRip and Random Forest are the most effective classifiers in terms of performance. The analyses also reveal the existence of several open issues and challenges that the research community should focus on in the future. Conclusion: Based on our findings, we argue that there is still room for the improvement of machine learning techniques in the context of code smell detection. The open issues emerged in this study can represent the input for researchers interested in developing more powerful techniques.}
}
@article{MOLINARIOS2020106238,
title = {Comparison of development methodologies in web applications},
journal = {Information and Software Technology},
volume = {119},
pages = {106238},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106238},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302551},
author = {Jimmy Molina-Ríos and Nieves Pedreira-Souto},
keywords = {Development methodologies, Web applications, Comparison, Agile methodologies},
abstract = {Context
Web applications development is at its peak due to the advance of technological trends and the constant dependence of the Internet. As a result of the needs of developers, new development methodologies have emerged. However, that does not mean that companies always implement an optimal development process; instead, there are several disadvantages presented by an inadequate and not versatile methodologies.
Objective
The aim is to compare web development methodologies based on dynamic features presented during the life cycle to identify their use, relevance, and characteristics. The process employing is an SLR and field research to Ecuadorian development companies.
Method
The method used is a systematic literature review (SLR) for the identification of characteristics and processes of development methodologies. Additionally, a survey of Ecuadorian web application developers was implemented to assess the importance of using a method during the project.
Results
The literature review exhibited as a result that UWE and OOHDM have greater flexibility than other methodologies before dynamic environments during the web development process. On the other hand, within field research was obtained that companies use different software development methods than those assessed in the study (hybrid methodologies). However, within the range of companies using the compared methodologies, UWE is the most selected.
Conclusions
Each methodology holds particular features and employment environment, which makes them useful in specific conditions. Through the field research, it is possible to conclude that most of the companies use different methodologies than the evaluated ones; thus, the process is guided by hybrids methods or models based on experience. On the other hand, through the SLR, we identified UWE as the most suitable methodology for web development under dynamic environments, such as the size of the company, the need to modify the requirements, or the knowledge that the development team has about the process.}
}
@article{SELLERISILVA201520,
title = {Using CMMI together with agile software development: A systematic review},
journal = {Information and Software Technology},
volume = {58},
pages = {20-43},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914002110},
author = {Fernando {Selleri Silva} and Felipe Santana Furtado Soares and Angela Lima Peres and Ivanildo Monteiro de Azevedo and Ana Paula L.F. Vasconcelos and Fernando Kenji Kamei and Silvio Romero de Lemos Meira},
keywords = {Software process improvement, CMMI, Agile methodology, Benefits, Limitations, Systematic review},
abstract = {Background
The search for adherence to maturity levels by using lightweight processes that require low levels of effort is regarded as a challenge for software development organizations.
Objective
This study seeks to evaluate, synthesize, and present results on the use of the Capability Maturity Model Integration (CMMI) in combination with agile software development, and thereafter to give an overview of the topics researched, which includes a discussion of their benefits and limitations, the strength of the findings, and the implications for research and practice.
Methods
The method applied was a Systematic Literature Review on studies published up to (and including) 2011.
Results
The search strategy identified 3193 results, of which 81 included studies on the use of CMMI together with agile methodologies. The benefits found were grouped into two main categories: those related to the organization in general and those related to the development process, and were organized into subcategories, according to the area to which they refer. The limitations were also grouped into these categories. Using the criteria defined, the strength of the evidence found was considered low. The implications of the results for research and practice are discussed.
Conclusion
Agile methodologies can be used by companies to reduce efforts in getting to levels 2 and 3 of CMMI, there even being reports of applying agile practices that led to achieving level 5. However, agile methodologies alone, according to the studies, were not sufficient to obtain a rating at a given level, it being necessary to resort to additional practices to do so.}
}
@article{PALACIOS2011171,
title = {Testing in Service Oriented Architectures with dynamic binding: A mapping study},
journal = {Information and Software Technology},
volume = {53},
number = {3},
pages = {171-189},
year = {2011},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002168},
author = {Marcos Palacios and José García-Fanjul and Javier Tuya},
keywords = {Software testing, Service Oriented Architectures, SOA, Dynamic binding, Mapping study, Systematic literature review},
abstract = {Context
Service Oriented Architectures (SOA) have emerged as a new paradigm to develop interoperable and highly dynamic applications.
Objective
This paper aims to identify the state of the art in the research on testing in Service Oriented Architectures with dynamic binding.
Method
A mapping study has been performed employing both manual and automatic search in journals, conference/workshop proceedings and electronic databases.
Results
A total of 33 studies have been reviewed in order to extract relevant information regarding a previously defined set of research questions. The detection of faults and the decision making based on the information gathered from the tests have been identified as the main objectives of these studies. To achieve these goals, monitoring and test case generation are the most proposed techniques testing both functional and non-functional properties. Furthermore, different stakeholders have been identified as participants in the tests, which are performed in specific points in time during the life cycle of the services. Finally, it has been observed that a relevant group of studies have not validated their approach yet.
Conclusions
Although we have only found 33 studies that address the testing of SOA where the discovery and binding of the services are performed at runtime, this number can be considered significant due to the specific nature of the reviewed topic. The results of this study have contributed to provide a body of knowledge that allows identifying current gaps in improving the quality of the dynamic binding in SOA using testing approaches.}
}
@article{AYORA2015248,
title = {VIVACE: A framework for the systematic evaluation of variability support in process-aware information systems},
journal = {Information and Software Technology},
volume = {57},
pages = {248-276},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001268},
author = {Clara Ayora and Victoria Torres and Barbara Weber and Manfred Reichert and Vicente Pelechano},
keywords = {Business process, Business process variability, Process-aware information systems, Process family, Systematic literature review},
abstract = {abstract
Context
The increasing adoption of process-aware information systems (PAISs) such as workflow management systems, enterprise resource planning systems, or case management systems, together with the high variability in business processes (e.g., sales processes may vary depending on the respective products and countries), has resulted in large industrial process model repositories. To cope with this business process variability, the proper management of process variants along the entire process lifecycle becomes crucial.
Objective
The goal of this paper is to develop a fundamental understanding of business process variability. In particular, the paper will provide a framework for assessing and comparing process variability approaches and the support they provide for the different phases of the business process lifecycle (i.e., process analysis and design, configuration, enactment, diagnosis, and evolution).
Method
We conducted a systematic literature review (SLR) in order to discover how process variability is supported by existing approaches.
Results
The SLR resulted in 63 primary studies which were deeply analyzed. Based on this analysis, we derived the VIVACE framework. VIVACE allows assessing the expressiveness of a process modeling language regarding the explicit specification of process variability. Furthermore, the support provided by a process-aware information system to properly deal with process model variants can be assessed with VIVACE as well.
Conclusions
VIVACE provides an empirically-grounded framework for process engineers that enables them to evaluate existing process variability approaches as well as to select that variability approach meeting their requirements best. Finally, it helps process engineers in implementing PAISs supporting process variability along the entire process lifecycle.}
}
@article{ROUHANI20151,
title = {A systematic literature review on Enterprise Architecture Implementation Methodologies},
journal = {Information and Software Technology},
volume = {62},
pages = {1-20},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000282},
author = {Babak Darvish Rouhani and Mohd Naz’ri Mahrin and Fatemeh Nikpay and Rodina Binti Ahmad and Pourya Nikfard},
keywords = {Enterprise Architecture Implementation Methodology, SLR, EAIM, Enterprise Architecture, Methodology},
abstract = {Context
Enterprise Architecture (EA) is a strategy to align business and Information Technology (IT) within an enterprise. EA is managed, developed, and maintained throughout the EA Implementation Methodology (EAIM).
Objective
The aims of this study are to identify the existing effective practices that are used by existing EAIMs, identify the factors that affect the effectiveness of EAIM, identify the current tools that are used by existing EAIMs, and identify the open problems and areas related to EAIM for improvement.
Method
A Systematic Literature Review (SLR) was carried out. 669 papers were retrieved by a manual search in 6 databases and 46 primary studies were finally included.
Result
From these studies 33% were journal articles, 41% were conference papers while 26% were contributions from the studies consisted of book chapters. Consequently, 28 practices, 19 factors, and 15 tools were identified and analysed.
Conclusion
Several rigorous researches have been done in order to provide effective EAIM, however there are still problems in components of EAIM, including: there is lack of tool support for whole part of EA implementation, there are deficiency in addressing the EAIM’s practices especially in modeling, management, and maintenance, there is lack of consideration on non-functional requirement in existing EAIM, there is no appropriate consideration on requirement analysis in most existing EAIM. This review provides researchers with some guidelines for future research on this topic. It also provides broad information on EAIM, which could be useful for practitioners.}
}
@article{SHARAFI201579,
title = {A systematic literature review on the usage of eye-tracking in software engineering},
journal = {Information and Software Technology},
volume = {67},
pages = {79-107},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001196},
author = {Zohreh Sharafi and Zéphyrin Soh and Yann-Gaël Guéhéneuc},
keywords = {Eye-tracking, Software engineering, Experiment},
abstract = {Context
Eye-tracking is a mean to collect evidence regarding some participants’ cognitive processes. Eye-trackers monitor participants’ visual attention by collecting eye-movement data. These data are useful to get insights into participants’ cognitive processes during reasoning tasks.
Objective
The Evidence-based Software Engineering (EBSE) paradigm has been proposed in 2004 and, since then, has been used to provide detailed insights regarding different topics in software engineering research and practice. Systematic Literature Reviews (SLR) are also useful in the context of EBSE by bringing together all existing evidence of research and results about a particular topic. This SLR evaluates the current state of the art of using eye-trackers in software engineering and provides evidence on the uses and contributions of eye-trackers to empirical studies in software engineering.
Method
We perform a SLR covering eye-tracking studies in software engineering published from 1990 up to the end of 2014. To search all recognised resources, instead of applying manual search, we perform an extensive automated search using Engineering Village. We identify 36 relevant publications, including nine journal papers, two workshop papers, and 25 conference papers.
Results
The software engineering community started using eye-trackers in the 1990s and they have become increasingly recognised as useful tools to conduct empirical studies from 2006. We observe that researchers use eye-trackers to study model comprehension, code comprehension, debugging, collaborative interaction, and traceability. Moreover, we find that studies use different metrics based on eye-movement data to obtain quantitative measures. We also report the limitations of current eye-tracking technology, which threaten the validity of previous studies, along with suggestions to mitigate these limitations.
Conclusion
However, not withstanding these limitations and threats, we conclude that the advent of new eye-trackers makes the use of these tools easier and less obtrusive and that the software engineering community could benefit more from this technology.}
}
@article{NIAZI20161,
title = {Challenges of project management in global software development: A client-vendor analysis},
journal = {Information and Software Technology},
volume = {80},
pages = {1-19},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916301227},
author = {Mahmood Niazi and Sajjad Mahmood and Mohammad Alshayeb and Mohammed Rehan Riaz and Kanaan Faisal and Narciso Cerpa and Siffat Ullah Khan and Ita Richardson},
keywords = {Global software development, Software project management, Challenges and barriers, Systematic literature review, Empirical study},
abstract = {Context
Global Software Development (GSD) is the process whereby software is developed by different teams located in various parts of the globe. One of the major reasons for GSD project failure is that a number of organizations endorse global development prior to understanding project management challenges for the global activity.
Objective
The objective of this paper is to identify the challenges, from the client and vendor perspectives, which can undermine the successful management of GSD projects.
Method
We followed a two-phase approach: we first identified the challenges via a Systematic Literature Review (SLR) and then the identified challenges were validated using a questionnaire-based survey.
Results
Through both approaches, we identified 19 challenges important to the success of GSD project management. A comparison of the challenges identified in client and vendor organizations indicates that there are more similarities than differences between the challenges. Our results show a positive correlation between the ranks obtained from the SLR and the questionnaire ((rs(19) = 0.102), p = 0.679). The results of t-test (i.e., t = 0.299, p = 0.768>0.05) show that there is no significant difference between the findings of SLR and questionnaire.
Conclusions
GSD organizations should try to address the identified challenges when managing their global software development activities to increase the probability of project success.}
}
@article{DEMAGALHAES201576,
title = {Investigations about replication of empirical studies in software engineering: A systematic mapping study},
journal = {Information and Software Technology},
volume = {64},
pages = {76-101},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000300},
author = {Cleyton V.C. {de Magalhães} and Fabio Q.B. {da Silva} and Ronnie E.S. Santos and Marcos Suassuna},
keywords = {Replications, Experiments, Empirical studies, Mapping study, Systematic literature review, Software engineering},
abstract = {Context
Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012).
Objective
In this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research.
Method
We applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996–2012 complemented by manual and automatic search procedures that collected articles published in 2013.
Results
We analyzed 37 papers reporting studies about replication published in the last 17years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012.
Conclusions
Replication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.}
}
@article{TUZUN201577,
title = {Empirical evaluation of a decision support model for adopting software product line engineering},
journal = {Information and Software Technology},
volume = {60},
pages = {77-101},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000026},
author = {Eray Tüzün and Bedir Tekinerdogan and Mert Emin Kalender and Semih Bilgen},
keywords = {Software product line engineering, Software product line transition strategies, Software product line engineering feasibility analysis, Decision support system, Systematic literature review, Case study design},
abstract = {Context
The software product line engineering (SPLE) community has provided several different approaches for assessing the feasibility of SPLE adoption and selecting transition strategies. These approaches usually include many rules and guidelines which are very often implicit or scattered over different publications. Hence, for the practitioners it is not always easy to select and use these rules to support the decision making process. Even in case the rules are known, the lack of automated support for storing and executing the rules seriously impedes the decision making process.
Objective
We aim to evaluate the impact of a decision support system (DSS) on decision-making in SPLE adoption. In alignment with this goal, we provide a decision support model (DSM) and the corresponding DSS.
Method
First, we apply a systematic literature review (SLR) on the existing primary studies that discuss and present approaches for analyzing the feasibility of SPLE adoption and transition strategies. Second, based on the data extraction and synthesis activities of the SLR, the required questions and rules are derived and implemented in the DSS. Third, for validation of the approach we conduct multiple case studies.
Results
In the course of the SLR, 31 primary studies were identified from which we could construct 25 aspects, 39 questions and 312 rules. We have developed the DSS tool Transit-PL that embodies these elements.
Conclusions
The multiple case study validation showed that the adoption of the developed DSS tool is justified to support the decision making process in SPLE adoption.}
}
@article{MAHMOOD2018148,
title = {Reproducibility and replicability of software defect prediction studies},
journal = {Information and Software Technology},
volume = {99},
pages = {148-163},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304202},
author = {Zaheed Mahmood and David Bowes and Tracy Hall and Peter C.R. Lane and Jean Petrić},
keywords = {Replication, Reproducibility, Software defect prediction},
abstract = {Context: Replications are an important part of scientific disciplines. Replications test the credibility of original studies and can separate true results from those that are unreliable. Objective: In this paper we investigate the replication of defect prediction studies and identify the characteristics of replicated studies. We further assess how defect prediction replications are performed and the consistency of replication findings. Method: Our analysis is based on tracking the replication of 208 defect prediction studies identified by a highly cited Systematic Literature Review (SLR) [1]. We identify how often each of these 208 studies has been replicated and determine the type of replication carried out. We identify quality, citation counts, publication venue, impact factor, and data availability from all 208 SLR defect prediction papers to see if any of these factors are associated with the frequency with which they are replicated. Results: Only 13 (6%) of the 208 studies are replicated. Replication seems related to original papers appearing in the Transactions of Software Engineering (TSE) journal. The number of citations an original paper had was also an indicator of replications. In addition, studies conducted using closed source data seems to have more replications than those based on open source data. Where a paper has been replicated, 11 (38%) out of 29 studies revealed different results to the original study. Conclusion: Very few defect prediction studies are replicated. The lack of replication means that it remains unclear how reliable defect prediction is. We provide practical steps for improving the state of replication.}
}
@article{NAIR2014689,
title = {An extended systematic literature review on provision of evidence for safety certification},
journal = {Information and Software Technology},
volume = {56},
number = {7},
pages = {689-717},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000603},
author = {Sunil Nair and Jose Luis {de la Vara} and Mehrdad Sabetzadeh and Lionel Briand},
keywords = {Safety–critical systems, Safety standards, Safety compliance, Safety certification, Safety evidence, Systematic literature review},
abstract = {Context
Critical systems in domains such as aviation, railway, and automotive are often subject to a formal process of safety certification. The goal of this process is to ensure that these systems will operate safely without posing undue risks to the user, the public, or the environment. Safety is typically ensured via complying with safety standards. Demonstrating compliance to these standards involves providing evidence to show that the safety criteria of the standards are met.
Objective
In order to cope with the complexity of large critical systems and subsequently the plethora of evidence information required for achieving compliance, safety professionals need in-depth knowledge to assist them in classifying different types of evidence, and in structuring and assessing the evidence. This paper is a step towards developing such a body of knowledge that is derived from a large-scale empirically rigorous literature review.
Method
We use a Systematic Literature Review (SLR) as the basis for our work. The SLR builds on 218 peer-reviewed studies, selected through a multi-stage process, from 4963 studies published between 1990 and 2012.
Results
We develop a taxonomy that classifies the information and artefacts considered as evidence for safety. We review the existing techniques for safety evidence structuring and assessment, and further study the relevant challenges that have been the target of investigation in the academic literature. We analyse commonalities in the results among different application domains and discuss implications of the results for both research and practice.
Conclusion
The paper is, to our knowledge, the largest existing study on the topic of safety evidence. The results are particularly relevant to practitioners seeking a better grasp on evidence requirements as well as to researchers in the area of system safety. As a major finding of the review, the results strongly suggest the need for more practitioner-oriented and industry-driven empirical studies in the area of safety certification.}
}
@article{NGUYENDUC2015277,
title = {The impact of global dispersion on coordination, team performance and software quality – A systematic literature review},
journal = {Information and Software Technology},
volume = {57},
pages = {277-294},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001414},
author = {Anh Nguyen-Duc and Daniela S. Cruzes and Reidar Conradi},
keywords = {Global software development, Systematic literature review, Global dispersion, Performance, Software quality, Meta analysis},
abstract = {Context
Global software development (GSD) contains different context setting dimensions, which are essential for effective teamwork and success of projects. Although considerable research effort has been made in this area, as yet, no agreement has been reached about the impact of these dispersion dimensions on team coordination and project outcomes.
Objective
This paper summarizes empirical evidence on the impact of global dispersion dimensions on coordination, team performance and project outcomes.
Method
We performed a systematic literature review of 46 publications from 25 journals and 19 conference and workshop proceedings, which were published between 2001 and 2013. Thematic analysis was used to identify global dimensions and their measures. Vote counting was used to decide on the impact trends of dispersion dimensions on team performance and software quality.
Results
Global dispersion dimensions are consistently conceptualized, but quantified in many different ways. Different dispersion dimensions are associated with a distinct set of coordination challenges. Overall, geographical dispersion tends to have a negative impact on team performance and software quality. Temporal dispersion tends to have a negative impact on software quality, but its impact on team performance is inconsistent and can be explained by type of performance.
Conclusion
For researchers, we reveal several opportunities for future research, such as coordination challenges in inter-organizational software projects, impact of processes and practices mismatches on project outcomes, evolution of coordination needs and mechanism over time and impact of dispersion dimensions on open source project outcomes. For practitioners, they should consider the tradeoff between cost and benefits while dispersing tasks, alignment impact of dispersion dimensions with individual and organizational objectives, coordination mechanisms as situational approaches and collocation of development activities of high quality demand components in GSD projects.}
}
@article{VERNER201454,
title = {Risks and risk mitigation in global software development: A tertiary study},
journal = {Information and Software Technology},
volume = {56},
number = {1},
pages = {54-78},
year = {2014},
note = {Special sections on International Conference on Global Software Engineering – August 2011 and Evaluation and Assessment in Software Engineering – April 2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913001341},
author = {J.M. Verner and O.P. Brereton and B.A. Kitchenham and M. Turner and M. Niazi},
keywords = {Global software development, Systematic literature review, Tertiary review, Risk, Risk mitigation, Evidence},
abstract = {Context There is extensive interest in global software development (GSD) which has led to a large number of papers reporting on GSD. A number of systematic literature reviews (SLRs) have attempted to aggregate information from individual studies.
Objective
We wish to investigate GSD SLR research with a focus on discovering what research has been conducted in the area and to determine if the SLRs furnish appropriate risk and risk mitigation advice to provide guidance to organizations involved with GSD.
Method
We performed a broad automated search to identify GSD SLRs. Data extracted from each study included: (1) authors, their affiliation and publishing venue, (2) SLR quality, (3) research focus, (4) GSD risks, (5) risk mitigation strategies and, (6) for each SLR the number of primary studies reporting each risk and risk mitigation strategy.
Results
We found a total of 37 papers reporting 24 unique GSD SLR studies. Major GSD topics covered include: (1) organizational environment, (2) project execution, (3) project planning and control and (4) project scope and requirements. We extracted 85 risks and 77 risk mitigation advice items and categorized them under four major headings: outsourcing rationale, software development, human resources, and project management. The largest group of risks was related to project management. GSD outsourcing rationale risks ranked highest in terms of primary study support but in many cases these risks were only identified by a single SLR.
Conclusions
The focus of the GSD SLRs we identified is mapping the research rather than providing evidence-based guidance to industry. Empirical support for the majority of risks identified is moderate to low, both in terms of the number of SLRs identifying the risk, and in the number of primary studies providing empirical support. Risk mitigation advice is also limited, and empirical support for these items is low.}
}
@article{IDRI2015206,
title = {Analogy-based software development effort estimation: A systematic mapping and review},
journal = {Information and Software Technology},
volume = {58},
pages = {206-230},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001815},
author = {Ali Idri and Fatima azzahra Amazal and Alain Abran},
keywords = {Mapping study, Systematic literature review, Software development effort estimation, Analogy, Case-based reasoning},
abstract = {Context
Analogy-based Software development Effort Estimation (ASEE) techniques have gained considerable attention from the software engineering community. However, existing systematic map and review studies on software development effort prediction have not investigated in depth several issues of ASEE techniques, to the exception of comparisons with other types of estimation techniques.
Objective
The objective of this research is twofold: (1) to classify ASEE studies which primary goal is to propose new or modified ASEE techniques according to five criteria: research approach, contribution type, techniques used in combination with ASEE methods, and ASEE steps, as well as identifying publication channels and trends and (2) to analyze these studies from five perspectives: estimation accuracy, accuracy comparison, estimation context, impact of the techniques used in combination with ASEE methods, and ASEE tools.
Method
We performed a systematic mapping of studies for which the primary goal is to develop or to improve ASEE techniques published in the period 1990–2012, and reviewed them based on an automated search of four electronic databases.
Results
In total, we identified 65 studies published between 1990 and 2012, and classified them based on our predefined classification criteria. The mapping study revealed that most researchers focus on addressing problems related to the first step of an ASEE process, that is, feature and case subset selection. The results of our detailed analysis show that ASEE methods outperform the eight techniques with which they were compared, and tend to yield acceptable results especially when combining ASEE techniques with Fuzzy Logic (FL) or Genetic Algorithms (GA).
Conclusion
Based on the findings of this study, the use of other techniques such FL and GA in combination with an ASEE method is promising to generate more accurate estimates. However, the use of ASEE techniques by practitioners is still limited: developing more ASEE tools may facilitate the application of these techniques and then lead to increasing the use of ASEE techniques in industry.}
}
@article{TIWARI2015128,
title = {A systematic literature review of use case specifications research},
journal = {Information and Software Technology},
volume = {67},
pages = {128-158},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001081},
author = {Saurabh Tiwari and Atul Gupta},
keywords = {Use case specifications, Use case templates, Guidelines, Systematic reviews, Evolution, Quality},
abstract = {Context
Use cases have been widely accepted and acknowledged as a specification tool for specifying the functional requirements of a software system. Many variations of use cases exist which tries to address the issues such as their completeness, degree of formalism, automated information extraction, usability, and pertinence.
Objective
The aim of this systematic review is to examine the existing literature for the evolution of the use cases, their applications, quality assessments, open issues, and the future directions.
Method
We perform keyword-based extensive search to identify the relevant studies related to use case specifications research reported in journal articles, conference papers, workshop papers, bulletins and book chapters.
Results
The specified search process resulted 119 papers, which were published between 1992 and February 2014. This included, 54 journal articles, 42 conference papers, 2 ACM/IEEE bulletins, 12 book chapters, 6 workshop papers and 3 white papers. We found that as many as twenty use case templates have been proposed and applied for various software specification problems ranging from informal descriptions with paragraph-style text to more formal keyword-oriented templates.
Conclusion
Use cases have been evolved from initial plain, semi-formal textual descriptions to a more formal template structure facilitating automated information extraction in various software development life cycle activities such as requirement documentation, requirement analysis, requirement validation, domain modeling, test case generation, planning and estimation, and maintenance. The issues that remain to be sorted out are (1) the right degree of formalism, (2) the efficient change management, (3) the industrial relevance, and (4) assessment of the quality of the specification. Additionally, its synergy with other software models that are used in the development processes is an issue that needs to be addressed.}
}
@article{KITCHENHAM2011638,
title = {Using mapping studies as the basis for further research – A participant-observer case study},
journal = {Information and Software Technology},
volume = {53},
number = {6},
pages = {638-651},
year = {2011},
note = {Special Section: Best papers from the APSEC},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002272},
author = {Barbara A. Kitchenham and David Budgen and O. {Pearl Brereton}},
keywords = {Case study, Systematic literature review, Mapping studies, Software engineering},
abstract = {Context
We are strong advocates of evidence-based software engineering (EBSE) in general and systematic literature reviews (SLRs) in particular. We believe it is essential that the SLR methodology is used constructively to support software engineering research.
Objective
This study aims to assess the value of mapping studies which are a form of SLR that aims to identify and categorise the available research on a broad software engineering topic.
Method
We used a multi-case, participant-observer case study using five examples of studies that were based on preceding mapping studies. We also validated our results by contacting two other researchers who had undertaken studies based on preceding mapping studies and by assessing review comments related to our follow-on studies.
Results
Our original case study identified 11 unique benefits that can accrue from basing research on a preceding mapping study of which only two were case specific. We also identified nine problems associated with using preceding mapping studies of which two were case specific. These results were consistent with the information obtained from the validation activities. We did not find an example of an independent research group making use of a mapping study produced by other researchers.
Conclusion
Mapping studies can save time and effort for researchers and provide baselines to assist new research efforts. However, they must be of high quality in terms of completeness and rigour if they are to be a reliable basis for follow-on research.}
}
@article{ZHANG2021106607,
title = {Processes, challenges and recommendations of Gray Literature Review: An experience report},
journal = {Information and Software Technology},
volume = {137},
pages = {106607},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106607},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000847},
author = {He Zhang and Runfeng Mao and Huang Huang and Qiming Dai and Xin Zhou and Haifeng Shen and Guoping Rong},
keywords = {Gray literature review, Evidence-based software engineering, DevSecOps},
abstract = {Context:
Systematic Literature Review (SLR), as a tool of Evidence-Based Software Engineering (EBSE), has been widely used in Software Engineering (SE). However, for certain topics in SE, especially those that are trendy or industry driven, academic literature is generally scarce and consequently Gray Literature (GL) becomes a major source of evidence. In recent years, the adoption of Gray Literature Review (GLR) or Multivocal Literature Review (MLR) is rising steadily to provide the state-of-the-practice of a specific topic where SLR is not a viable option.
Objective:
Although some SLR guidelines recommend the use of GL and several MLR guidelines have already been proposed in SE, researchers still have conflicting views on the value of GL and commonly accepted GLR or MLR studies are generally lacking in terms of publication. This experience report aims to shed some light on GLR through a case study that uses SLR and MLR guidelines to conduct a GLR on an emerging topic in SE to specifically answer the questions related to the reasons of using GL, the processes of conducting GL, and the impacts of GL on review results.
Method:
We retrospect the review process of conducting a GLR on the topic of DevSecOps with reference to Kitchenham’s SLR and Garousi’s MLR guidelines. We specifically reflect on the processes we had to adapt in order to tackle the challenges we faced. We also compare and contrast our GLR with existing MLRs or GLRs in SE to contextualize our reflections.
Results:
We distill ten challenges in nine activities of a GLR process. We provide reasons for these challenges and further suggest ways to tackle them during a GLR process. We also discuss the decision process of selecting a suitable review methodology among SLR, MLR and GLR and elaborate the impacts of GL on our review results.
Conclusion:
Although our experience on GLR is mainly derived from a specific case study on DevSecOps, we conjecture that it is relevant and would be beneficial to other GLR or MLR studies. We also expect our experience would contribute to future GLR or MLR guidelines, in a way similar to how SLR guidelines learned from the SLR experience report a dozen years ago. In addition, other researchers may find our decision making process useful before they conduct their own reviews.}
}
@article{GAROUSI2022106697,
title = {Introduction to the Special Issue on: Grey Literature and Multivocal Literature Reviews (MLRs) in software engineering},
journal = {Information and Software Technology},
volume = {141},
pages = {106697},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106697},
url = {https://www.sciencedirect.com/science/article/pii/S095058492100152X},
author = {Vahid Garousi and Austen Rainer and Michael Felderer and Mika V. Mäntylä},
keywords = {Grey literature, Multivocal literature review, Evidence-based software engineering, Epistemology},
abstract = {In parallel to academic (peer-reviewed) literature (e.g., journal and conference papers), an enormous extent of grey literature (GL) has accumulated since the inception of software engineering (SE). GL is often defined as “literature that is not formally published in sources such as books or journal articles”, e.g., in the form of trade magazines, online blog-posts, technical reports, and online videos such as tutorial and presentation videos. GL is typically produced by SE practitioners. We have observed that researchers are increasingly using and benefitting from the knowledge available within GL. Related to the notion of GL is the notion of Multivocal Literature Reviews (MLRs) in SE, i.e., a MLR is a form of a Systematic Literature Review (SLR) which includes knowledge and/or evidence from the GL in addition to the peer-reviewed literature. MLRs are useful for both researchers and practitioners because they provide summaries of both the state-of-the-art and -practice in a given area. MLRs are popular in other fields and have started to appear in SE community. It is timely then for a Special Issue (SI) focusing on GL and MLRs in SE. From the pool of 13 submitted papers, and after following a rigorous peer review process, seven papers were accepted for this SI. In this introduction we provide a brief overview of GL and MLRs in SE, and then a brief summary of the seven papers published in this SI.}
}
@article{KHATIBSYARBINI201874,
title = {Test case prioritization approaches in regression testing: A systematic literature review},
journal = {Information and Software Technology},
volume = {93},
pages = {74-93},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916304888},
author = {Muhammad Khatibsyarbini and Mohd Adham Isa and Dayang N.A. Jawawi and Rooster Tumeng},
keywords = {Test case prioritization, Regression testing, Software testing, Systematic literature review},
abstract = {Context
Software quality can be assured by going through software testing process. However, software testing phase is an expensive process as it consumes a longer time. By scheduling test cases execution order through a prioritization approach, software testing efficiency can be improved especially during regression testing.
Objective
It is a notable step to be taken in constructing important software testing environment so that a system's commercial value can increase. The main idea of this review is to examine and classify the current test case prioritization approaches based on the articulated research questions.
Method
Set of search keywords with appropriate repositories were utilized to extract most important studies that fulfill all the criteria defined and classified under journal, conference paper, symposiums and workshops categories. 69 primary studies were nominated from the review strategy.
Results
There were 40 journal articles, 21 conference papers, three workshop articles, and five symposium articles collected from the primary studies. As for the result, it can be said that TCP approaches are still broadly open for improvements. Each approach in TCP has specified potential values, advantages, and limitation. Additionally, we found that variations in the starting point of TCP process among the approaches provide a different timeline and benefit to project manager to choose which approaches suite with the project schedule and available resources.
Conclusion
Test case prioritization has already been considerably discussed in the software testing domain. However, it is commonly learned that there are quite a number of existing prioritization techniques that can still be improved especially in data used and execution process for each approach.}
}
@article{ALZUBIDY201772,
title = {Vision for SLR tooling infrastructure: Prioritizing value-added requirements},
journal = {Information and Software Technology},
volume = {91},
pages = {72-81},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916304645},
author = {Ahmed Al-Zubidy and Jeffrey C. Carver and David P. Hale and Edgar E. Hassler},
keywords = {Systematic literature review, Empirical software engineering, Tooling infrastructure},
abstract = {Context
Even with the increasing use of Systematic Literature Reviews (SLR) in software engineering (SE), there are still a number of barriers faced by SLR authors. These barriers increase the cost of conducting SLRs.
Objective
For many of these barriers, appropriate tool support could reduce their impact. In this paper, we use interactions with the SLR community in SE to identify and prioritize a set of requirements for SLR tooling infrastructure.
Method
This paper analyzes and combines the results from three studies on SLR process barriers and SLR tool requirements to produce a prioritized list of functional requirements for SLR tool support. Using this list of requirements, we perform a feature analysis of the current SLR support tools to identify requirements that are supported as well as identify the need for additional tooling infrastructure.
Results
The analysis resulted in a list 112 detailed requirements (consolidated into a set of composite requirements) that SE community desires in SLR support tools. The requirements span all the phases of the SLR process. The results show that, while recent tools cover more of the requirements, there are a number of high-priority requirements that are not yet fully covered by any of the existing tools.
Conclusion
The existing set of SLR tools do not cover all the requirements posed by the community. The list of requirements in this paper is useful for tool developers and researchers wishing to provide support to the SLR community with SE.}
}
@article{KHAN2011693,
title = {Barriers in the selection of offshore software development outsourcing vendors: An exploratory study using a systematic literature review},
journal = {Information and Software Technology},
volume = {53},
number = {7},
pages = {693-706},
year = {2011},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910001527},
author = {Siffat Ullah Khan and Mahmood Niazi and Rashid Ahmad},
keywords = {Systematic literature review, Software development outsourcing, Vendors, Barriers},
abstract = {Context
Software development outsourcing is a contract-based relationship between client and vendor organisations in which a client contracts out all or part of its software development activities to a vendor, who provides agreed services for remuneration.
Objective
The objective is to identify various barriers that have a negative impact on software outsourcing clients in the selection process of offshore software development outsourcing vendors.
Method
We have performed a systematic literature review (SLR) process for the identification of barriers. We have performed all the SLR steps such as the protocol development, initial selection, final selection, quality assessment, data extraction and data synthesis.
Results
We have identified barriers such as ‘language and cultural barriers’, ‘country instability’, ‘lack of project management’, ‘lack of protection for intellectual property rights’ and ‘lack of technical capability’ that generally have a negative impact on outsourcing clients. We have identified only one common frequently cited barrier in three types of organisations (i.e. small, medium and large) which is ‘language and cultural barriers’. We did not identify any common frequently cited barrier in three continents (Asia, North America and Europe) and in two decades (1990–1999 and 2000–mid 2008). The results also reveal the similarities and differences in the barriers identified through different study strategies.
Conclusions
Vendors should address frequently cited barriers such as ‘language and cultural barriers’, ‘country instability’, ‘lack of project management’, ‘lack of protection for intellectual property rights’ and ‘lack of technical capability’ in order to compete in the offshore outsourcing business.}
}
@article{HODA201760,
title = {Systematic literature reviews in agile software development: A tertiary study},
journal = {Information and Software Technology},
volume = {85},
pages = {60-70},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917300538},
author = {Rashina Hoda and Norsaremah Salleh and John Grundy and Hui Mien Tee},
keywords = {Agile software development, Tertiary study, Systematic literature reviews, Mapping study},
abstract = {Context
A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist.
Objective
The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners.
Method
We followed the tertiary study guidelines by Kitchenham et al. to find SLRs published between late 1990s to December 2015.
Results
We found 28 SLRs focusing on ten different ASD research areas: adoption, methods, practices, human and social aspects, CMMI, usability, global software engineering (GSE), organizational agility, embedded systems, and software product line engineering. The number of SLRs on ASD topics, similar to those on software engineering (SE) topics in general, is on the rise. A majority of the SLRs applied standardized guidelines and the quality of these SLRs on ASD topics was found to be slightly higher for journal publications than for conferences. While some individuals and institutions seem to lead this area, the spread of authors and institutions is wide. With respect to prior review recommendations, significant progress was noticed in the area of connecting agile to established domains such as usability, CMMI, and GSE; and considerable progress was observed in focusing on management-oriented approaches as Scrum and sustaining ASD in different contexts such as embedded systems.
Conclusion
SLRs of ASD studies are on the rise and cover a variety of ASD aspects, ranging from early adoption issues to newer applications of ASD such as in product line engineering. ASD research can benefit from further primary and secondary studies on evaluating benefits and challenges of ASD methods, agile hybrids in large-scale setups, sustainability, motivation, teamwork, and project management; as well as a fresh review of empirical studies in ASD to cover the period post 2008.}
}
@article{SANTOS201778,
title = {Benefits and limitations of project-to-project job rotation in software organizations: A synthesis of evidence},
journal = {Information and Software Technology},
volume = {89},
pages = {78-96},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917303646},
author = {Ronnie E.S. Santos and Fabio Q.B. {da Silva} and Maria Teresa Baldassarre and Cleyton V.C. {de Magalhães}},
keywords = {Job rotation, Work design, Software engineering, Systematic literature review, Case study, Replication},
abstract = {Context
Job rotation has been proposed as a managerial practice to be applied in the organizational environment to reduce job monotony, boredom, and exhaustion resulting from job simplification, specialization, and repetition. The scientific literature distinguishes between job-to-job and project-to-project rotations. Despite the potential benefits and its actual use on behalf of software companies, we do not have an accumulated body of scientific knowledge about benefits and limitations of job rotation in software engineering practice. In particular, we have no concrete empirical evidence about the use of project-to-project rotations in practice.
Goal
We aim to identify and discuss evidence about project-to-project (P2P) job rotation, in order to understand the potential benefits and limitations of this practice in software organizations.
Method
We deployed a mix-method research strategy to collect and analyze empirical evidence from the scientific literature, performing a systematic literature review, on one hand and from industrial practice, performing qualitative case studies on the other. We synthesized the evidence using techniques from meta-ethnography.
Results
We found eight benefits, nine limitations, and two factors classified as both benefits and limitations of P2P rotations in software engineering. Different research methods yielded confirmatory and complementary evidence, emphasizing the importance of conducting mix-method research. We found no contradictory evidence and five factors were identified in more than one study using different research methods, contributing to the strength of the evidence.
Conclusion
We synthesized evidence from multiple sources and used different research methods concerning the benefits and limitations of P2P rotation in software engineering practice. Our findings show that rotation tends to benefit important job outcomes, such as motivation, and to decrease job monotony. The main limitations were associated with the potential increase in intra-group social conflicts, individual cognitive effort, and workload, and a temporary decrease in productivity.}
}
@article{RINKEVICS2013267,
title = {Equality in cumulative voting: A systematic review with an improvement proposal},
journal = {Information and Software Technology},
volume = {55},
number = {2},
pages = {267-287},
year = {2013},
note = {Special Section: Component-Based Software Engineering (CBSE), 2011},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912001589},
author = {K. Riņķevičs and R. Torkar},
keywords = {Cumulative voting, Prioritization, Requirements engineering, Compositional data, Log-ratio},
abstract = {Context
Prioritization is an essential part of requirements engineering, software release planning and many other software engineering disciplines. Cumulative Voting (CV) is known as a relatively simple method for prioritizing requirements on a ratio scale. Historically, CV has been applied in decision-making in government elections, corporate governance, and forestry. However, CV prioritization results are of a special type of data—compositional data.
Objectives
The purpose of this study is to aid decision-making by collecting knowledge on the empirical use of CV and develop a method for detecting prioritization items with equal priority.
Methods
We present a systematic literature review of CV and CV analysis methods. The review is based on searching electronic databases and snowball sampling of the found primary studies. Relevant studies are selected based on titles, abstracts, and full text inspection. Additionally, we propose Equality of Cumulative Votes (ECVs)—a CV result analysis method that identifies prioritization items with equal priority.
Results
CV has been used in not only requirements prioritization and release planning but also in e.g. software process improvement, change impact analysis and model driven software development. The review presents a collection of state of the practice studies and CV result analysis methods. In the end, ECV was applied to 27 prioritization cases from 14 studies and identified nine groups of equal items in three studies.
Conclusions
We believe that the analysis of the collected studies and the CV result analysis methods can help in the adoption of CV prioritization method. The evaluation of ECV indicates that it is able to detect prioritization items with equal priority and thus provide the practitioner with a more fine-grained analysis.}
}
@article{OVEREEM2022106890,
title = {API-m-FAMM: A focus area maturity model for API Management},
journal = {Information and Software Technology},
volume = {147},
pages = {106890},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106890},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000532},
author = {Michiel Overeem and Max Mathijssen and Slinger Jansen},
keywords = {API Management, Maturity model, Focus area maturity models},
abstract = {Context:
Organizations are increasingly connecting software applications using Application Programming Interfaces (APIs) to share data, services, functionality, and even complete business processes. However, the creation and management of APIs is non-trivial. Aspects such as traffic management, community engagement, documentation, and version management are often rushed afterthoughts.
Objective:
In this research, we present and evaluate a focus area maturity model for API Management (API-m-FAMM). A focus area maturity model can be used to establish the maturity level of an organization in a specific functional domain described through a number of areas. The API-m-FAMM addresses the areas Lifecycle Management, Security, Performance, Observability, Community, and Commercial.
Method:
The model is constructed using established methods for the design of a focus area maturity model. It is grounded in literature and practice, and was developed and evaluated through a systematic literature Review, eleven expert interviews, and five case studies at software producing organizations.
Result:
The model is described in detail, and its application is illustrated by six case studies.
Conclusions:
The evaluations are reported on, and show that the API-m-FAMM is an efficient tool for aiding organizations in gaining a better understanding of their current implementation of API management practices, and provides them with guidance towards higher levels of maturity. The detailed description of the construction of the API-m-FAMM gives researchers an example to further support the available methodologies, specifically how to combine design science research with these methodologies. Additionally, this study’s unique case study design shows that maturity models can be successfully deployed in practice with minimal involvement of researchers. The focus area maturity model for API Management is maintained on www.maturitymodels.org, allowing practitioners to benefit from its useful insights.}
}
@article{FREGNAN2019159,
title = {A survey on software coupling relations and tools},
journal = {Information and Software Technology},
volume = {107},
pages = {159-178},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918302441},
author = {Enrico Fregnan and Tobias Baum and Fabio Palomba and Alberto Bacchelli},
keywords = {Software engineering, Coupling relations, Software metrics},
abstract = {Context
Coupling relations reflect the dependencies between software entities and can be used to assess the quality of a program. For this reason, a vast amount of them has been developed, together with tools to compute their related metrics. However, this makes the coupling measures suitable for a given application challenging to find.
Goals
The first objective of this work is to provide a classification of the different kinds of coupling relations, together with the metrics to measure them. The second consists in presenting an overview of the tools proposed until now by the software engineering academic community to extract these metrics.
Method
This work constitutes a systematic literature review in software engineering. To retrieve the referenced publications, publicly available scientific research databases were used. These sources were queried using keywords inherent to software coupling. We included publications from the period 2002 to 2017 and highly cited earlier publications. A snowballing technique was used to retrieve further related material.
Results
Four groups of coupling relations were found: structural, dynamic, semantic and logical. A fifth set of coupling relations includes approaches too recent to be considered an independent group and measures developed for specific environments. The investigation also retrieved tools that extract the metrics belonging to each coupling group.
Conclusion
This study shows the directions followed by the research on software coupling: e.g., developing metrics for specific environments. Concerning the metric tools, three trends have emerged in recent years: use of visualization techniques, extensibility and scalability. Finally, some coupling metrics applications were presented (e.g., code smell detection), indicating possible future research directions. Public preprint [https://doi.org/10.5281/zenodo.2002001].}
}
@article{BEHUTIYE2020106225,
title = {Management of quality requirements in agile and rapid software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {123},
pages = {106225},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106225},
url = {https://www.sciencedirect.com/science/article/pii/S095058491930240X},
author = {Woubshet Behutiye and Pertti Karhapää and Lidia López and Xavier Burgués and Silverio Martínez-Fernández and Anna Maria Vollmer and Pilar Rodríguez and Xavier Franch and Markku Oivo},
keywords = {Quality requirements, Non-functional requirements, Agile software development, Rapid software development, Systematic mapping study, Systematic literature reviews},
abstract = {Context
Quality requirements (QRs) describe the desired quality of software, and they play an important role in the success of software projects. In agile software development (ASD), QRs are often ill-defined and not well addressed due to the focus on quickly delivering functionality. Rapid software development (RSD) approaches (e.g., continuous delivery and continuous deployment), which shorten delivery times, are more prone to neglect QRs. Despite the significance of QRs in both ASD and RSD, there is limited synthesized knowledge on their management in those approaches.
Objective
This study aims to synthesize state-of-the-art knowledge about QR management in ASD and RSD, focusing on three aspects: bibliometric, strategies, and challenges.
Research method
Using a systematic mapping study with a snowballing search strategy, we identified and structured the literature on QR management in ASD and RSD.
Results
We found 156 primary studies: 106 are empirical studies, 16 are experience reports, and 34 are theoretical studies. Security and performance were the most commonly reported QR types. We identified various QR management strategies: 74 practices, 43 methods, 13 models, 12 frameworks, 11 advices, 10 tools, and 7 guidelines. Additionally, we identified 18 categories and 4 non-recurring challenges of managing QRs. The limited ability of ASD to handle QRs, time constraints due to short iteration cycles, limitations regarding the testing of QRs and neglect of QRs were the top categories of challenges.
Conclusion
Management of QRs is significant in ASD and is becoming important in RSD. This study identified research gaps, such as the need for more tools and guidelines, lightweight QR management strategies that fit short iteration cycles, investigations of the link between QRs challenges and technical debt, and extension of empirical validation of existing strategies to a wider context. It also synthesizes QR management strategies and challenges, which may be useful for practitioners.}
}
@article{DING2014545,
title = {Knowledge-based approaches in software documentation: A systematic literature review},
journal = {Information and Software Technology},
volume = {56},
number = {6},
pages = {545-567},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000196},
author = {Wei Ding and Peng Liang and Antony Tang and Hans {van Vliet}},
keywords = {Knowledge-based approach, Software documentation, Systematic literature review, Knowledge activity, Software architecture design},
abstract = {Context
Software documents are core artifacts produced and consumed in documentation activity in the software lifecycle. Meanwhile, knowledge-based approaches have been extensively used in software development for decades, however, the software engineering community lacks a comprehensive understanding on how knowledge-based approaches are used in software documentation, especially documentation of software architecture design.
Objective
The objective of this work is to explore how knowledge-based approaches are employed in software documentation, their influences to the quality of software documentation, and the costs and benefits of using these approaches.
Method
We use a systematic literature review method to identify the primary studies on knowledge-based approaches in software documentation, following a pre-defined review protocol.
Results
Sixty studies are finally selected, in which twelve quality attributes of software documents, four cost categories, and nine benefit categories of using knowledge-based approaches in software documentation are identified. Architecture understanding is the top benefit of using knowledge-based approaches in software documentation. The cost of retrieving information from documents is the major concern when using knowledge-based approaches in software documentation.
Conclusions
The findings of this review suggest several future research directions that are critical and promising but underexplored in current research and practice: (1) there is a need to use knowledge-based approaches to improve the quality attributes of software documents that receive less attention, especially credibility, conciseness, and unambiguity; (2) using knowledge-based approaches with the knowledge content in software documents which gets less attention in current applications of knowledge-based approaches in software documentation, to further improve the practice of software documentation activity; (3) putting more focus on the application of software documents using the knowledge-based approaches (knowledge reuse, retrieval, reasoning, and sharing) in order to make the most use of software documents; and (4) evaluating the costs and benefits of using knowledge-based approaches in software documentation qualitatively and quantitatively.}
}
@article{MOTTA2019231,
title = {A conceptual perspective on interoperability in context-aware software systems},
journal = {Information and Software Technology},
volume = {114},
pages = {231-257},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919301442},
author = {Rebeca C. Motta and Káthia M. {de Oliveira} and Guilherme H. Travassos},
keywords = {Interoperability, Context-aware software systems, Internet of Things, Contemporary software systems, Ubiquitous computing, -systematic literature review, Evidence-based software engineering},
abstract = {Context
Context-aware software systems can interact with different devices to complete their tasks and act according to the context, regardless of their development and organizational differences. Interoperability is a big challenge in the engineering of such systems.
Objective
To discuss how interoperability has been addressed in context-aware software systems, strengthening the scientific basis for its understanding and conceptualization.
Method
A quasi-systematic literature review was undertaken to observe interoperability in such context-aware software systems to support the discussions. Its dataset includes 17 from 408 papers identified in the technical literature. The extracted information was qualitatively analyzed by following the principles of Grounded Theory.
Results
The analysis allowed to identify ten interoperability concepts, organized into a Theoretical Framework according to structural and behavioral perspectives, which deals with interoperability as the ability of things (an object, a place, an application or anything that can engage interaction with a system) to interact for a particular purpose, once their differences (development platforms, data formats, culture, legal issues) have been overcome. Once the interoperability is established from structural concepts (context, perspective, purpose, the level of provided support and system attributes), it can be measured, improved and observed from the behavioral concepts (evaluation method, challenges, issues, and benefits).
Conclusions
The Interoperability Theoretical Framework provides relevant information to organize the knowledge related to interoperability, considering context, and can be used to guide the evolution of software systems regarding changes focused on interoperability.}
}
@article{SIEVIKORTE2019234,
title = {Challenges and recommended practices for software architecting in global software development},
journal = {Information and Software Technology},
volume = {106},
pages = {234-253},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918302209},
author = {Outi Sievi-Korte and Sarah Beecham and Ita Richardson},
keywords = {Global software development, Software architecture, Software design, Design practice, Systematic literature review},
abstract = {Context
Global software development (GSD), although now a norm in the software industry, carries with it enormous challenges mostly regarding communication and coordination. Aforementioned challenges are highlighted when there is a need to transfer knowledge between sites, particularly when software artifacts assigned to different sites depend on each other. The design of the software architecture and associated task dependencies play a major role in reducing some of these challenges.
Objective
The current literature does not provide a cohesive picture of how the distributed nature of software development is taken into account during the design phase: what to avoid, and what works in practice. The objective of this paper is to gain an understanding of software architecting in the context of GSD, in order to develop a framework of challenges and solutions that can be applied in both research and practice.
Method
We conducted a systematic literature review (SLR) that synthesises (i) challenges which GSD imposes on software architecture design, and (ii) recommended practices to alleviate these challenges.
Results
We produced a comprehensive set of guidelines for performing software architecture design in GSD based on 55 selected studies. Our framework comprises nine key challenges with 28 related concerns, and nine recommended practices, with 22 related concerns for software architecture design in GSD. These challenges and practices were mapped to a thematic conceptual model with the following concepts: Organization (Structure and Resources), Ways of Working (Architecture Knowledge Management, Change Management and Quality Management), Design Practices, Modularity and Task Allocation.
Conclusion
The synthesis of findings resulted in a thematic conceptual model of the problem area, a mapping of the key challenges to practices, and a concern framework providing concrete questions to aid the design process in a distributed setting. This is a first step in creating more concrete architecture design practices and guidelines.}
}
@article{ACHIMUGU2014568,
title = {A systematic literature review of software requirements prioritization research},
journal = {Information and Software Technology},
volume = {56},
number = {6},
pages = {568-585},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000354},
author = {Philip Achimugu and Ali Selamat and Roliana Ibrahim and Mohd Naz’ri Mahrin},
keywords = {Stakeholders, Requirements, Prioritization, Software systems, Requirement engineering},
abstract = {Context
During requirements engineering, prioritization is performed to grade or rank requirements in their order of importance and subsequent implementation releases. It is a major step taken in making crucial decisions so as to increase the economic value of a system.
Objective
The purpose of this study is to identify and analyze existing prioritization techniques in the context of the formulated research questions.
Method
Search terms with relevant keywords were used to identify primary studies that relate requirements prioritization classified under journal articles, conference papers, workshops, symposiums, book chapters and IEEE bulletins.
Results
73 Primary studies were selected from the search processes. Out of these studies; 13 were journal articles, 35 were conference papers and 8 were workshop papers. Furthermore, contributions from symposiums as well as IEEE bulletins were 2 each while the total number of book chapters amounted to 13.
Conclusion
Prioritization has been significantly discussed in the requirements engineering domain. However, it was generally discovered that, existing prioritization techniques suffer from a number of limitations which includes: lack of scalability, methods of dealing with rank updates during requirements evolution, coordination among stakeholders and requirements dependency issues. Also, the applicability of existing techniques in complex and real setting has not been reported yet.}
}
@article{SEPULVEDA201616,
title = {Requirements modeling languages for software product lines: A systematic literature review},
journal = {Information and Software Technology},
volume = {69},
pages = {16-36},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001494},
author = {Samuel Sepúlveda and Ania Cravero and Cristina Cachero},
keywords = {Requirements engineering, Modeling languages, Software product lines, Systematic literature review},
abstract = {Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs. Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption. Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013. Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46% of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6% of the languages have been empirically validated, 41% report some kind of industry adoption and 71% of the languages are independent from any development process. Last but not least, 57% of the languages have been proposed by the academia, while 43% have been the result of a joint effort between academia and industry. Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve.}
}
@article{BUDGEN201862,
title = {Reporting systematic reviews: Some lessons from a tertiary study},
journal = {Information and Software Technology},
volume = {95},
pages = {62-74},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916303548},
author = {David Budgen and Pearl Brereton and Sarah Drummond and Nikki Williams},
keywords = {Systematic review, Reporting quality, Provenance of findings},
abstract = {Context
Many of the systematic reviews published in software engineering are related to research or methodological issues and hence are unlikely to be of direct benefit to practitioners or teachers. Those that are relevant to practice and teaching need to be presented in a form that makes their findings usable with minimum interpretation.
Objective
We have examined a sample of the many systematic reviews that have been published over a period of six years, in order to assess how well these are reported and identify useful lessons about how this might be done.
Method
We undertook a tertiary study, performing a systematic review of systematic reviews. Our study found 178 systematic reviews published in a set of major software engineering journals over the period 2010–2015. Of these, 37 provided recommendations or conclusions of relevance to education and/or practice and we used the DARE criteria as well as other attributes related to the systematic review process to analyse how well they were reported.
Results
We have derived a set of 12 ‘lessons’ that could help authors with reporting the outcomes of a systematic review in software engineering. We also provide an associated checklist for use by journal and conference referees.
Conclusion
There are several areas where better reporting is needed, including quality assessment, synthesis, and the procedures followed by the reviewers. Researchers, practitioners, teachers and journal referees would all benefit from better reporting of systematic reviews, both for clarity and also for establishing the provenance of any findings.}
}
@article{FEBRERO201618,
title = {Software reliability modeling based on ISO/IEC SQuaRE},
journal = {Information and Software Technology},
volume = {70},
pages = {18-29},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001652},
author = {Felipe Febrero and Coral Calero and M. {Ángeles Moraga}},
keywords = {Software quality, Software reliability modeling, International standard, SQuaRE},
abstract = {Context
The increasing dependence of our society on software driven systems has led Software Reliability to become a key factor as well as making it a highly active research area with hundreds of works being published every year. It would, however, appear that this activity is much more reduced as regards how to apply representative international standards on Product Quality to industrial environments, with just a few works on Standard Based software reliability modeling (SB-SRM). This is surprising given the relevance of such International Standards in industry.
Objective
To identify and analyze the existing works on the modeling of Software Reliability based on International Standards as the starting point for a reliability assessment proposal based on ISO/IEC-25000 “Software Product Quality Requirements and Evaluation” (SQuaRE) series.
Method
The work methodology is based on the guidelines provided in Evidence Based Software Engineering for Systematic Literature Reviews (SLR).
Results
A total of 1820 works were obtained as a result of the SLR search, more than 800 primary studies were selected after data filtering. After scrutiny, over thirty of those were thoroughly analyze, the results obtained show a very limited application of SB-SRM particularly to industrial environment.
Conclusion
Our analysis point to the complexity of the proposed models together with the difficulties involved in applying them to the management of engineering activities as a root cause to be considered for such limited application. The various stakeholder needs are also a point of paramount importance that should be better covered if the industrial applicability of the proposed models is to be increased.}
}
@article{AMPATZOGLOU2010888,
title = {Software engineering research for computer games: A systematic review},
journal = {Information and Software Technology},
volume = {52},
number = {9},
pages = {888-901},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910000820},
author = {Apostolos Ampatzoglou and Ioannis Stamelos},
keywords = {Software engineering, Computer games, Systematic review},
abstract = {Context
Currently, computer game development is one of the fastest growing industries in the worldwide economy. In addition to that, computer games are rapidly evolving in the sense that newer game versions arrive in a very short interval. Thus, software engineering techniques are needed for game development in order to achieve greater flexibility and maintainability, less cost and effort, better design, etc. In addition, games present several characteristics that differentiate their development from classical software development.
Objective
This study aims to assess the state of the art on research concerning software engineering for computer games and discuss possible important areas for future research.
Method
We employed a standard methodology for systematic literature reviews using four well known digital libraries.
Results
Software engineering for computer games is a research domain that has doubled its research activity during the last 5years. The dominant research topic has proven to be requirements engineering, while topics such as software verification and maintenance have been neglected up to now.
Conclusion
The results of the study suggest that software engineering for computer games is a field that embraces many techniques and methods from conventional software engineering and adapts them so as to fit the specific requirements of game development. In addition to that, the study proposes the employment of more elaborate empirical methods, i.e. controlled experiments and case studies, in game software engineering research, which, have not been extensively used up to now.}
}
@article{DOMINGUEZ20121045,
title = {A systematic review of code generation proposals from state machine specifications},
journal = {Information and Software Technology},
volume = {54},
number = {10},
pages = {1045-1066},
year = {2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912000924},
author = {Eladio Domı´nguez and Beatriz Pérez and Ángel L. Rubio and Marı´a A. Zapata},
keywords = {UML state machines, Finite state machines, Statecharts, Code generation, Systematic review},
abstract = {Context
Model Driven Development (MDD) encourages the use of models for developing complex software systems. Following a MDD approach, modelling languages are used to diagrammatically model the structure and behaviour of object-oriented software, among which state-based languages (including UML state machines, finite state machines and Harel statecharts) constitute the most widely used to specify the dynamic behaviour of a system. However, generating code from state machine models as part of the final system constitutes one of the most challenging tasks due to its dynamic nature and because many state machine concepts are not supported by the object-oriented programming languages. Therefore, it is not surprising that such code generation has received great attention over the years.
Objective
The overall objective of this paper is to plot the landscape of published proposals in the field of object oriented code generation from state machine specifications, restricting the search neither to a specific context nor to a particular programming language.
Method
We perform a systematic, accurate literature review of published studies focusing on the object oriented implementation of state machine specifications.
Results
The systematic review is based on a comprehensive set of 53 resources in all, which we have classified into two groups: pattern-based and not pattern-based. For each proposal, we have analysed both the state machine specification elements they support and the means the authors propose for their implementation. Additionally, the review investigates which proposals take into account desirable features to be considered in software development such as maintenance or reusability.
Conclusions
One of the conclusions drawn from the review is that most of the analysed works are based on a software design pattern. Another key finding is that many papers neither support several of the main components of the expressive richness of state machine specifications nor provide an implementation strategy that considers relevant qualitative aspects in software development.}
}
@article{GONZALEZMOYANO2022107028,
title = {Uses of business process modeling in agile software development projects},
journal = {Information and Software Technology},
volume = {152},
pages = {107028},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107028},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001483},
author = {Cielo {González Moyano} and Luise Pufahl and Ingo Weber and Jan Mendling},
keywords = {Process models, Agile methodologies, Multi-method, Literature review, Thematic synthesis, Focus group},
abstract = {Context:
Agile methodologies and frameworks are widely used in software development projects because of their support for continuous change and delivery. Agile software development advocates de-prioritizing aspects such as processes and documentation. In traditional software engineering methodologies, however, business process models have been extensively used to support these aspects. Up until now, it is unclear to what extent recommendations to focus on code imply that conceptual modeling should be discontinued.
Objective:
The objective of this study is to investigate this hypothesis. More specifically, we develop a theoretical argument of how business process models are and can be used to support agile software development projects.
Method:
To this end, we use a multi-method study design. First, we conduct a systematic literature review, in which we identify studies on the usage of business process models in agile software development. Second, we apply procedures from thematic synthesis to analyze the connection between these uses and the phases of the development cycle. Third, we use a focus group design with practitioners to systematically reflect upon how these uses can help regarding four categories of challenges in agile software development: management, team, technology, and process.
Results:
From 37 relevant studies, we distill 15 different uses. The results highlight the benefits of process modeling as an instrument to support agile software development projects from different angles and in all project phases. Process modeling appears to be particularly relevant for the first phases of the development cycle, and for management and process issues in agile projects.
Conclusion:
We conclude that business process models indeed provide benefits for agile software development projects. Our findings have practical implications and emphasize the need for future research on modeling and agile development.}
}
@article{MAHMOOD2017102,
title = {Key factors that influence task allocation in global software development},
journal = {Information and Software Technology},
volume = {91},
pages = {102-122},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304470},
author = {Sajjad Mahmood and Sajid Anwer and Mahmood Niazi and Mohammad Alshayeb and Ita Richardson},
keywords = {Empirical study, Systematic Literature Review, Global Software Development, Task Allocation},
abstract = {Context
Planning and managing task allocation in Global Software Development (GSD) projects is both critical and challenging. To date, a number of models that support task allocation have been proposed, including cost models and risk-based multi-criteria optimization models.
Objective
The objective of this paper is to identify the factors that influence task allocation in the GSD project management context.
Method
First, we implemented a formal Systematic Literature Review (SLR) approach and identified a set of factors that influence task allocation in GSD projects. Second, a questionnaire survey was developed based on the SLR, and we collected feedback from 62 industry practitioners.
Results
The findings of this combined SLR and questionnaire survey indicate that site technical expertise, time zone difference, resource cost, task dependency, task size and vendor reliability are the key criteria for the distribution of work units in a GSD project. The results of the t-test show that there is no significant difference between the findings of the SLR and questionnaire survey. However, the industry study data indicates that resource cost and task dependency are more important to a centralized GSD project structure while task size is a key factor in a decentralized GSD project structure.
Conclusion
GSD organizations should try to consider the identified task allocation factors when managing their global software development activities to better understand, plan and manage work distribution decisions.}
}
@article{KITCHENHAM2012804,
title = {Three empirical studies on the agreement of reviewers about the quality of software engineering experiments},
journal = {Information and Software Technology},
volume = {54},
number = {8},
pages = {804-819},
year = {2012},
note = {Special Issue: Voice of the Editorial Board},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911002321},
author = {Barbara Ann Kitchenham and Dag I.K. Sjøberg and Tore Dybå and Dietmar Pfahl and Pearl Brereton and David Budgen and Martin Höst and Per Runeson},
keywords = {Quality evaluation, Empirical studies, Human-intensive experiments, Experimentation, Software engineering},
abstract = {Context
During systematic literature reviews it is necessary to assess the quality of empirical papers. Current guidelines suggest that two researchers should independently apply a quality checklist and any disagreements must be resolved. However, there is little empirical evidence concerning the effectiveness of these guidelines.
Aims
This paper investigates the three techniques that can be used to improve the reliability (i.e. the consensus among reviewers) of quality assessments, specifically, the number of reviewers, the use of a set of evaluation criteria and consultation among reviewers. We undertook a series of studies to investigate these factors.
Method
Two studies involved four research papers and eight reviewers using a quality checklist with nine questions. The first study was based on individual assessments, the second study on joint assessments with a period of inter-rater discussion. A third more formal randomised block experiment involved 48 reviewers assessing two of the papers used previously in teams of one, two and three persons to assess the impact of discussion among teams of different size using the evaluations of the “teams” of one person as a control.
Results
For the first two studies, the inter-rater reliability was poor for individual assessments, but better for joint evaluations. However, the results of the third study contradicted the results of Study 2. Inter-rater reliability was poor for all groups but worse for teams of two or three than for individuals.
Conclusions
When performing quality assessments for systematic literature reviews, we recommend using three independent reviewers and adopting the median assessment. A quality checklist seems useful but it is difficult to ensure that the checklist is both appropriate and understood by reviewers. Furthermore, future experiments should ensure participants are given more time to understand the quality checklist and to evaluate the research papers.}
}
@article{MELEGATI2021106465,
title = {Understanding Hypotheses Engineering in Software Startups through a Gray Literature Review},
journal = {Information and Software Technology},
volume = {133},
pages = {106465},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106465},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920302111},
author = {Jorge Melegati and Eduardo Guerra and Xiaofeng Wang},
keywords = {hypotheses engineering, software startups, gray literature review},
abstract = {Context
The higher availability of software usage data and the influence of the Lean Startup led to the rise of experimentation in software engineering, a new approach for development based on experiments to understand the user needs. In the models proposed to guide this approach, the first step is generally to identify, prioritize, and specify the hypotheses that will be tested through experimentation. However, although practitioners have proposed several techniques to handle hypotheses, the scientific literature is still scarce.
Objective
The goal of this study is to understand what activities, as proposed in industry, are entailed to handle hypotheses, facilitating the comparison, creation, and evaluation of relevant techniques.
Methods
We performed a gray literature review (GLR) on the practices proposed by practitioners to handle hypotheses in the context of software startups. We analyzed the identified documents using thematic synthesis.
Results
The analysis revealed that techniques proposed for software startups in practice compress five different activities: elicitation, prioritization, specification, analysis, and management. It also showed that practitioners often classify hypotheses in types and which qualities they aim for these statements.
Conclusion
Our results represent the first description for hypotheses engineering grounded in practice data. This mapping of the state-of-practice indicates how research could go forward in investigating hypotheses for experimentation in the context of software startups. For practitioners, they represent a catalog of available practices to be used in this context.}
}
@article{MORENOMONTESDEOCA2015187,
title = {A systematic literature review of studies on business process modeling quality},
journal = {Information and Software Technology},
volume = {58},
pages = {187-205},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914001797},
author = {Isel {Moreno-Montes de Oca} and Monique Snoeck and Hajo A. Reijers and Abel Rodríguez-Morffi},
keywords = {Systematic literature review, Business process modeling, Modeling quality},
abstract = {Context
Business process modeling is an essential part of understanding and redesigning the activities that a typical enterprise uses to achieve its business goals. The quality of a business process model has a significant impact on the development of any enterprise and IT support for that process.
Objective
Since the insights on what constitutes modeling quality are constantly evolving, it is unclear whether research on business process modeling quality already covers all major aspects of modeling quality. Therefore, the objective of this research is to determine the state of the art on business process modeling quality: What aspects of process modeling quality have been addressed until now and which gaps remain to be covered?
Method
We performed a systematic literature review of peer reviewed articles as published between 2000 and August 2013 on business process modeling quality. To analyze the contributions of the papers we use the Formal Concept Analysis technique.
Results
We found 72 studies addressing quality aspects of business process models. These studies were classified into different dimensions: addressed model quality type, research goal, research method, and type of research result. Our findings suggest that there is no generally accepted framework of model quality types. Most research focuses on empirical and pragmatic quality aspects, specifically with respect to improving the understandability or readability of models. Among the various research methods, experimentation is the most popular one. The results from published research most often take the form of intangible knowledge.
Conclusion
We believe there is a lack of an encompassing and generally accepted definition of business process modeling quality. This evidences the need for the development of a broader quality framework capable of dealing with the different aspects of business process modeling quality. Different dimensions of business process quality and of the process of modeling still require further research.}
}

@article{PELTONEN2021106571,
title = {Motivations, benefits, and issues for adopting Micro-Frontends: A Multivocal Literature Review},
journal = {Information and Software Technology},
volume = {136},
pages = {106571},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106571},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000549},
author = {Severi Peltonen and Luca Mezzalira and Davide Taibi},
keywords = {Micro-Frontends, Microservices, Web front-end development, Software architectures, Multivocal Literature Review},
abstract = {Context:
Micro-Frontends are increasing in popularity, being adopted by several large companies, such as DAZN, Ikea, Starbucks and may others. Micro-Frontends enable splitting of monolithic frontends into independent and smaller micro applications. However, many companies are still hesitant to adopt Micro-Frontends, due to the lack of knowledge concerning their benefits. Additionally, provided online documentation is often times perplexed and contradictory.
Objective:
The goal of this work is to map the existing knowledge on Micro-Frontends, by understanding the motivations of companies when adopting such applications as well as possible benefits and issues.
Method:
For this purpose, we surveyed the academic and grey literature by means of the Multivocal Literature Review process, analysing 173 sources, of which 43 reported motivations, benefits and issues.
Results:
The results show that existing architectural options to build web applications are cumbersome if the application and development team grows, and if multiple teams need to develop the same frontend application. In such cases, companies adopted Micro-Frontends to increase team independence and to reduce the overall complexity of the frontend. The application of the Micro-Frontend, confirmed the expected benefits, and Micro-Frontends resulted to provide the same benefits as microservices on the back end side, combining the development team into a fully cross-functional development team that can scale processes when needed. However, Micro-Frontends also showed some issues, such as the increased payload size of the application, increased code duplication and coupling between teams, and monitoring complexity.
Conclusions:
Micro-Frontends allow companies to scale development according to business needs in the same way microservices do with the back end side. In addition, Micro-Frontends have a lot of overhead and require careful planning if an advantage is achieved by using Micro-Frontends. Further research is needed to carefully investigate this new hype, by helping practitioners to understand how to use Micro-Frontends as well as understand in which contexts they are the most beneficial.}
}
@article{CAPILLA2021106439,
title = {Software engineering and advanced applications conference 2019 – selected papers},
journal = {Information and Software Technology},
volume = {130},
pages = {106439},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106439},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301920},
author = {Rafael Capilla and Miroslaw Staron},
abstract = {Software Engineering and Advanced Applications (SEAA) is a long-standing international forum for researchers, practitioners, and students to present and discuss the latest innovations, trends, experiences, and concerns in the field of Software Engineering and Advanced Applications in information technology for software-intensive systems. In this special issue, we present a selection of papers which show the current trends in software engineering – improved systematic reviews, deep learning and cloud computing.}
}
@article{MARIANI201714,
title = {A systematic review on search-based refactoring},
journal = {Information and Software Technology},
volume = {83},
pages = {14-34},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916303779},
author = {Thainá Mariani and Silvia Regina Vergilio},
keywords = {Search-based software engineering, Refactoring, Evolutionary algorithms},
abstract = {Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest. Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities. Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation. Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler’s Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics. Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.}
}
@article{STAPLES2008605,
title = {Systematic review of organizational motivations for adopting CMM-based SPI},
journal = {Information and Software Technology},
volume = {50},
number = {7},
pages = {605-620},
year = {2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2007.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584907000778},
author = {Mark Staples and Mahmood Niazi},
keywords = {CMM, CMMI, Capability Maturity Model, Software Process Improvement},
abstract = {Background: Software Process Improvement (SPI) is intended to improve software engineering, but can only be effective if used. To improve SPI’s uptake, we should understand why organizations adopt SPI. CMM-based SPI approaches are widely known and studied. Objective: We investigated why organizations adopt CMM-based SPI approaches, and how these motivations relate to organizations’ size. Method: We performed a systematic review, examining reasons reported in more than forty primary studies. Results: Reasons usually related to product quality and project performance, and less commonly, to process. Organizations reported customer reasons infrequently and employee reasons very rarely. We could not show that reasons related to size. Conclusion: Despite its origins in helping to address customer-related issues for the USAF, CMM-based SPI has mostly been adopted to help organizations improve project performance and product quality issues. This reinforces a view that the goal of SPI is not to improve process per se, but instead to provide business benefits.}
}
@article{RICHARDSON20121175,
title = {A Process Framework for Global Software Engineering Teams},
journal = {Information and Software Technology},
volume = {54},
number = {11},
pages = {1175-1191},
year = {2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912000912},
author = {Ita Richardson and Valentine Casey and Fergal McCaffery and John Burton and Sarah Beecham},
keywords = {Global Software Engineering, Global Teams, Global Teaming Process Area, Project Management, Software Process},
abstract = {Context
Global Software Engineering (GSE) continues to experience substantial growth and is fundamentally different to collocated development. As a result, software managers have a pressing need for support in how to successfully manage teams in a global environment. Unfortunately, de facto process frameworks such as the Capability Maturity Model Integration (CMMI®) do not explicitly cater for the complex and changing needs of global software management.
Objective
To develop a Global Teaming (GT) process area to address specific problems relating to temporal, cultural, geographic and linguistic distance which will meet the complex and changing needs of global software management.
Method
We carried out three in-depth case studies of GSE within industry from 1999 to 2007. To supplement these studies we conducted three literature reviews. This allowed us to identify factors which are important to GSE. Based on a gap analysis between these GSE factors and the CMMI®, we developed the GT process area. Finally, the literature and our empirical data were used to identify threats to software projects if these processes are not implemented.
Results
Our new GT process area brings together practices drawn from the GSE literature and our previous empirical work, including many socio-technical factors important to global software development. The GT process area presented in this paper encompasses recommended practices that can be used independently or with existing models. We found that if managers are not proactive in implementing new GT practices they are putting their projects under threat of failure. We therefore include a list of threats that if ignored could have an adverse effect on an organization’s competitive advantage, employee satisfaction, timescales, and software quality.
Conclusion
The GT process area and associated threats presented in this paper provides both a guide and motivation for software managers to better understand how to manage technical talent across the globe.}
}
@article{CRUZES2011440,
title = {Research synthesis in software engineering: A tertiary study},
journal = {Information and Software Technology},
volume = {53},
number = {5},
pages = {440-455},
year = {2011},
note = {Special Section on Best Papers from XP2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S095058491100005X},
author = {Daniela S. Cruzes and Tore Dybå},
keywords = {Evidence-based software engineering, Empirical software engineering, Systematic review, Qualitative methods, Mixed-methods},
abstract = {Context
Comparing and contrasting evidence from multiple studies is necessary to build knowledge and reach conclusions about the empirical support for a phenomenon. Therefore, research synthesis is at the center of the scientific enterprise in the software engineering discipline.
Objective
The objective of this article is to contribute to a better understanding of the challenges in synthesizing software engineering research and their implications for the progress of research and practice.
Method
A tertiary study of journal articles and full proceedings papers from the inception of evidence-based software engineering was performed to assess the types and methods of research synthesis in systematic reviews in software engineering.
Results
As many as half of the 49 reviews included in the study did not contain any synthesis. Of the studies that did contain synthesis, two thirds performed a narrative or a thematic synthesis. Only a few studies adequately demonstrated a robust, academic approach to research synthesis.
Conclusion
We concluded that, despite the focus on systematic reviews, there is limited attention paid to research synthesis in software engineering. This trend needs to change and a repertoire of synthesis methods needs to be an integral part of systematic reviews to increase their significance and utility for research and practice.}
}
@article{NGUYEN2017116,
title = {Model-based security engineering for cyber-physical systems: A systematic mapping study},
journal = {Information and Software Technology},
volume = {83},
pages = {116-135},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916303214},
author = {Phu H. Nguyen and Shaukat Ali and Tao Yue},
keywords = {Cyber-physical systems, Security, Model-based engineering, Security engineering, Systematic mapping, Snowballing, Survey},
abstract = {Context
Cyber-physical systems (CPSs) have emerged to be the next generation of engineered systems driving the so-called fourth industrial revolution. CPSs are becoming more complex, open and more prone to security threats, which urges security to be engineered systematically into CPSs. Model-Based Security Engineering (MBSE) could be a key means to tackle this challenge via security by design, abstraction, and automation.
Objective
We aim at providing an initial assessment of the state of the art in MBSE for CPSs (MBSE4CPS). Specifically, this work focuses on finding out 1) the publication statistics of MBSE4CPS studies; 2) the characteristics of MBSE4CPS studies; and 3) the open issues of MBSE4CPS research.
Method
We conducted a systematic mapping study (SMS) following a rigorous protocol that was developed based on the state-of-the-art SMS and systematic review guidelines. From thousands of relevant publications, we systematically identified 48 primary MBSE4CPS studies for data extraction and synthesis to answer predefined research questions.
Results
SMS results show that for three recent years (2014–2016) the number of primary MBSE4CPS studies has increased significantly. Within the primary studies, the popularity of using Domain-Specific Languages (DSLs) is comparable with the use of the standardised UML modelling notation. Most primary studies do not explicitly address specific security concerns (e.g., confidentiality, integrity) but rather focus on security analyses in general on threats, attacks or vulnerabilities. Few primary studies propose to engineer security solutions for CPSs. Many focus on the early stages of development lifecycle such as security requirement engineering or analysis.
Conclusion
The SMS does not only provide the state of the art in MBSE4CPS, but also points out several open issues that would deserve more investigation, e.g., the lack of engineering security solutions for CPSs, limited tool support, too few industrial case studies, and the challenge of bridging DSLs in engineering secure CPSs.}
}
@article{LISBOA20101,
title = {A systematic review of domain analysis tools},
journal = {Information and Software Technology},
volume = {52},
number = {1},
pages = {1-13},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909000834},
author = {Liana Barachisio Lisboa and Vinicius Cardoso Garcia and Daniel Lucrédio and Eduardo Santana {de Almeida} and Silvio Romero {de Lemos Meira} and Renata Pontin {de Mattos Fortes}},
keywords = {Systematic review, Domain analysis, Tools},
abstract = {The domain analysis process is used to identify and document common and variable characteristics of systems in a specific domain. In order to achieve an effective result, it is necessary to collect, organize and analyze several sources of information about different applications in this domain. Consequently, this process involves distinct phases and activities and also needs to identify which artifacts, arising from these activities, have to be traceable and consistent. In this context, performing a domain analysis process without tool support increases the risks of failure, but the used tool should support the complete process and not just a part of it. This article presents a systematic review of domain analysis tools that aims at finding out how the available tools offer support to the process. As a result, the review identified that these tools are usually focused on supporting only one process and there are still gaps in the complete process support. Furthermore, the results can provide insights for new research in the domain engineering area for investigating and defining new tools, and the study also aids in the identification of companies’ needs for a domain analysis tool.}
}
@article{RAINER2019231,
title = {Heuristics for improving the rigour and relevance of grey literature searches for software engineering research},
journal = {Information and Software Technology},
volume = {106},
pages = {231-233},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918302192},
author = {Austen Rainer and Ashley Williams},
keywords = {Grey literature review, Search engines, Reasoning, Quality criteria},
abstract = {Background: Software engineering research has a growing interest in grey literature (GL). Aim: To improve the identification of relevant and rigorous GL. Method: We develop and demonstrate heuristics to find more relevant and rigorous GL. The heuristics generate stratified samples of search and post–search datasets using a formally structured set of search keywords. Conclusion: The heuristics require further evaluation. We are developing a tool to implement the heuristics.}
}
@article{FRANCOBEDOYA2017160,
title = {Open source software ecosystems: A Systematic mapping},
journal = {Information and Software Technology},
volume = {91},
pages = {160-185},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304512},
author = {Oscar Franco-Bedoya and David Ameller and Dolors Costal and Xavier Franch},
keywords = {Software ecosystem, Open source software, Systematic mapping, Literature review, OSS, SECO, OSSECO},
abstract = {Context: Open source software (OSS) and software ecosystems (SECOs) are two consolidated research areas in software engineering. OSS influences the way organizations develop, acquire, use and commercialize software. SECOs have emerged as a paradigm to understand dynamics and heterogeneity in collaborative software development. For this reason, SECOs appear as a valid instrument to analyze OSS systems. However, there are few studies that blend both topics together. Objective: The purpose of this study is to evaluate the current state of the art in OSS ecosystems (OSSECOs) research, specifically: (a) what the most relevant definitions related to OSSECOs are; (b) what the particularities of this type of SECO are; and (c) how the knowledge about OSSECO is represented. Method: We conducted a systematic mapping following recommended practices. We applied automatic and manual searches on different sources and used a rigorous method to elicit the keywords from the research questions and selection criteria to retrieve the final papers. As a result, 82 papers were selected and evaluated. Threats to validity were identified and mitigated whenever possible. Results: The analysis allowed us to answer the research questions. Most notably, we did the following: (a) identified 64 terms related to the OSSECO and arranged them into a taxonomy; (b) built a genealogical tree to understand the genesis of the OSSECO term from related definitions; (c) analyzed the available definitions of SECO in the context of OSS; and (d) classified the existing modelling and analysis techniques of OSSECOs. Conclusion: As a summary of the systematic mapping, we conclude that existing research on several topics related to OSSECOs is still scarce (e.g., modelling and analysis techniques, quality models, standard definitions, etc.). This situation calls for further investigation efforts on how organizations and OSS communities actually understand OSSECOs.}
}
@article{ASSYNE2022107020,
title = {The essential competencies of software professionals: A unified competence framework},
journal = {Information and Software Technology},
volume = {151},
pages = {107020},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107020},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001446},
author = {Nana Assyne and Hadi Ghanbari and Mirja Pulkkinen},
keywords = {Software engineering, Software development, Competence, Competencies, Kano model},
abstract = {Context
Developing high-quality software requires skilled software professionals equipped with a set of basic and essential software engineering competencies (SEC). These competencies and the satisfaction levels derived from them change over a project's lifecycle, or as software professionals move from one project to another.
Objective
Previous studies suggest a lack of means enabling SEC stakeholders to identify and assess competencies suitable for different projects. Additionally, previous research has mainly portrayed SEC to be static and overlooked their evolution over time and across projects. We investigate how we could effectively identify and match the competencies of software professionals necessary for different projects.
Method
We follow a mixed-method approach to iteratively develop and evaluate a framework for identifying and managing SEC. In so doing, we use the results of an extensive literature review, focus group discussions with experts from academia and industry, and data collected through interviews with 138 individuals with a supervisory role in the software industry.
Results
Drawing on the Kano model and Competency Framework for Software Engineers, we propose a Unified Competence Gate for Software Professionals (UComGSP), a framework for identifying and managing SEC. The UComGSP consists of 62 hard competencies, 63 soft competencies, and 25 essential SEC competencies. Additionally, we propose three stakeholders’ satisfaction levels for SEC assessment: basic, performance, and delighter. Furthermore, based on empirical observation, we report 27 competencies not mentioned in the reviewed literature; 11 of them are considered essential competencies.
Conclusion
Competence development involves different stakeholders, including software professionals, educators, and the software industry. The UComGSP framework enables SEC stakeholders to (i) identify SE competencies, (ii) identify the essential SEC, and (iii) assess the satisfaction levels that can be derived from different competencies. Future research is needed to evaluate the effectiveness of the proposed framework across software development projects.}
}
@article{DOGAN2022106737,
title = {Towards a taxonomy of code review smells},
journal = {Information and Software Technology},
volume = {142},
pages = {106737},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106737},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001877},
author = {Emre Doğan and Eray Tüzün},
keywords = {Modern code review, Bad practices, Conformance checking, Code review smell, Process smell, Process debt},
abstract = {Context:
Code review is a crucial step of the software development life cycle in order to detect possible problems in source code before merging the changeset to the codebase. Although there is no consensus on a formally defined life cycle of the code review process, many companies and open source software (OSS) communities converge on common rules and best practices. In spite of minor differences in different platforms, the primary purpose of all these rules and practices leads to a faster and more effective code review process. Non-conformance of developers to this process does not only reduce the advantages of the code review but can also introduce waste in later stages of the software development.
Objectives:
The aim of this study is to provide an empirical understanding of the bad practices followed in the code review process, that are code review (CR) smells.
Methods:
We first conduct a multivocal literature review in order to gather code review bad practices discussed in white and gray literature. Then, we conduct a targeted survey with 32 experienced software practitioners and perform follow-up interviews in order to get their expert opinion. Based on this process, a taxonomy of code review smells is introduced. To quantitatively demonstrate the existence of these smells, we analyze 226,292 code reviews collected from eight OSS projects.
Results:
We observe that a considerable number of code review smells exist in all projects with varying degrees of ratios. The empirical results illustrate that 72.2% of the code reviews among eight projects are affected by at least one code review smell.
Conclusion:
The empirical analysis shows that the OSS projects are substantially affected by the code review smells. The provided taxonomy could provide a foundation for best practices and tool support to detect and avoid code review smells in practice.}
}
@article{SALTAN2021106510,
title = {Bridging the state-of-the-art and the state-of-the-practice of SaaS pricing: A multivocal literature review},
journal = {Information and Software Technology},
volume = {133},
pages = {106510},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106510},
url = {https://www.sciencedirect.com/science/article/pii/S095058492100001X},
author = {Andrey Saltan and Kari Smolander},
keywords = {Software-as-a-Service, SaaS, Pricing, Software Economics, Software Product Management, Multi-vocal Literature Review},
abstract = {Context
Pricing is an essential element of software business strategy and tactics. Informed pricing decision-making requires the involvement of different stakeholders and comprehensive data analysis. Achieving both appears to be challenging, and pricing remains one of the most under-managed processes in the software business. Simultaneously, a coherent SaaS pricing body of knowledge and verified solutions to assist SaaS providers while designing and implementing pricing are missing.
Objective
There is a lack of integration among different research areas focused on SaaS pricing and, more importantly, between academia and industry. The primary aim of this paper is to clarify this misconception by classifying, thematically analyzing, and putting in correspondent academic state-of-the-art and industrial state-of-the-practice of SaaS pricing.
Method
A multivocal literature review (MLR) approach was used for the study, exploring both “white” literature as well as “grey” literature. The body of literature of 387 bibliography items was collected using a formal protocol. Of these, 57 were white literature items, and 330 were grey. A multistage content analysis process was implemented to classify the rich literature body across multiple dimensions with further mapping, synthesis, and reporting.
Results
A taxonomy of pricing-related concepts was created. It classifies SaaS pricing aspects, affecting factors, and challenges facing SaaS providers. The findings and interpretations are summarized to emphasize the major research themes and practical challenges of SaaS pricing practices’ transformation and provide further research guidelines in this area.
Conclusion
SaaS pricing is a maturing and prominent area of research that requires further investigation. The conducted MLR formed a clear picture of SaaS pricing research and practice and identified different SaaS pricing aspects and affecting factors. The study will enable both scholars and practitioners to assess the current state-of-the-art in research and practice.}
}
@article{MORSCHHEUSER2018219,
title = {How to design gamification? A method for engineering gamified software},
journal = {Information and Software Technology},
volume = {95},
pages = {219-237},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.10.015},
url = {https://www.sciencedirect.com/science/article/pii/S095058491730349X},
author = {Benedikt Morschheuser and Lobna Hassan and Karl Werder and Juho Hamari},
keywords = {Gamification, Software engineering, Design science research, Persuasive technology, Gameful design, Playfulness, Game design},
abstract = {Context
Since its inception around 2010, gamification has become one of the top technology and software trends. However, gamification has also been regarded as one of the most challenging areas of software engineering. Beyond traditional software design requirements, designing gamification requires the command of disciplines such as (motivational/behavioral) psychology, game design, and narratology, making the development of gamified software a challenge for traditional software developers. Gamification software inhabits a finely tuned niche of software engineering that seeks for both high functionality and engagement; beyond technical flawlessness, gamification has to motivate and affect users. Consequently, it has also been projected that most gamified software is doomed to fail.
Objective
This paper seeks to advance the understanding of designing gamification and to provide a comprehensive method for developing gamified software.
Method
We approach the research problem via a design science research approach; firstly, by synthesizing the current body of literature on gamification design methods and by interviewing 25 gamification experts, producing a comprehensive list of design principles for developing gamified software. Secondly, and more importantly, we develop a detailed method for engineering of gamified software based on the gathered knowledge and design principles. Finally, we conduct an evaluation of the artifacts via interviews of ten gamification experts and implementation of the engineering method in a gamification project.
Results
As results of the study, we present the method and key design principles for engineering gamified software. Based on the empirical and expert evaluation, the developed method was deemed as comprehensive, implementable, complete, and useful. We deliver a comprehensive overview of gamification guidelines and shed novel insights into the nature of gamification development and design discourse.
Conclusion
This paper takes first steps towards a comprehensive method for gamified software engineering.}
}
@article{QAMAR2022106972,
title = {Taxonomy of bug tracking process smells: Perceptions of practitioners and an empirical analysis},
journal = {Information and Software Technology},
volume = {150},
pages = {106972},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106972},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001094},
author = {Khushbakht Ali Qamar and Emre Sülün and Eray Tüzün},
keywords = {The bug tracking system, Process mining, Conformance checking, Anti-patterns, Bug tracking smells, Process smell},
abstract = {Context:
While there is no consensus on a formally specified bug tracking process, some certain rules and best practices for an optimal bug tracking process are accepted by many companies and open-source software (OSS) projects. Despite slight variations between different platforms, the primary aim of all these rules and practices is to perform a more efficient bug tracking process. Practitioners’ non-compliance with the best practices not only impedes the benefits of the bug tracking process but also negatively affects the other phases of software development life cycle.
Objective:
The goal of this study is to gain a better knowledge of the bad practices that occur during the bug tracking process (bug tracking process smells) and to perform quantitative analysis to show that these process smells exist in bug tracking systems. Moreover, we want to know the perception of software practitioners related to these process smells and also observe the impact of process smells on the bug tracking process.
Methods:
Based on the results of a multivocal literature review, we analyzed 60 sources in academic and gray literature and propose a taxonomy of 12 bad practices in the bug tracking process. To quantitatively analyze these process smells, we inspected bug reports collected from eight projects which use Jira, Bugzilla, and GitHub Issues. To get an idea about the perception of practitioners about the taxonomy of bug tracking process smells, we conducted a targeted survey with 30 software practitioners. Moreover, we statistically analyzed the impact of bug tracking process smells on the resolution time and reopening count of bugs.
Results:
We observed from our empirical results that a considerable amount of bug tracking process smells exist in all projects and some of the process smell categories have statistically significant impacts on quality and speed. Survey results shows that the majority of software practitioners agree with the proposed taxonomy of BT process smells.
Conclusion:
The statistical analysis reveals that bug tracking process smells have an impact on OSS projects. The proposed taxonomy may serve as a foundation for best practices and tool support for detecting and avoiding bug tracking process smells.}
}
@article{GOMEZ20141033,
title = {Understanding replication of experiments in software engineering: A classification},
journal = {Information and Software Technology},
volume = {56},
number = {8},
pages = {1033-1048},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000858},
author = {Omar S. Gómez and Natalia Juristo and Sira Vegas},
keywords = {Software engineering, Experimental software engineering, Experimentation, Replication},
abstract = {Context
Replication plays an important role in experimental disciplines. There are still many uncertainties about how to proceed with replications of SE experiments. Should replicators reuse the baseline experiment materials? How much liaison should there be among the original and replicating experimenters, if any? What elements of the experimental configuration can be changed for the experiment to be considered a replication rather than a new experiment?
Objective
To improve our understanding of SE experiment replication, in this work we propose a classification which is intend to provide experimenters with guidance about what types of replication they can perform.
Method
The research approach followed is structured according to the following activities: (1) a literature review of experiment replication in SE and in other disciplines, (2) identification of typical elements that compose an experimental configuration, (3) identification of different replications purposes and (4) development of a classification of experiment replications for SE.
Results
We propose a classification of replications which provides experimenters in SE with guidance about what changes can they make in a replication and, based on these, what verification purposes such a replication can serve. The proposed classification helped to accommodate opposing views within a broader framework, it is capable of accounting for less similar replications to more similar ones regarding the baseline experiment.
Conclusion
The aim of replication is to verify results, but different types of replication serve special verification purposes and afford different degrees of change. Each replication type helps to discover particular experimental conditions that might influence the results. The proposed classification can be used to identify changes in a replication and, based on these, understand the level of verification.}
}
@article{KUUTILA2020106257,
title = {Time pressure in software engineering: A systematic review},
journal = {Information and Software Technology},
volume = {121},
pages = {106257},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106257},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920300045},
author = {Miikka Kuutila and Mika Mäntylä and Umar Farooq and Maëlick Claes},
abstract = {Context
Large project overruns and overtime work have been reported in the software industry, resulting in additional expense for companies and personal issues for developers. Experiments and case studies have investigated the relationship between time pressure and software quality and productivity.
Objective
The present work aims to provide an overview of studies related to time pressure in software engineering; specifically, existing definitions, possible causes, and metrics relevant to time pressure were collected, and a mapping of the studies to software processes and approaches was performed. Moreover, we synthesize results of existing quantitative studies on the effects of time pressure on software development, and offer practical takeaways for practitioners and researchers, based on empirical evidence.
Method
Our search strategy examined 5414 sources, found through repository searches and snowballing. Applying inclusion and exclusion criteria resulted in the selection of 102 papers, which made relevant contributions related to time pressure in software engineering.
Results
The majority of high quality studies report increased productivity and decreased quality under time pressure. The most frequent categories of studies focus on quality assurance, cost estimation, and process simulation. It appears that time pressure is usually caused by errors in cost estimation. The effect of time pressure is most often identified during software quality assurance.
Conclusions
The majority of empirical studies report increased productivity under time pressure, while the most cost estimation and process simulation models assume that compressing the schedule increases the total needed hours. We also find evidence of the mediating effect of knowledge on the effects of time pressure, and that tight deadlines impact tasks with an algorithmic nature more severely. Future research should better contextualize quantitative studies to account for the existing conflicting results and to provide an understanding of situations when time pressure is either beneficial or harmful.}
}
@article{AMPATZOGLOU2019201,
title = {Identifying, categorizing and mitigating threats to validity in software engineering secondary studies},
journal = {Information and Software Technology},
volume = {106},
pages = {201-230},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918302106},
author = {Apostolos Ampatzoglou and Stamatia Bibi and Paris Avgeriou and Marijn Verbeek and Alexander Chatzigeorgiou},
keywords = {Empirical software engineering, Secondary studies, Threats to Validity, Literature Review},
abstract = {Context
Secondary studies are vulnerable to threats to validity. Although, mitigating these threats is crucial for the credibility of these studies, we currently lack a systematic approach to identify, categorize and mitigate threats to validity for secondary studies.
Objective
In this paper, we review the corpus of secondary studies, with the aim to identify: (a) the trend of reporting threats to validity, (b) the most common threats to validity and corresponding mitigation actions, and (c) possible categories in which threats to validity can be classified.
Method
To achieve this goal we employ the tertiary study research method that is used for synthesizing knowledge from existing secondary studies. In particular, we collected data from more than 100 studies, published until December 2016 in top quality software engineering venues (both journals and conference).
Results
Our results suggest that in recent years, secondary studies are more likely to report their threats to validity. However, the presentation of such threats is rather ad hoc, e.g., the same threat may be presented with a different name, or under a different category. To alleviate this problem, we propose a classification schema for reporting threats to validity and possible mitigation actions. Both the classification of threats and the associated mitigation actions have been validated by an empirical study, i.e., Delphi rounds with experts.
Conclusion
Based on the proposed schema, we provide a checklist, which authors of secondary studies can use for identifying and categorizing threats to validity and corresponding mitigation actions, while readers of secondary studies can use the checklist for assessing the validity of the reported results.}
}
@article{KAUPPINEN2004937,
title = {Implementing requirements engineering processes throughout organizations: success factors and challenges},
journal = {Information and Software Technology},
volume = {46},
number = {14},
pages = {937-953},
year = {2004},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2004.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584904000692},
author = {Marjo Kauppinen and Matti Vartiainen and Jyrki Kontio and Sari Kujala and Reijo Sulonen},
keywords = {Requirements engineering, Process improvement, Case study, Action research},
abstract = {This paper aims at identifying critical factors affecting organization-wide implementation of requirements engineering (RE) processes. The paper is based on a broad literature review and three longitudinal case studies that were carried out using an action research method. The results indicate that RE process implementation is a demanding undertaking, and its success greatly depends on such human factors as motivation, commitment and enthusiasm. Therefore, it is essential that the RE process is useful for its individual users. Furthermore, the results indicate that organizations can gain benefits from RE by defining a simple RE process, by focusing on a small set of RE practices, and by supporting the systematic usage of these practices.}
}
@article{AFZAL2009957,
title = {A systematic review of search-based testing for non-functional system properties},
journal = {Information and Software Technology},
volume = {51},
number = {6},
pages = {957-976},
year = {2009},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2008.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584908001833},
author = {Wasif Afzal and Richard Torkar and Robert Feldt},
keywords = {Systematic review, Non-functional system properties, Search-based software testing},
abstract = {Search-based software testing is the application of metaheuristic search techniques to generate software tests. The test adequacy criterion is transformed into a fitness function and a set of solutions in the search space are evaluated with respect to the fitness function using a metaheuristic search technique. The application of metaheuristic search techniques for testing is promising due to the fact that exhaustive testing is infeasible considering the size and complexity of software under test. Search-based software testing has been applied across the spectrum of test case design methods; this includes white-box (structural), black-box (functional) and grey-box (combination of structural and functional) testing. In addition, metaheuristic search techniques have also been applied to test non-functional properties. The overall objective of undertaking this systematic review is to examine existing work into non-functional search-based software testing (NFSBST). We are interested in types of non-functional testing targeted using metaheuristic search techniques, different fitness functions used in different types of search-based non-functional testing and challenges in the application of these techniques. The systematic review is based on a comprehensive set of 35 articles obtained after a multi-stage selection process and have been published in the time span 1996–2007. The results of the review show that metaheuristic search techniques have been applied for non-functional testing of execution time, quality of service, security, usability and safety. A variety of metaheuristic search techniques are found to be applicable for non-functional testing including simulated annealing, tabu search, genetic algorithms, ant colony methods, grammatical evolution, genetic programming (and its variants including linear genetic programming) and swarm intelligence methods. The review reports on different fitness functions used to guide the search for each of the categories of execution time, safety, usability, quality of service and security; along with a discussion of possible challenges in the application of metaheuristic search techniques.}
}
@article{FREIRE2018119,
title = {A Bayesian networks-based approach to assess and improve the teamwork quality of agile teams},
journal = {Information and Software Technology},
volume = {100},
pages = {119-132},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917300204},
author = {Arthur Freire and Mirko Perkusich and Renata Saraiva and Hyggo Almeida and Angelo Perkusich},
keywords = {Teamwork, Agile teams, Bayesian network, Assessment, Measurement, Improvement},
abstract = {CONTEXT: According to the agile principles and values, as well as recent research articles, teamwork factors are critical to achieve success in agile projects. However, teamwork does not automatically arise. There are some existing instruments with the purpose of assessing the teamwork quality based on Structural Equation Modeling (i.e., empirically derived) and Radar Plots, but they may not be useful in a concrete situation because these techniques are not advised for prediction and diagnosis purposes. OBJECTIVE: Analytically derive a Bayesian network model based on a literature review and a practitioner’s knowledge; and to assess its practical utility through a case study. METHOD: To build the model, we executed a top-down approach using data collected through a literature review and a domain practitioner. We assessed the model with a case study executed in three Scrum teams. RESULTS: Given the context of the case study, the model assists agile teams on assessing teamwork quality and identifying improvement opportunities, is easy to learn, and the cost-benefit for using it with the proposed procedure is positive. CONCLUSION: We concluded that we achieved promising results with the presented solution. However, it needs more evaluation and validation to generalize the obtained results.}
}
@article{RAHMANI201627,
title = {CIP-UQIM: A unified model for quality improvement in software SME's based on CMMI level 2 and 3},
journal = {Information and Software Technology},
volume = {71},
pages = {27-57},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001755},
author = {Hosein Rahmani and Ashkan Sami and Abdullah Khalili},
keywords = {Software Process Improvement, Multi-Model Harmonization, Small and Medium sized Enterprise, CMMI-DEV 1.3, ISO 9001:2008, PMBOK 5},
abstract = {Context
Software Process Improvement (SPI) is among the most effective ways to improve the quality of software products and services. Despite many research and industrial reports, SPI planning and implementation in Multi-Model Environments (MMEs) still face various issues and difficulties which increase the risk of failure especially for Small and Medium size Enterprises (SMEs). As a popular approach, harmonization techniques are used to meet the MME's specific issues through recognizing the common or similar areas among the implemented models and developing an integrated solution for quality improvement throughout the organization.
Objective
This paper has two main objectives: (1) recognizing the set of most popular models used in worldwide harmonization projects and implemented or requested by Iranian software SMEs. (2) Comparing and integrating these models in Process Activity level to develop a unified quality improvement model.
Method
A combination of literature review and questionnaire methods was used to identify the set of the most popular models. Then, based on harmonization techniques the common or similar areas among these models are recognized and then an initial version of the Unified Quality Improvement Model (UQIM) is developed, named CIP-UQIM. Next, this initial model was presented in a two-day workshop for SPI experts and it was refined and finalized by incorporating their feedbacks.
Results
Investigations showed that the set {CMMI-DEV, ISO 9001, and PMBOK} was the most popular. Thus in CIP-UQIM, for the first time, the last version of these models was unified at the activity level details. Finally, the experience of using CIP-UQIM in an Iranian SME has been reported which demonstrate its applicability and advantages.
Conclusion
Considering the high similarity among the three models and CIP-UQIM level of details, it can be concluded that CIP-UQIM can be beneficial to resolve or reduce SPI issues in MME's, especially in case of software SMEs.}
}
@article{AKBAR2022106894,
title = {Toward successful DevSecOps in software development organizations: A decision-making framework},
journal = {Information and Software Technology},
volume = {147},
pages = {106894},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106894},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000568},
author = {Muhammad Azeem Akbar and Kari Smolander and Sajjad Mahmood and Ahmed Alsanad},
keywords = {DevOps, DevSecOps, Challenges, Multivocal literature review, Fuzzy analytical hierarchy process},
abstract = {Context
Development and Operations (DevOps) is a methodology that aims to establish collaboration between programmers and operators to automate the continuous delivery of new software to reduce the development life cycle and produce quality software. Development, Security, and Operations (DevSecOps) is developing the DevOps concept, which integrates security methods into a DevOps process. DevSecOps is a software development process where security is built in to ensure application confidentiality, integrity, and availability.
Objective
This paper aims to identify and prioritize the challenges associated with implementing the DevSecOps process.
Method
We performed a multivocal literature review (MLR) and conducted a questionnaire-based survey to identify challenges associated with DevSecOps-based projects. Moreover, interpretive structure modeling (ISM) was applied to study the relationships among the core categories of the challenges. Finally, we used the fuzzy technique for order preference by similarity to an ideal solution (TOPSIS) to prioritize the identified challenges associated with DevSecOps projects.
Results
We identified 18 challenges for the DevSecOps process and mapped them to 10 core categories. The ISM results indicate that the “standards” category has the most decisive influence on the other nine core categories of the identified challenges. Moreover, the fuzzy TOPSIS indicates that “lack of secure coding standards,” “lack of automated testing tools for security in DevOps,” and “ignorance in static testing for security due to lack of knowledge” are the highest priority challenges for the DevSecOps paradigm.
Conclusion
Organizations using DevOps should consider the identified challenges in developing secure software.}
}
@article{ENGSTROM201014,
title = {A systematic review on regression test selection techniques},
journal = {Information and Software Technology},
volume = {52},
number = {1},
pages = {14-30},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909001219},
author = {Emelie Engström and Per Runeson and Mats Skoglund},
keywords = {Regression testing, Test selection, Systematic review, Empirical studies},
abstract = {Regression testing is verifying that previously functioning software remains after a change. With the goal of finding a basis for further research in a joint industry-academia research project, we conducted a systematic review of empirical evaluations of regression test selection techniques. We identified 27 papers reporting 36 empirical studies, 21 experiments and 15 case studies. In total 28 techniques for regression test selection are evaluated. We present a qualitative analysis of the findings, an overview of techniques for regression test selection and related empirical evidence. No technique was found clearly superior since the results depend on many varying factors. We identified a need for empirical studies where concepts are evaluated rather than small variations in technical implementations.}
}
@article{SPINOLA2012759,
title = {Towards a framework to characterize ubiquitous software projects},
journal = {Information and Software Technology},
volume = {54},
number = {7},
pages = {759-785},
year = {2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912000304},
author = {Rodrigo Oliveira Spínola and Guilherme Horta Travassos},
keywords = {Ubiquitous computing, Software projects characterization, Systematic review, Experimental software engineering},
abstract = {Context
Ubiquitous Computing (or UbiComp) represents a paradigm in which information processing is thoroughly integrated into everyday objects and activities. From a Software Engineering point of view this development scenario brings new challenges in tailoring or building software processes, impacting current software technologies. However, it has not yet been explicitly shown how to characterize a software project with the perception of ubiquitous computing.
Objective
This paper presents a conceptual framework to support the characterization of ubiquitous software projects according to their ubiquity adherence level. It also intends to apply such characterization approach to some projects, aiming at observing their adherence with ubiquitous computing principles.
Method
To follow a research strategy based on systematic reviews and surveys to acquire UbiComp knowledge and organize a conceptual framework regarding ubiquitous computing, which can be used to characterize UbiComp software projects. Besides, to demonstrate its application by characterizing some software projects.
Results
Ubiquitous computing encapsulates at least 11 different high abstraction level characteristics represented by 123 functional and 45 restrictive factors. Based on this a checklist was organized to allow the characterization of ubiquitous software projects, which has been applied on 26 ubiquitous software projects from four different application domains (ambient intelligence, pervasive healthcare, U-learning, and urban space). No project demonstrated to support more than 65% of the characteristics set. Service omnipresence was observed in all of these projects. However, some characteristics, although identified as necessary in the checklist, were not identified in any of them.
Conclusion
There are characteristics that identify a software project as ubiquitous. However, a ubiquitous software project does not necessarily have to implement all of them. The application domain can influence the appearing of UbiComp characteristics in software projects, promoting an increase of their adherence to UbiComp and, thus, for additional software technologies to deal with these ubiquitous requirements.}
}
@article{VIZCAINO20131200,
title = {Applying Q-methodology to analyse the success factors in GSD},
journal = {Information and Software Technology},
volume = {55},
number = {7},
pages = {1200-1211},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913000141},
author = {Aurora Vizcaíno and Félix García and José Carlos Villar and Mario Piattini and Javier Portillo},
keywords = {Global Software Development (GSD), Success factor, Q-methodology},
abstract = {Context
The context of this paper is Global Software Development (GSD) which is a current trend concerning the development of software in a distributed manner throughout different countries. This paradigm has several advantages, but unfortunately there are a number of challenges that hinder projects’ successful development.
Objective
The main goal of this paper is to discover which factors affect the success of GSD projects and how these are ranked by researchers and practitioners.
Method
This paper analyses the relevant success factors reported in literature. These were collected by conducting a literature review, as a result of which 39 GSD success factors were selected. Q-methodology was then followed to conduct a survey from which the opinions of 21 experts in GSD were collected.
Results
The data indicated that the best ranked GSD success factors are staff motivation, skilled human resources and the identification of roles and responsibilities. The lowest scores were, surprisingly, language barriers, time zone differences between sites, cultural differences and geographical distance which, to date, have frequently been considered by researchers as the most influential factors in GSD. This study additionally shows the results according to the different points of view of the respondents involved and the context of the projects.
Conclusion
This study indicates that there are different points of view as regards which issues are most important to success when setting up a GSD project. For instance, some experts prefer a knowledge focus, while others prefer a project management approach in which the most important issues are those related to management (risks, coordination) and so on. The results obtained have also shown that the challenges of GSD are changing, since the critical issues were initially related to the various types of distances (geographical, temporal, socio-cultural, language). However, there is now a greater concern for the team members’ features and skills.}
}
@article{ZHANG2013822,
title = {A survey of experienced user perceptions about software design patterns},
journal = {Information and Software Technology},
volume = {55},
number = {5},
pages = {822-835},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2012.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584912002297},
author = {Cheng Zhang and David Budgen},
keywords = {Software design patterns, Survey, Software design},
abstract = {Context
Although the concept of the software design pattern is well-established, there is relatively little empirical knowledge about the patterns that experienced users consider to be most valuable.
Aim
To identify which patterns from the set catalogued by the ‘Gang of Four’ are considered to be useful by experienced users, which ones are considered as not being useful, and why this is so.
Method
We undertook a web-based survey of experienced pattern users, seeking information about their experiences as software developers and maintainers. Our sampling frame consisted of the authors of all of the pattern papers that we had identified in a preceding systematic review of studies of patterns.
Results
We received 206 usable responses, corresponding to a response rate of 19% from the original sampling frame. Most respondents were involved with software development rather than maintenance.
Conclusion
While patterns can provide a means of sharing ‘knowledge schemas’ between designers, only three patterns were widely regarded as valuable. Around one quarter of the patterns gained very low approval or worse. These observations need to be considered when using patterns; teaching students about the pattern concept; and planning empirical studies about patterns.}
}
@article{DELGADO2014134,
title = {An integrated approach based on execution measures for the continuous improvement of business processes realized by services},
journal = {Information and Software Technology},
volume = {56},
number = {2},
pages = {134-162},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913001742},
author = {Andrea Delgado and Barbara Weber and Francisco Ruiz and Ignacio {Garcia-Rodríguez de Guzmán} and Mario Piattini},
keywords = {Business Process Management (BPM), Service Oriented Computing (SOC), Continuous Process Improvement (CPI), Business Process Intelligence (BPI), Business Process execution measurement, ProM framework},
abstract = {Context
Organizations are rapidly adopting Business Process Management (BPM) as they focus on their business processes (BPs), seeing them to be key elements in controlling and improving the way they perform their business. Business Process Intelligence (BPI) takes as its focus the collection and analysis of information from the execution of BPs for the support of decision making, based on the discovery of improvement opportunities. Realizing BPs by services introduces an intermediate service layer that enables us to separate the specification of BPs in terms of models from the technologies implementing them, thus improving their modifiability by decoupling the model from its implementation.
Objective
To provide an approach for the continuous improvement of BPs, based on their realization with services and execution measurement. It comprises an improvement process to integrate the improvements into the BPs and services, an execution measurement model defining and categorizing several measures for BPs and service execution, and tool support for both.
Method
We carried out a systematic literature review, to collect existing proposals related to our research work. Then, in close collaboration with business experts from the Hospital General de Ciudad Real (HGCR), Spain, and following design science principles, we developed the methods and artifacts described in this paper, which were validated by means of a case study.
Results
We defined an improvement process extending the BP lifecycle with measurement and improvement activities, integrating an execution measurement model comprising a set of execution measures. Moreover, we developed a plug-in for the ProM framework to visualize the measurement results as a proof-of-concept prototype. The case study with the HGCR has shown its feasibility.
Conclusions
Our improvement vision, based on BPs realized by services and on measurement of their execution, in conjunction with a systematic approach to integrate the detected improvements, provides useful guidance to organizations.}
}
@article{YAO2021106664,
title = {The impact of using biased performance metrics on software defect prediction research},
journal = {Information and Software Technology},
volume = {139},
pages = {106664},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106664},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001270},
author = {Jingxiu Yao and Martin Shepperd},
keywords = {Software engineering, Machine learning, Software defect prediction, Computational experiment, Classification metrics},
abstract = {Context:
Software engineering researchers have undertaken many experiments investigating the potential of software defect prediction algorithms. Unfortunately some widely used performance metrics are known to be problematic, most notably F1, but nevertheless F1 is widely used.
Objective:
To investigate the potential impact of using F1 on the validity of this large body of research.
Method:
We undertook a systematic review to locate relevant experiments and then extract all pairwise comparisons of defect prediction performance using F1 and the unbiased Matthews correlation coefficient (MCC).
Results:
We found a total of 38 primary studies. These contain 12,471 pairs of results. Of these comparisons, 21.95% changed direction when the MCC metric is used instead of the biased F1 metric. Unfortunately, we also found evidence suggesting that F1 remains widely used in software defect prediction research.
Conclusion:
We reiterate the concerns of statisticians that the F1 is a problematic metric outside of an information retrieval context, since we are concerned about both classes (defect-prone and not defect-prone units). This inappropriate usage has led to a substantial number (more than one fifth) of erroneous (in terms of direction) results. Therefore we urge researchers to (i) use an unbiased metric and (ii) publish detailed results including confusion matrices such that alternative analyses become possible.}
}
@article{20051,
title = {Systematic Reviews in Evidence-based Software Technology and Software Engineering},
journal = {Information and Software Technology},
volume = {47},
number = {1},
pages = {1},
year = {2005},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2004.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584904001636}
}
@article{HANNAY20091110,
title = {The effectiveness of pair programming: A meta-analysis},
journal = {Information and Software Technology},
volume = {51},
number = {7},
pages = {1110-1122},
year = {2009},
note = {Special Section: Software Engineering for Secure Systems},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909000123},
author = {Jo E. Hannay and Tore Dybå and Erik Arisholm and Dag I.K. Sjøberg},
keywords = {Pair programming, Evidence-based software engineering, Systematic review, Meta-analysis, Fixed effects, Random effects},
abstract = {Several experiments on the effects of pair versus solo programming have been reported in the literature. We present a meta-analysis of these studies. The analysis shows a small significant positive overall effect of pair programming on quality, a medium significant positive overall effect on duration, and a medium significant negative overall effect on effort. However, between-study variance is significant, and there are signs of publication bias among published studies on pair programming. A more detailed examination of the evidence suggests that pair programming is faster than solo programming when programming task complexity is low and yields code solutions of higher quality when task complexity is high. The higher quality for complex tasks comes at a price of considerably greater effort, while the reduced completion time for the simpler tasks comes at a price of noticeably lower quality. We conclude that greater attention should be given to moderating factors on the effects of pair programming.}
}
@article{JADHAV2009555,
title = {Evaluating and selecting software packages: A review},
journal = {Information and Software Technology},
volume = {51},
number = {3},
pages = {555-563},
year = {2009},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2008.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584908001262},
author = {Anil S. Jadhav and Rajendra M. Sonar},
keywords = {Software evaluation, Software selection, Evaluation criteria, Software selection tools},
abstract = {Evaluating and selecting software packages that meet an organization’s requirements is a difficult software engineering process. Selection of a wrong software package can turn out to be costly and adversely affect business processes. The aim of this paper is to provide a basis to improve the process of evaluation and selection of the software packages. This paper reports a systematic review of papers published in journals and conference proceedings. The review investigates methodologies for selecting software packages, software evaluation techniques, software evaluation criteria, and systems that support decision makers in evaluating software packages. The key findings of the review are: (1) analytic hierarchy process has been widely used for evaluation of the software packages, (2) there is lack of a common list of generic software evaluation criteria and its meaning, and (3) there is need to develop a framework comprising of software selection methodology, evaluation technique, evaluation criteria, and system to assist decision makers in software selection.}
}
@article{DYBA2008833,
title = {Empirical studies of agile software development: A systematic review},
journal = {Information and Software Technology},
volume = {50},
number = {9},
pages = {833-859},
year = {2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2008.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584908000256},
author = {Tore Dybå and Torgeir Dingsøyr},
keywords = {Empirical software engineering, Evidence-based software engineering, Systematic review, Research synthesis, Agile software development, XP, Extreme programming, Scrum},
abstract = {Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations.}
}
@article{PRIKLADNICKI2010779,
title = {Process models in the practice of distributed software development: A systematic review of the literature},
journal = {Information and Software Technology},
volume = {52},
number = {8},
pages = {779-791},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910000492},
author = {Rafael Prikladnicki and Jorge Luis Nicolas Audy},
keywords = {Distributed software development, Global software engineering, Offshoring, Process models, Process improvement},
abstract = {Context
Distributed Software Development (DSD) has recently become an active research area. Although considerable research effort has been made in this area, as yet, no agreement has been reached as to an appropriate process model for DSD.
Purpose
This paper is intended to identify and synthesize papers that describe process models for distributed software development in the context of overseas outsourcing, i.e. “offshoring”.
Method
We used a systematic review methodology to search seven digital libraries and one topic-specific conference.
Results
We found 27 primary studies describing stage-related DSD process models. Only five of such studies looked into outsourcing to a subsidiary company (i.e. “internal offshoring”). Nineteen primary studies addressed the need for DSD process models. Eight primary studies and three literature surveys described stage-based DSD process models, but only three of such models were empirically evaluated.
Conclusion
We need more research aimed at internal offshoring. Furthermore, proposed models need to be empirically validated.}
}
@article{SULAYMAN2014807,
title = {Towards a theoretical framework of SPI success factors for small and medium web companies},
journal = {Information and Software Technology},
volume = {56},
number = {7},
pages = {807-820},
year = {2014},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2014.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584914000482},
author = {Muhammad Sulayman and Emilia Mendes and Cathy Urquhart and Mehwish Riaz and Ewan Tempero},
keywords = {Software process improvement, Success factors, Comparative analysis, Theoretical integration, Theoretical framework, Small and medium web companies},
abstract = {Context
The context of this research is software process improvement (SPI) success factors for small and medium Web companies.
Objective
The primary objective of this paper is to propose a theoretical framework of SPI success factors for small and medium Web companies.
Method
The theoretical framework presented in this study aggregated the results of three previous research phases by applying principles of theoretical integration and comparative analysis. Those three previous phases were all empirical in nature, and comprise: a systematic review of SPI in small and medium Web companies [1], [2]; a replication study [3] and a grounded theory-based initial exploratory framework of factors in small and medium Web companies [4].
Results
The theoretical framework includes 18 categories of SPI success factors, 148 properties of these categories and 25 corresponding relationships, which bind these categories together. With the help of these relationships, the categories and properties of SPI success factors can be directly translated into a set of guidelines, which can then be used by the practitioners of small and medium Web companies to improve the current state of SPI in their companies and achieve overall company success.
Conclusion
The comprehensive theoretical framework of SPI success factors presented herein provides evidence regarding key factors for predicting SPI success for small and medium Web companies. The framework can be used as a baseline for a successful implementation of SPI initiatives in the mentioned domain.}
}
@article{MADEYSKI2018118,
title = {Introduction to the special section on Enhancing Credibility of Empirical Software Engineering},
journal = {Information and Software Technology},
volume = {99},
pages = {118-119},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918300557},
author = {Lech Madeyski and Barbara Kitchenham and Krzysztof Wnuk},
keywords = {empirical software engineering, research credibility, reproducible research, replication, systematic review}
}
@article{PETERSEN2011317,
title = {Measuring and predicting software productivity: A systematic map and review},
journal = {Information and Software Technology},
volume = {53},
number = {4},
pages = {317-343},
year = {2011},
note = {Special section: Software Engineering track of the 24th Annual Symposium on Applied Computing},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002156},
author = {Kai Petersen},
keywords = {Software productivity, Software development, Efficiency, Performance, Measurement, Prediction},
abstract = {Context
Software productivity measurement is essential in order to control and improve the performance of software development. For example, by identifying role models (e.g. projects, individuals, tasks) when comparing productivity data. The prediction is of relevance to determine whether corrective actions are needed, and to discover which alternative improvement action would yield the best results.
Objective
In this study we identify studies for software productivity prediction and measurement. Based on the identified studies we first create a classification scheme and map the studies into the scheme (systematic map). Thereafter, a detailed analysis and synthesis of the studies is conducted.
Method
As a research method for systematically identifying and aggregating the evidence of productivity measurement and prediction approaches systematic mapping and systematic review have been used.
Results
In total 38 studies have been identified, resulting in a classification scheme for empirical research on software productivity. The mapping allowed to identify the rigor of the evidence with respect to the different productivity approaches. In the detailed analysis the results were tabulated and synthesized to provide recommendations to practitioners.
Conclusion
Risks with simple ratio-based measurement approaches were shown. In response to the problems data envelopment analysis seems to be a strong approach to capture multivariate productivity measures, and allows to identify reference projects to which inefficient projects should be compared. Regarding simulation no general prediction model can be identified. Simulation and statistical process control are promising methods for software productivity prediction. Overall, further evidence is needed to make stronger claims and recommendations. In particular, the discussion of validity threats should become standard, and models need to be compared with each other.}
}
@article{SVAHNBERG2010237,
title = {A systematic review on strategic release planning models},
journal = {Information and Software Technology},
volume = {52},
number = {3},
pages = {237-248},
year = {2010},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909002067},
author = {Mikael Svahnberg and Tony Gorschek and Robert Feldt and Richard Torkar and Saad Bin Saleem and Muhammad Usman Shafique},
keywords = {Strategic release planning models, Systematic review, Road-mapping, Requirements selection factors},
abstract = {Context
Strategic release planning (sometimes referred to as road-mapping) is an important phase of the requirements engineering process performed at product level. It is concerned with selection and assignment of requirements in sequences of releases such that important technical and resource constraints are fulfilled.
Objectives
In this study we investigate which strategic release planning models have been proposed, their degree of empirical validation, their factors for requirements selection, and whether they are intended for a bespoke or market-driven requirements engineering context.
Methods
In this systematic review a number of article sources are used, including Compendex, Inspec, IEEE Xplore, ACM Digital Library, and Springer Link. Studies are selected after reading titles and abstracts to decide whether the articles are peer reviewed, and relevant to the subject.
Results
Twenty four strategic release planning models are found and mapped in relation to each other, and a taxonomy of requirements selection factors is constructed.
Conclusions
We conclude that many models are related to each other and use similar techniques to address the release planning problem. We also conclude that several requirement selection factors are covered in the different models, but that many methods fail to address factors such as stakeholder value or internal value. Moreover, we conclude that there is a need for further empirical validation of the models in full scale industry trials.}
}
@article{HOST2011616,
title = {A systematic review of research on open source software in commercial software product development},
journal = {Information and Software Technology},
volume = {53},
number = {6},
pages = {616-624},
year = {2011},
note = {Special Section: Best papers from the APSEC},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910002259},
author = {Martin Höst and Alma Oručević-Alagić},
keywords = {Open source software, Proprietary, Commercial, Component based software engineering, Business models},
abstract = {Context
The popularity of the open source software development in the last decade, has brought about an increased interest from the industry on how to use open source components, participate in the open source community, build business models around this type of software development, and learn more about open source development methodologies. There is a need to understand the results of research in this area.
Objective
Since there is a need to understand conducted research, the aim of this study is to summarize the findings of research that has ben carried out on usage of open source components and development methodologies by the industry, as well as companies’ participation in the open source community.
Method
Systematic review through searches in library databases and manual identification of articles from the open source conference. The search was first carried out in May 2009 and then once again in May 2010.
Results
In 2009, 237 articles were first found, from which 19 were selected based on content and quality, and in 2010, 76 new articles were found from which four were selected. Twenty three articles were identified in total.
Conclusions
The articles could be divided into four categories: open source as part of component based software engineering, business models with open source in commercial organization, company participation in open source development communities, and usage of open source processes within a company.}
}
@article{BENMENACHEM2008241,
title = {Towards management of software as assets: A literature review with additional sources},
journal = {Information and Software Technology},
volume = {50},
number = {4},
pages = {241-258},
year = {2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2007.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584907000833},
author = {Mordechai Ben-Menachem},
keywords = {Software Engineering, Software Asset Management, Project Management, Software Quality Management, Goals Management},
abstract = {How should and how can software be managed? What is the management concept or paradigm? Software professionals, if they think about management of software at all, think in terms of Configuration Management. This is not a method for over-all software management; it merely controls software items’ versions. This is much too fine a level of granularity. Management begins with accurate and timely information. Managers tend to view software as something (unfortunately) very necessary but troubling because, they have very little real information about it and control is still nebulous, at best. Accountants view software as an incomprehensible intangible, neither wholly an expense nor really an asset. They do not have, nor do they produce information concerning it. Their data concerning software barely touches on direct outlays and contains no element of effort. Part of this disorientation is the basic confusion between “business software” and “engineering software”. This “Gordian Knot” must be opened; it needs to be made much more clear. This article shows a direction how such clarity may be achieved.}
}
@article{NEIVA2016137,
title = {Towards pragmatic interoperability to support collaboration: A systematic review and mapping of the literature},
journal = {Information and Software Technology},
volume = {72},
pages = {137-150},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916000021},
author = {Frâncila Weidt Neiva and José Maria N. David and Regina Braga and Fernanda Campos},
keywords = {Pragmatic interoperability, Collaboration, Collaborative systems, Groupware, Interoperability},
abstract = {Context: Many researchers have argued that providing interoperability support only considering the format and meaning (i.e. syntax and semantic) of data exchange is not enough to achieve complete, effective and meaningful collaboration. Pragmatic interoperability has been highlighted as a key requirement to enhance collaboration. However, fulfilling this requirement is not a trivial task and there is a lack of works discussing solutions to achieve this level of interoperability. Objectives: The aim of this study is to present a systematic review and mapping of the literature in order to identify, analyse and classify the published solutions to achieve pragmatic interoperability. Method: To conduct a systematic review and mapping in accordance with the guidelines proposed in the evidence-based software engineering literature. Results: Our study identified 13 papers reporting pragmatic interoperability computational solutions. The first paper in our set of selected papers was published in 2004; the main strategies used to address pragmatic interoperability issues were service discovery, composition and/or selection and ontologies. The application domain of the identified solutions was mainly e-business. In addition, most of the identified solutions were software architectures. Conclusion: Mature proposals addressing pragmatic interoperability are still rare in the literature. Although many works have discussed the importance of pragmatic interoperability, it is necessary that researchers report solutions that implement and evaluate pragmatic interoperability in order to make progress in this area.}
}
@article{SCHELLER2015145,
title = {Automated measurement of API usability: The API Concepts Framework},
journal = {Information and Software Technology},
volume = {61},
pages = {145-162},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000178},
author = {Thomas Scheller and Eva Kühn},
keywords = {API usability, API design, Complexity measures, Metrics},
abstract = {Context
Usability is an important software quality attribute for APIs. Unfortunately, measuring it is not an easy task since many things like experienced evaluators, suitable test users, and a functional product are needed. This makes existing usability measurement methods difficult to use, especially for non-professionals.
Objective
To make API usability measurement easier, an automated and objective measurement method would be needed. This article proposes such a method. Since it would be impossible to find and integrate all possible factors that influence API usability in one step, the main goal is to prove the feasibility of the introduced approach, and to define an extensible framework so that additional factors can easily be defined and added later.
Method
A literature review is conducted to find potential factors influencing API usability. From these factors, a selected few are investigated more closely with usability studies. The statistically evaluated results from these studies are used to define specific elements of the introduced framework. Further, the influence of the user as a critical factor for the framework’s feasibility is evaluated.
Results
The API Concepts Framework is defined, with an extensible structure based on concepts that represent the user’s actions, measurable properties that define what influences the usability of these concepts, and learning effects that represent the influence of the user’s experience. A comparison of values calculated by the framework with user studies shows promising results.
Conclusion
It is concluded that the introduced approach is feasible and provides useful results for evaluating API usability. The extensible framework easily allows to add new concepts and measurable properties in the future.}
}
@article{SILVA201719,
title = {A systematic review on search based mutation testing},
journal = {Information and Software Technology},
volume = {81},
pages = {19-35},
year = {2017},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300167},
author = {Rodolfo Adamshuk Silva and Simone do Rocio {Senger de Souza} and Paulo Sérgio {Lopes de Souza}},
keywords = {Mutation testing, Search based software testing, Meta-heuristic},
abstract = {Context
Search Based Software Testing refers to the use of meta-heuristics for the optimization of a task in the context of software testing. Meta-heuristics can solve complex problems in which an optimum solution must be found among a large amount of possibilities. The use of meta-heuristics in testing activities is promising because of the high number of inputs that should be tested. Previous studies on search based software testing have focused on the application of meta-heuristics for the optimization of structural and functional criteria. Recently, some researchers have proposed the use of SBST for mutation testing and explored solutions for the cost of application of this testing criterion.
Objective
The objective is to identify how SBST has been explored in the context of mutation testing, how fitness functions are defined and the challenges and research opportunities in the application of meta-heuristic search techniques.
Method
A systematic review involving 263 papers published between 1996 and 2014 examined the studies on the use of meta-heuristic search techniques for the optimization of mutation testing.
Results
The results show meta-heuristic search techniques have been applied for the optimization of test data generation, mutant generation and selection of effective mutation operators. Five meta-heuristic techniques, namely Genetic Algorithm, Ant Colony, Bacteriological Algorithm, Hill Climbing and Simulated Annealing have been used in search based mutation testing. The review addressed different fitness functions used to guide the search.
Conclusion
Search based mutation testing is a field of interest, however, some issues remain unexplored. For instance, the use of meta-heuristics for the selection of effective mutation operators was identified in only one study. The results have pointed a range of possibilities for new studies to be developed, i.e., identification of equivalent mutants, experimental studies and application to different domains, such as concurrent programs.}
}
@article{RIAZ201514,
title = {How have we evaluated software pattern application? A systematic mapping study of research design practices},
journal = {Information and Software Technology},
volume = {65},
pages = {14-38},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000774},
author = {Maria Riaz and Travis Breaux and Laurie Williams},
keywords = {Software pattern, Mapping study, Systematic review, Empirical evaluation, Empirical design},
abstract = {Context
Software patterns encapsulate expert knowledge for constructing successful solutions to recurring problems. Although a large collection of software patterns is available in literature, empirical evidence on how well various patterns help in problem solving is limited and inconclusive. The context of these empirical findings is also not well understood, limiting applicability and generalizability of the findings.
Objective
To characterize the research design of empirical studies exploring software pattern application involving human participants.
Method
We conducted a systematic mapping study to identify and analyze 30 primary empirical studies on software pattern application, including 24 original studies and 6 replications. We characterize the research design in terms of the questions researchers have explored and the context of empirical research efforts. We also classify the studies in terms of measures used for evaluation, and threats to validity considered during study design and execution.
Results
Use of software patterns in maintenance is the most commonly investigated theme, explored in 16 studies. Object-oriented design patterns are evaluated in 14 studies while 4 studies evaluate architectural patterns. We identified 10 different constructs with 31 associated measures used to evaluate software patterns. Measures for ‘efficiency’ and ‘usability’ are commonly used to evaluate the problem solving process. While measures for ‘completeness’, ‘correctness’ and ‘quality’ are commonly used to evaluate the final artifact. Overall, ‘time to complete a task’ is the most frequently used measure, employed in 15 studies to measure ‘efficiency’. For qualitative measures, studies do not report approaches for minimizing biases 27% of the time. Nine studies do not discuss any threats to validity.
Conclusion
Subtle differences in study design and execution can limit comparison of findings. Establishing baselines for participants’ experience level, providing appropriate training, standardizing problem sets, and employing commonly used measures to evaluate performance can support replication and comparison of results across studies.}
}
@article{BREIVOLD201216,
title = {A systematic review of software architecture evolution research},
journal = {Information and Software Technology},
volume = {54},
number = {1},
pages = {16-40},
year = {2012},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2011.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584911001376},
author = {Hongyu Pei Breivold and Ivica Crnkovic and Magnus Larsson},
keywords = {Software evolvability, Systematic review, Software architecture, Architecture evolution, Architecture analysis, Evolvability analysis},
abstract = {Context
Software evolvability describes a software system’s ability to easily accommodate future changes. It is a fundamental characteristic for making strategic decisions, and increasing economic value of software. For long-lived systems, there is a need to address evolvability explicitly during the entire software lifecycle in order to prolong the productive lifetime of software systems. For this reason, many research studies have been proposed in this area both by researchers and industry practitioners. These studies comprise a spectrum of particular techniques and practices, covering various activities in software lifecycle. However, no systematic review has been conducted previously to provide an extensive overview of software architecture evolvability research.
Objective
In this work, we present such a systematic review of architecting for software evolvability. The objective of this review is to obtain an overview of the existing approaches in analyzing and improving software evolvability at architectural level, and investigate impacts on research and practice.
Method
The identification of the primary studies in this review was based on a pre-defined search strategy and a multi-step selection process.
Results
Based on research topics in these studies, we have identified five main categories of themes: (i) techniques supporting quality consideration during software architecture design, (ii) architectural quality evaluation, (iii) economic valuation, (iv) architectural knowledge management, and (v) modeling techniques. A comprehensive overview of these categories and related studies is presented.
Conclusion
The findings of this review also reveal suggestions for further research and practice, such as (i) it is necessary to establish a theoretical foundation for software evolution research due to the fact that the expertise in this area is still built on the basis of case studies instead of generalized knowledge; (ii) it is necessary to combine appropriate techniques to address the multifaceted perspectives of software evolvability due to the fact that each technique has its specific focus and context for which it is appropriate in the entire software lifecycle.}
}
@article{KAMPENES20071073,
title = {A systematic review of effect size in software engineering experiments},
journal = {Information and Software Technology},
volume = {49},
number = {11},
pages = {1073-1086},
year = {2007},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2007.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0950584907000195},
author = {Vigdis By Kampenes and Tore Dybå and Jo E. Hannay and Dag I.K. Sjøberg},
keywords = {Empirical software engineering, Controlled experiments, Effect size, Statistical significance, Practical importance},
abstract = {An effect size quantifies the effects of an experimental treatment. Conclusions drawn from hypothesis testing results might be erroneous if effect sizes are not judged in addition to statistical significance. This paper reports a systematic review of 92 controlled experiments published in 12 major software engineering journals and conference proceedings in the decade 1993–2002. The review investigates the practice of effect size reporting, summarizes standardized effect sizes detected in the experiments, discusses the results and gives advice for improvements. Standardized and/or unstandardized effect sizes were reported in 29% of the experiments. Interpretations of the effect sizes in terms of practical importance were not discussed beyond references to standard conventions. The standardized effect sizes computed from the reviewed experiments were equal to observations in psychology studies and slightly larger than standard conventions in behavioral science.}
}
@article{KUMARA2021106593,
title = {The do’s and don’ts of infrastructure code: A systematic gray literature review},
journal = {Information and Software Technology},
volume = {137},
pages = {106593},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106593},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000720},
author = {Indika Kumara and Martín Garriga and Angel Urbano Romeu and Dario {Di Nucci} and Fabio Palomba and Damian Andrew Tamburri and Willem-Jan {van den Heuvel}},
keywords = {Infrastructure-as-code, DevOps, Gray literature review},
abstract = {Context:
Infrastructure-as-code (IaC) is the DevOps tactic of managing and provisioning software infrastructures through machine-readable definition files, rather than manual hardware configuration or interactive configuration tools.
Objective:
From a maintenance and evolution perspective, the topic has picked the interest of practitioners and academics alike, given the relative scarcity of supporting patterns and practices in the academic literature. At the same time, a considerable amount of gray literature exists on IaC. Thus we aim to characterize IaC and compile a catalog of best and bad practices for widely used IaC languages, all using gray literature materials.
Method:
In this paper, we systematically analyze the industrial gray literature on IaC, such as blog posts, tutorials, white papers using qualitative analysis techniques.
Results:
We proposed a definition for IaC and distilled a broad catalog summarized in a taxonomy consisting of 10 and 4 primary categories for best practices and bad practices, respectively, both language-agnostic and language-specific ones, for three IaC languages, namely Ansible, Puppet, and Chef. The practices reflect implementation issues, design issues, and the violation of/adherence to the essential principles of IaC.
Conclusion:
Our findings reveal critical insights concerning the top languages as well as the best practices adopted by practitioners to address (some of) those challenges. We evidence that the field of development and maintenance IaC is in its infancy and deserves further attention.}
}
@article{BUDGEN2022106840,
title = {Short communication: Evolution of secondary studies in software engineering},
journal = {Information and Software Technology},
volume = {145},
pages = {106840},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106840},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000179},
author = {David Budgen and Pearl Brereton},
keywords = {Systematic review, Mapping study, Qualitative study, Experience of authors},
abstract = {Context:
Other disciplines commonly employ secondary studies to address the needs of practitioners and policy-makers. Since being adopted by software engineering in 2004, many have been undertaken by researchers.
Objective:
To assess how the role of secondary studies in software engineering has evolved.
Methods:
We examined a sample of 131 secondary studies published in a set of five major software engineering journals for the years 2010, 2015 and 2020. These were categorised by their type (e.g. mapping study), their research focus (quantitative/qualitative and practice/methodological), as well as the experience of the first authors.
Results:
Secondary studies are now a well-established research tool. They are predominantly qualitative and there is extensive use of mapping studies to profile research in particular areas. A significant number are clearly produced as part of postgraduate study, although experienced researchers also conduct many secondary studies. They are sometimes also used as part of a multi-method study.
Conclusion:
Existing guidelines largely focus upon quantitative systematic reviews. Based on our findings, we suggest that more guidance is needed on how to conduct, analyse, and report qualitative secondary studies.}
}
@article{PHILLIPS2018150,
title = {An architecture, system engineering, and acquisition approach for space system software resiliency},
journal = {Information and Software Technology},
volume = {94},
pages = {150-164},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917300575},
author = {Dewanne M. Phillips and Thomas A. Mazzuchi and Shahram Sarkani},
keywords = {Software, Architecture, Resiliency, Systems engineering, Life cycle, Vulnerabilities, Threats, Cybersecurity},
abstract = {Context
Software-intensive space systems can harbor defects and vulnerabilities that may enable external adversaries or malicious insiders to disrupt or disable system functions, risking mission compromise or loss. Mitigating this risk demands a sustained focus on the security and resiliency of the system architecture including software, hardware, and other components.
Objective
In this paper we offer methodical approaches for improving space system resiliency through software architecture design, system engineering, and increased software security, thereby reducing the risk of latent software defects and vulnerabilities.
Method
We conducted a systematic review of existing architectural practices, standards, security and coding practices, various threats, defects, and vulnerabilities that impact space systems from hundreds of relevant publications and interviews of subject matter experts. We expanded on the system-level body of knowledge for resiliency and identified a new software architecture framework and acquisition methodology to improve the resiliency of space systems from a software perspective with an emphasis on the early phases of the systems engineering life cycle. This methodology involves seven steps: 1) Define technical resiliency requirements, 1a) Identify standards/policy for software resiliency, 2) Develop a request for proposal (RFP)/statement of work (SOW) for resilient space systems software, 3) Define software resiliency goals for space systems, 4) Establish software resiliency quality attributes, 5) Perform architectural tradeoffs and identify risks, 6) Conduct architecture assessments as part of the procurement process, and 7) Ascertain space system software architecture resiliency metrics.
Results
Data illustrates that software vulnerabilities can lead to opportunities for malicious cyber activities, which could degrade the space mission capability for its user community. Reducing the number of vulnerabilities by improving architecture and software system engineering practices can contribute to making space systems more resilient.
Conclusion
Since cyber-attacks [1] are enabled by shortfalls in software, robust software engineering practices and an architectural design are foundational to resiliency, which is a quality that allows the system to take a hit to a critical component and recover in a known, bounded, and generally acceptable period of time. To achieve software resiliency for space systems, acquirers and suppliers must identify relevant factors and systems engineering practices to apply across the life cycle, in software requirements analysis, architecture development, design, implementation, verification and validation, and maintenance phases.}
}
@article{BARROSJUSTO20181,
title = {What software reuse benefits have been transferred to the industry? A systematic mapping study},
journal = {Information and Software Technology},
volume = {103},
pages = {1-21},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301083},
author = {José L. Barros-Justo and Fernando Pinciroli and Santiago Matalonga and Nelson Martínez-Araujo},
keywords = {Systematic mapping study, Systematic review, Software reuse, Software reuse processes, Software reuse benefits, Real-world setting, Industry, Evidence-based software engineering},
abstract = {Context
The term software reuse was first used in 1968 at the NATO conference. Since then, work in the scientific literature has stated that the application of software reuse offers benefits such as increase in quality and productivity. Nonetheless, in spite of many publications reporting software reuse experiences, evidence that such benefits having reached industrial settings is scarce.
Objective
To identify and classify the benefits transferred to real-world settings by the application of software reuse strategies.
Method
We conducted a systematic mapping study (SMS). Our search strategies retrieved a set of 2,413 papers out of which 49 were selected as primary studies. We defined five facets to classify these studies: (a) the type of benefit, (b) the reuse process, (c) the industry's domain, (d) the type of reuse and (e) the type of research reported.
Results
Quality increase (28 papers) and Productivity increase (25 papers) were the two most mentioned benefits. Component-Based Development (CBD) was the most reported reuse strategy (41%), followed by Software Product Lines (SPL, 30%). The selected papers mentioned fourteen industrial domains, of which four stand out: aerospace and defense, telecommunications, electronics and IT services. The application of systematic reuse was reported in 78% of the papers. Regarding the research type, 50% use evaluation research as the investigation method. Finally, 13 papers (27%) reported validity threats for the research method applied.
Conclusions
The literature analyzed presents a lack of empirical data, making it difficult to evaluate the effective transfer of benefits to the industry. This work did not find any relationship between the reported benefits and the reuse strategy applied by the industry or the industry domain. Although the most reported research method was industrial case studies (25 works), half of these works (12) did not report threats to validity.}
}
@article{SHERMAN2018148,
title = {Leveraging organizational climate theory for understanding industry-academia collaboration},
journal = {Information and Software Technology},
volume = {98},
pages = {148-160},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304020},
author = {Sofia Sherman and Irit Hadar and Gil Luria},
keywords = {Industry-academia collaboration, Empirical research, Software engineering, Stakeholder involvement, Organizational climate, Management commitment},
abstract = {Context
Industry-academia collaboration (IAC) in the field of software engineering is widely discussed in the literature, highlighting its importance and benefits. However, along with the benefits, academic researchers face challenges while performing empirical studies in industry, risking their success. Awareness of these challenges and the importance of addressing them has recently grown, and became the center of discussion in several publication venues.
Objective
In this paper, we aim to address one of the key challenges affecting the success of IAC: stakeholder involvement. To this end, we propose a vision for leveraging organizational climate theory toward an effective management of IAC in software engineering research. Organizational climate is defined as the organization's priorities as perceived by its employees and was found to be an effective means of predicting employee behavior.
Method
To provide a basis and motivation for our vision, we conducted a literature review, focused on the workshop series of CESI, Conducting Empirical Studies in Industry, in order to elicit the relevant reported challenges of IAC, and to analyze them through the lens of the organizational climate theory.
Results
Emergent categories of the elicited challenges of IAC are related to the two basic components that determine the emergence of organizational climate: management commitment and communication. This result demonstrates that analyzing stakeholder involvement-related challenges of IAC through the lens of organizational climate theory provides an indication of the climate components that should be enhanced in order to address these challenges.
Conclusion
The above analysis lays the foundation for our vision that organizational climate may serve as an effective means of addressing the discussed challenges. We propose that developing measures of organizational research collaboration climate and deploying respective interventions for improvement would be instrumental for enhancing stakeholder involvement in IAC. We further propose a research outline toward fulfilling these potential contributions.}
}
@article{COX2005891,
title = {A roadmap of problem frames research},
journal = {Information and Software Technology},
volume = {47},
number = {14},
pages = {891-902},
year = {2005},
note = {Special Issue on Problem Frames},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2005.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584905001217},
author = {Karl Cox and Jon G. Hall and Lucia Rapanotti},
keywords = {Problem frames, Literature review, Requirements engineering, Domain modelling},
abstract = {It has been a decade since Michael Jackson introduced problem frames to the software engineering community. Since then, he has published further work addressing problem frames as well as presenting several keynote addresses. Other authors have researched problem frames, have written about their experiences and have expressed their opinions. It was not until 2004 that an opportunity presented itself for researchers in the field to gather as a community. The first International Workshop on Advances and Applications of Problem Frames (IWAAPF'04) was held at the International Conference on Software Engineering in Edinburgh on 24th May 2004. This event attracted over 30 participants: Jackson delivered a keynote address, researchers presented their work and an expert panel discussed the challenges of problem frames. Featuring in this special issue are two extended papers from the workshop, an invited contribution from Jackson in which he positions problem frames in the context of the software engineering discipline, and this article, where we provide a review of the literature.}
}
@article{MOHAGHEGHI20091646,
title = {Definitions and approaches to model quality in model-based software development – A review of literature},
journal = {Information and Software Technology},
volume = {51},
number = {12},
pages = {1646-1669},
year = {2009},
note = {Quality of UML Models},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909000457},
author = {Parastoo Mohagheghi and Vegard Dehlen and Tor Neple},
keywords = {Systematic review, Modelling, Model quality, Model-driven development, UML},
abstract = {More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.}
}
@article{BJORNSON20081055,
title = {Knowledge management in software engineering: A systematic review of studied concepts, findings and research methods used},
journal = {Information and Software Technology},
volume = {50},
number = {11},
pages = {1055-1068},
year = {2008},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2008.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584908000487},
author = {Finn Olav Bjørnson and Torgeir Dingsøyr},
keywords = {Software engineering, Knowledge management, Learning software organization, Software process improvement, Systematic review},
abstract = {Software engineering is knowledge-intensive work, and how to manage software engineering knowledge has received much attention. This systematic review identifies empirical studies of knowledge management initiatives in software engineering, and discusses the concepts studied, the major findings, and the research methods used. Seven hundred and sixty-two articles were identified, of which 68 were studies in an industry context. Of these, 29 were empirical studies and 39 reports of lessons learned. More than half of the empirical studies were case studies. The majority of empirical studies relate to technocratic and behavioural aspects of knowledge management, while there are few studies relating to economic, spatial and cartographic approaches. A finding reported across multiple papers was the need to not focus exclusively on explicit knowledge, but also consider tacit knowledge. We also describe implications for research and for practice.}
}
@article{MEDEIROS2020106194,
title = {Requirements specification for developers in agile projects: Evaluation by two industrial case studies},
journal = {Information and Software Technology},
volume = {117},
pages = {106194},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106194},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302010},
author = {Juliana Medeiros and Alexandre Vasconcelos and Carla Silva and Miguel Goulão},
keywords = {Software requirements specification, Agile software development, Empirical software engineering},
abstract = {Context
An inadequate requirements specification activity acts as a catalyst to other problems, such as low team productivity and difficulty in maintaining software. Although Agile Software Development (ASD) has grown in recent years, research pointed out several limitations concerning its requirements engineering activities, such as Software Requirements Specification (SRS) provided in high level and targeted to the customer, lack of information required to perform design activities and low availability of the customer. To overcome these issues, the RSD (Requirements Specification for Developers) approach was proposed to create an SRS that provides information closer to development needs. In addition, existing literature reviews identify a demand for more empirical studies on the requirements specification activity in ASD.
Objective
Face to this, this work presents the evaluation of the RSD approach with respect to how it affects the teamwork and to identify its strengths and limitations.
Methods
This evaluation was performed by means of two industrial case studies conducted using a multiple-case design, focusing on software engineers as the analysis unit. Data were collected during 15 months from documents, observations, and interviews. They were triangulated, analyzed, and synthesized using techniques of grounded theory.
Results
The findings pointed out that the readability of SRS was compromised when several requirements are specified in the same RSD artifact. Evaluation also indicated the need of prioritization and categorization of the acceptance criteria, a tool for creating, searching and tracing the artifacts, and obtaining acceptance tests from acceptance criteria. On the other hand, the findings showed that the practices used to specify requirements using the RSD approach have the potential to produce a more objective SRS, tailored for the development team.
Conclusion
As a consequence, the structure of the RSD artifact was considered as a factor that improved the team performance in the two case studies.}
}
@article{DYBA2006745,
title = {A systematic review of statistical power in software engineering experiments},
journal = {Information and Software Technology},
volume = {48},
number = {8},
pages = {745-755},
year = {2006},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2005.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950584905001333},
author = {Tore Dybå and Vigdis By Kampenes and Dag I.K. Sjøberg},
keywords = {Empirical software engineering, Controlled experiment, Systematic review, Statistical power, Effect size},
abstract = {Statistical power is an inherent part of empirical studies that employ significance testing and is essential for the planning of studies, for the interpretation of study results, and for the validity of study conclusions. This paper reports a quantitative assessment of the statistical power of empirical software engineering research based on the 103 papers on controlled experiments (of a total of 5,453 papers) published in nine major software engineering journals and three conference proceedings in the decade 1993–2002. The results show that the statistical power of software engineering experiments falls substantially below accepted norms as well as the levels found in the related discipline of information systems research. Given this study's findings, additional attention must be directed to the adequacy of sample sizes and research designs to ensure acceptable levels of statistical power. Furthermore, the current reporting of significance tests should be enhanced by also reporting effect sizes and confidence intervals.}
}
@article{METH20131695,
title = {The state of the art in automated requirements elicitation},
journal = {Information and Software Technology},
volume = {55},
number = {10},
pages = {1695-1709},
year = {2013},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2013.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584913000827},
author = {Hendrik Meth and Manuel Brhel and Alexander Maedche},
keywords = {Requirements Engineering, Requirements Elicitation, Automation, Requirements Reuse, Systematic Review},
abstract = {Context
In large software development projects a huge number of unstructured text documents from various stakeholders becomes available and needs to be analyzed and transformed into structured requirements. This elicitation process is known to be time-consuming and error-prone when performed manually by a requirements engineer. Consequently, substantial research has been done to automate the process through a plethora of tools and technologies.
Objective
This paper aims to capture the current state of automated requirements elicitation and derive future research directions by identifying gaps in the existing body of knowledge and through relating existing works to each other. More specifically, we are investigating the following research question: What is the state of the art in research covering tool support for automated requirements elicitation from natural language documents?
Method
A systematic review of the literature in automated requirements elicitation is performed. Identified works are categorized using an analysis framework comprising tool categories, technological concepts and evaluation approaches. Furthermore, the identified papers are related to each other through citation analysis to trace the development of the research field.
Results
We identified, categorized and related 36 relevant publications. Summarizing the observations we made, we propose future research to (1) investigate alternative elicitation paradigms going beyond a pure automation approach (2) compare the effects of different types of knowledge on elicitation results (3) apply comparative evaluation methods and multi-dimensional evaluation measures and (4) strive for a closer integration of research activities across the sub-fields of automatic requirements elicitation.
Conclusion
Through the results of our paper, we intend to contribute to the Requirements Engineering body of knowledge by (1) conceptualizing an analysis framework for works in the area of automated requirements elicitation, going beyond former classifications (2) providing an extensive overview and categorization of existing works in this area (3) formulating concise directions for future research.}
}
@article{WAINER20091081,
title = {Empirical evaluation in Computer Science research published by ACM},
journal = {Information and Software Technology},
volume = {51},
number = {6},
pages = {1081-1085},
year = {2009},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2009.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584909000093},
author = {Jacques Wainer and Claudia G. {Novoa Barsottini} and Danilo Lacerda and Leandro Rodrigues {Magalhães de Marco}},
keywords = {Empirical evaluation, Research evaluation, Experimentation, Systematic review},
abstract = {This paper repeats part of the analysis performed in the 1995 paper “Experimental evaluation in Computer Science: a quantitative study” by Tichy and collaborators, for 147 papers randomly selected from the ACM, published in the year 2005. The papers published in 2005 are classified in the following way: 4% theory, 17% empirical, 4.7% hypothesis testing, 3.4% other, and 70% design and modeling (using the 1995 paper categories). Within the design and modeling class, 33% of the papers have no evaluation. The numbers of the 2005 sample are very similar to the original figures for the 1995 sample, which shows that Computer Science research has not increased significantly its empirical or experimental component.}
}
@article{SOUZA201926,
title = {Deriving architectural models from requirements specifications: A systematic mapping study},
journal = {Information and Software Technology},
volume = {109},
pages = {26-39},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919300035},
author = {Eric Souza and Ana Moreira and Miguel Goulão},
keywords = {Software architecture, Mapping study, Literature review},
abstract = {Context
Software architecture design creates and documents the high-level structure of a software system. Such structure, expressed in architectural models, comprises software elements, relations among them, and properties of these elements and relations. Existing software architecture methods offer ways to derive architectural models from requirements specifications. These models must balance different forces that should be analyzed during this derivation process, such as those imposed by different application domains and quality attributes. Such balance is difficult to achieve, requiring skilled and experienced architects.
Object
The purpose of this paper is to provide a comprehensive overview of the existing methods to derive architectural models from requirements specifications and offer a research roadmap to challenge the community to address the identified limitations and open issues that require further investigation.
Method
To achieve this goal, we performed a systematic mapping study following the good practices from the Evidence-Based Software Engineering field.
Results
This study resulted in 39 primary studies selected for analysis and data extraction, from the 2575 initially retrieved.
Conclusion
The major findings indicate that current architectural derivation methods rely heavily on the architects’ tacit knowledge (experience and intuition), do not offer sufficient support for inexperienced architects, and lack explicit evaluation mechanisms. These and other findings are synthesized in a research roadmap which results would benefit researchers and practitioners.}
}
@article{SHEN2011137,
title = {Assessing PSP effect in training disciplined software development: A Plan–Track–Review model},
journal = {Information and Software Technology},
volume = {53},
number = {2},
pages = {137-148},
year = {2011},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2010.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584910001710},
author = {Wen-Hsiang Shen and Nien-Lin Hsueh and Wei-Mann Lee},
keywords = {Personal software process (PSP), Software process improvement (SPI), Effect assessment, Plan–Track–Review},
abstract = {Context
In training disciplined software development, the PSP is said to result in such effect as increased estimation accuracy, better software quality, earlier defect detection, and improved productivity. But a systematic mechanism that can be easily adopted to assess and interpret PSP effect is scarce within the existing literature.
Objective
The purpose of this study is to explore the possibility of devising a feasible assessment model that ties up critical software engineering values with the pertinent PSP metrics.
Method
A systematic review of the literature was conducted to establish such an assessment model (we called a Plan–Track–Review model). Both mean and median approaches along with a set of simplified procedures were used to assess the commonly accepted PSP training effects. A set of statistical analyses further followed to increase understanding of the relationships among the PSP metrics and to help interpret the application results.
Results
Based on the results of this study, PSP training effect on the controllability, manageability, and reliability of a software engineer is quite positive and largely consistent with the literature. However, its effect on one’s predictability on project in general (and on project size in particular) is not implied as said in the literature. As for one’s overall project efficiency, our results show a moderate improvement. Our initial finding also suggests that a prior stage PSP effect could have an impact on later stage training outcomes.
Conclusion
It is concluded that this Plan–Track–Review model with the associated framework can be used to assess PSP effect regarding a disciplined software development. The generated summary report serves to provide useful feedback for both PSP instructors and students based on internal as well as external standards.}
}
@article{DEEPA2016160,
title = {Securing web applications from injection and logic vulnerabilities: Approaches and challenges},
journal = {Information and Software Technology},
volume = {74},
pages = {160-180},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300234},
author = {G. Deepa and P. Santhi Thilagam},
keywords = {SQL injection, Cross-site scripting, Business logic vulnerabilities, Application logic vulnerabilities, Web application security, Injection flaws},
abstract = {Context: Web applications are trusted by billions of users for performing day-to-day activities. Accessibility, availability and omnipresence of web applications have made them a prime target for attackers. A simple implementation flaw in the application could allow an attacker to steal sensitive information and perform adversary actions, and hence it is important to secure web applications from attacks. Defensive mechanisms for securing web applications from the flaws have received attention from both academia and industry. Objective: The objective of this literature review is to summarize the current state of the art for securing web applications from major flaws such as injection and logic flaws. Though different kinds of injection flaws exist, the scope is restricted to SQL Injection (SQLI) and Cross-site scripting (XSS), since they are rated as the top most threats by different security consortiums. Method: The relevant articles recently published are identified from well-known digital libraries, and a total of 86 primary studies are considered. A total of 17 articles related to SQLI, 35 related to XSS and 34 related to logic flaws are discussed. Results: The articles are categorized based on the phase of software development life cycle where the defense mechanism is put into place. Most of the articles focus on detecting the flaws and preventing the attacks against web applications. Conclusion: Even though various approaches are available for securing web applications from SQLI and XSS, they are still prevalent due to their impact and severity. Logic flaws are gaining attention of the researchers since they violate the business specifications of applications. There is no single solution to mitigate all the flaws. More research is needed in the area of fixing flaws in the source code of applications.}
}
@article{THEUNISSEN2022106733,
title = {A mapping study on documentation in Continuous Software Development},
journal = {Information and Software Technology},
volume = {142},
pages = {106733},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106733},
url = {https://www.sciencedirect.com/science/article/pii/S095058492100183X},
author = {Theo Theunissen and Uwe {van Heesch} and Paris Avgeriou},
keywords = {Systematic mapping studies, Systematic reviews, Continuous Software Development, Lean, Agile, DevOps, Documentation},
abstract = {Context:
With an increase in Agile, Lean, and DevOps software methodologies over the last years (collectively referred to as Continuous Software Development (CSD)), we have observed that documentation is often poor.
Objective:
This work aims at collecting studies on documentation challenges, documentation practices, and tools that can support documentation in CSD.
Method:
A systematic mapping study was conducted to identify and analyze research on documentation in CSD, covering publications between 2001 and 2019.
Results:
A total of 63 studies were selected. We found 40 studies related to documentation practices and challenges, and 23 studies related to tools used in CSD. The challenges include: informal documentation is hard to understand, documentation is considered as waste, productivity is measured by working software only, documentation is out-of-sync with the software and there is a short-term focus. The practices include: non-written and informal communication, the usage of development artifacts for documentation, and the use of architecture frameworks. We also made an inventory of numerous tools that can be used for documentation purposes in CSD. Overall, we recommend the usage of executable documentation, modern tools and technologies to retrieve information and transform it into documentation, and the practice of minimal documentation upfront combined with detailed design for knowledge transfer afterwards.
Conclusion:
It is of paramount importance to increase the quantity and quality of documentation in CSD. While this remains challenging, practitioners will benefit from applying the identified practices and tools in order to mitigate the stated challenges.}
}
@article{CHANG20161,
title = {A review on exception analysis},
journal = {Information and Software Technology},
volume = {77},
pages = {1-16},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2016.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916300830},
author = {Byeong-Mo Chang and Kwanghoon Choi},
keywords = {Exception analysis, Static analysis, Dynamic analysis, Exception flow, Testing, Debugging},
abstract = {Context: Exception handling has become popular in most major programming languages, including Ada, C++, Java, and ML. Since exception handling was introduced in programming languages, there have been various kinds of exception analyses, which analyze exceptional behavior of programs statically or dynamically. Exception analyses have also been applied to various software engineering tasks such as testing, slicing, verification and visualization. Objective: This paper aims at providing a comprehensive view of studies on exception analysis. We conducted a review on exception analysis to identify and classify the studies. Method: We referred to the literature review method, and selected a comprehensive set of 87 papers on exception analysis from 515 papers published in journals and conference proceedings. The categorization and classification were done according to the research questions regarding to when they analyze, what they analyze, how to analyze, and applications of exception analysis. Results: We first identify three categories of static exception analysis and two categories of dynamic exception analysis together with the main applications of the exception analyses. We also discuss the main concepts, research methods used and major contributions of the studies on exception analysis. Conclusion: We have provided the comprehensive review of exception analysis. To the best of our knowledge, this is the first comprehensive review on exception analysis. As a further work, it would be interesting to see how the existing exception analysis techniques reviewed in this paper can be applied to other programming languages with exception handling mechanism, such as C#, Scala, and Eiffel, which have been rarely explored.}
}
@article{NIE2015198,
title = {Combinatorial testing, random testing, and adaptive random testing for detecting interaction triggered failures},
journal = {Information and Software Technology},
volume = {62},
pages = {198-213},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000440},
author = {Changhai Nie and Huayao Wu and Xintao Niu and Fei-Ching Kuo and Hareton Leung and Charles J. Colbourn},
keywords = {Software testing, Random Testing (RT), Adaptive Random Testing (ART), Combinatorial Testing (CT), Interaction Triggered Failure (ITF), Minimal Failure-causing Schema (MFS)},
abstract = {Context
Software behavior depends on many factors, and some failures occur only when certain factors interact. This is known as an interaction triggered failure, and the corresponding selection of factor values can be modeled as a Minimal Failure-causing Schema (MFS). (An MFS involving m factors is an m-MFS.) Combinatorial Testing (CT) has been developed to exercise (“hit”) all MFS with few tests. Adaptive Random Resting (ART) endeavors to make tests as different as possible, ensuring that testing of MFS is not unnecessarily repeated. Random Testing (RT) chooses tests at random without regard to the MFS already treated. CT might be expected to improve on RT for finding interaction triggered faults, and yet some studies report no significant difference. CT can also be expected to be better than ART, and yet other studies report that ART can be much better than RT. In light of these, the relative merits of CT, ART, and RT for finding interaction triggered faults are unclear.
Objective
To investigate the relationships among CT, ART, and RT, we conduct the first complete and systematic comparison for the purpose of hitting MFS.
Method
A systematic review of six aspects of CT, RT and ART is conducted first. Then two kinds of experiments are used to compare them under four metrics.
Results
ART improves upon RT, but t-way CT is better than both. In hitting t′-MFS the advantage is typically in the range from 10% to 30% when t=t′, but becomes much smaller when t′<t, and there may be no advantage when t′>t. The latter case may explain the studies reporting no significant difference between RT and CT.
Conclusion
RT is easily implemented. However, depending on its implementation, ART can improve upon RT. CT does as well as ART whether or nott′=t, but provides a valuable improvement in the cases when t′=t.}
}
@article{AGH2022106864,
title = {A checklist for the evaluation of software process line approaches},
journal = {Information and Software Technology},
volume = {146},
pages = {106864},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.106864},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922000349},
author = {Halimeh Agh and Félix García and Mario Piattini},
keywords = {Software process line, Software development process, Software process variability, Evaluation checklist},
abstract = {Context
A Software Process Line (SPrL) can help organisations to construct bespoke software development processes for specific project situations by reusing core assets. However, as there are diverse approaches for SPrL Engineering (SPrLE), this necessitates proper assistance to organisations in selecting the SPrL approach best suited to their needs.
Objective
This paper aims to identify an evaluation checklist that can be used for evaluating SPrLs.
Method
The checklist was constructed in five stages: first, relevant aspects for managing process variability in the context of SPrLs were identified; based on these, research questions were then formed in the second stage. In the third stage, to answer the research questions, a literature review was conducted that focused on analysing 39 primary studies. In the fourth stage, the checklist was built by synthesising the literature results. In the fifth stage, the checklist was applied to two SPrL approaches as a proof of concept.
Results
The checklist includes seven main aspects, including the modelling language used, the type of the approach based on the number of artefacts produced, the language constructs provided for variability modelling, the process perspectives covered, the tool used for supporting the SPrL approach, the variability-specific features provided to support process variability throughout the SPrL lifecycle, and the empirical evaluation conducted to evaluate the approach.
Conclusion
The checklist can be used by organisations to compare SPrLs and then select the most suitable SPrL approach; furthermore, it can be used by researchers to propose novel SPrL approaches that consider important aspects for variability management throughout the SPrL lifecycle. Although we have provided an example of the use of the checklist to compare SPrLs, an empirical evaluation of the checklist is required to get feedback from the organisations regarding the strengths and weaknesses of the checklist.}
}
@article{SHEPPERD2018120,
title = {The role and value of replication in empirical software engineering results},
journal = {Information and Software Technology},
volume = {99},
pages = {120-132},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304305},
author = {Martin Shepperd and Nemitari Ajienka and Steve Counsell},
keywords = {Software engineering, Experiment, Reliability, Replication, Meta-analysis},
abstract = {Context
Concerns have been raised from many quarters regarding the reliability of empirical research findings and this includes software engineering. Replication has been proposed as an important means of increasing confidence.
Objective
We aim to better understand the value of replication studies, the level of confirmation between replication and original studies, what confirmation means in a statistical sense and what factors modify this relationship.
Method
We perform a systematic review to identify relevant replication experimental studies in the areas of (i) software project effort prediction and (ii) pair programming. Where sufficient details are provided we compute prediction intervals.
Results
Our review locates 28 unique articles that describe replications of 35 original studies that address 75 research questions. Of these 10 are external, 15 internal and 3 internal-same-article replications. The odds ratio of internal to external (conducted by independent researchers) replications of obtaining a ‘confirmatory’ result is 8.64. We also found incomplete reporting hampered our ability to extract estimates of effect sizes. Where we are able to compute replication prediction intervals these were surprisingly large.
Conclusion
We show that there is substantial evidence to suggest that current approaches to empirical replications are highly problematic. There is a consensus that replications are important, but there is a need for better reporting of both original and replicated studies. Given the low power and incomplete reporting of many original studies, it can be unclear the extent to which a replication is confirmatory and to what extent it yields additional knowledge to the software engineering community. We recommend attention is switched from replication research to meta-analysis.}
}
@article{KOSAR201677,
title = {Domain-Specific Languages: A Systematic Mapping Study},
journal = {Information and Software Technology},
volume = {71},
pages = {77-91},
year = {2016},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915001858},
author = {Tomaž Kosar and Sudev Bohra and Marjan Mernik},
keywords = {Domain-Specific Languages, Systematic Mapping Study, Systematic Review},
abstract = {Context: In this study we report on a Systematic Mapping Study (SMS) for Domain-Specific Languages (DSLs), based on an automatic search including primary studies from journals, conferences, and workshops during the period from 2006 until 2012. Objective: The main objective of the described work was to perform an SMS on DSLs to better understand the DSL research field, identify research trends, and any possible open issues. The set of research questions was inspired by a DSL survey paper published in 2005. Method: We conducted a SMS over 5 stages: defining research questions, conducting the search, screening, classifying, and data extraction. Our SMS included 1153 candidate primary studies from the ISI Web of Science and ACM Digital Library, 390 primary studies were classified after screening. Results: This SMS discusses two main research questions: research space and trends/demographics of the literature within the field of DSLs. Both research questions are further subdivided into several research sub-questions. The results from the first research question clearly show that the DSL community focuses more on the development of new techniques/methods rather than investigating the integrations of DSLs with other software engineering processes or measuring the effectiveness of DSL approaches. Furthermore, there is a clear lack of evaluation research. Amongst different DSL development phases more attention is needed in regard to domain analysis, validation, and maintenance. The second research question revealed that the number of publications remains stable, and has not increased over the years. Top cited papers and venues are mentioned, as well as identifying the more active institutions carrying DSL research. Conclusion: The statistical findings regarding research questions paint an interesting picture about the mainstreams of the DSL community, as well as open issues where researchers can improve their research in their future work.}
}
@article{PAROLIA201530,
title = {Conflict resolution effectiveness on the implementation efficiency and achievement of business objectives in IT programs: A study of IT vendors},
journal = {Information and Software Technology},
volume = {66},
pages = {30-39},
year = {2015},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2015.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0950584915000889},
author = {Neeraj Parolia and Jengchung Victor Chen and James J. Jiang and Gary Klein},
keywords = {Conflict resolution, Program management, Constructive controversy, Trust, Interpersonal cooperation},
abstract = {Context
The information technology (IT) field presents a unique context for the management of multiple projects because of the variety of stakeholders involved, the complexity of interdependencies among projects, and the frequent use of external vendors. In practice, IT vendors typically employ advanced project governance techniques such as program management to work effectively with the numbers and variety of clients while still pursuing the benefits of a single oversight. These structural features lend themselves to conflict across teams with individual requirements. However, little research exists on program management, much less in the IT context, that represents conflict across IT project teams.
Objective
In this study, the effectiveness of conflict resolution on the implementation efficiency and fulfillment of business objectives is studied through the lens of constructive controversy theories. A number of hypotheses are derived by tailoring the constructive conflict resolution concepts to IT context and making a comprehensive literature review to identify the mediator and dependent variables. A model is developed to consider the management of conflict across multiple projects combined into a single program.
Method
A quantitative questionnaire related to the program environment was developed for five variables to include conflict resolution, cognition-based trust, interpersonal cooperation, business objectives and implementation efficiency. The hypotheses were tested by performing a survey study, where a number of well-established measures in the literature were used. 92 paired responses from program teams in 38 organizations located in India were obtained and represent a variety of individual characteristics, and program sizes.
Results
This study identified the composite role of constructive conflict resolution and cognition-based trust in improving interpersonal cooperation. The impacts of constructive conflict resolution on business objectives were not fully mediated by cognition-based trust and interpersonal cooperation, although implementation efficiency is fully mediated.
Conclusion
The management of conflict promotes trust and interpersonal cooperation necessary to improve the efficient completion of the program and benefits to the organization.}
}
@article{JAYATILLEKE2018163,
title = {A systematic review of requirements change management},
journal = {Information and Software Technology},
volume = {93},
pages = {163-185},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0950584917304664},
author = {Shalinka Jayatilleke and Richard Lai},
keywords = {Requirements change management, Agile, Systematic review},
abstract = {Context
Software requirements are often not set in concrete at the start of a software development project; and requirements changes become necessary and sometimes inevitable due to changes in customer requirements and changes in business rules and operating environments; hence, requirements development, which includes requirements changes, is a part of a software process. Previous work has shown that failing to manage software requirements changes well is a main contributor to project failure. Given the importance of the subject, there's a plethora of research work that discuss the management of requirements change in various directions, ways and means. An examination of these works suggests that there's a room for improvement.
Objective
In this paper, we present a systematic review of research in Requirements Change Management (RCM) as reported in the literature.
Method
We use a systematic review method to answer four key research questions related to requirements change management. The questions are: (1) What are the causes of requirements changes? (2) What processes are used for requirements change management? (3) What techniques are used for requirements change management? and (4) How do organizations make decisions regarding requirements changes? These questions are aimed at studying the various directions in the field of requirements change management and at providing suggestions for future research work.
Results
The four questions were answered; and the strengths and weaknesses of existing techniques for RCM were identified.
Conclusions
This paper has provided information about the current state-of-the-art techniques and practices for RCM and the research gaps in existing work. Benefits, risks and difficulties associated with RCM are also made available to software practitioners who will be in a position of making better decisions on activities related to RCM. Better decisions will lead to better planning which will increase the chance of project success.}
}
@article{CRUZ201846,
title = {ARSENAL-GSD: A framework for trust estimation in virtual teams based on sentiment analysis},
journal = {Information and Software Technology},
volume = {95},
pages = {46-61},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2017.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0950584916304517},
author = {Guilherme Augusto Maldonado da Cruz and Elisa Hatsue Moriya-Huzita and Valéria Delisandra Feltrim},
keywords = {Trust, Versioning system, Sentiment analysis, Virtual teams, Global software development},
abstract = {Context
Technology advances has enabled the emergence of virtual teams. In these teams, people are in different places and possibly over different time zones, making use of computer mediated communication to interact. At the same time distribution brings benefits, it poses challenges as the difficulty to develop trust, which is essential for team efficiency.
Objective
In this paper, we present ARSENAL-GSD, an automatic framework for detecting trust among members of global software development teams based on sentiment analysis.
Methods
To design ARSENAL-GSD we made a literature review to identify trust evidences, especially those that could be captured or inferred from the automatic analysis of data generated by members’ interactions in a versioning system. We applied a survey to validate the framework and evidences found.
Results
On a scale of 0–9, evidences were evaluated as having importance greater or equal to 5.23, and the extraction techniques used to estimate them were considered as good enough. Regarding differences between subjects profile, no difference was found in responses of participants with theoretical knowledge/none and those with medium/high knowledge in GSD, except for the evidence mimicry, which was considered more important for the group of participants with medium/high knowledge in GSD.
Conclusion
We concluded that our framework is valid and trust information provided by it could be used to allocate members to a new team and/or, to monitor them during project development.}
}
@article{GAROUSI201840,
title = {Multi-objective regression test selection in practice: An empirical study in the defense software industry},
journal = {Information and Software Technology},
volume = {103},
pages = {40-54},
year = {2018},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301186},
author = {Vahid Garousi and Ramazan Özkan and Aysu Betin-Can},
keywords = {Regression testing, Multi-objective optimization, Genetic algorithms, Empirical study, Defence software industry, Action-research},
abstract = {Context
Executing an entire regression test-suite after every code change is often costly in large software projects. To cope with this challenge, researchers have proposed various regression test-selection techniques.
Objective
This paper was motivated by a real industrial need to improve regression-testing practices in the context of a safety-critical industrial software in the defence domain in Turkey. To address our objective, we set up and conducted an “action-research” collaborative project between industry and academia.
Method
After a careful literature review, we selected a conceptual multi-objective regression-test selection framework (called MORTO) and adopted it to our industrial context by developing a custom-built genetic algorithm (GA) based on that conceptual framework. GA is able to provide full coverage of the affected (changed) requirements while considering multiple cost and benefit factors of regression testing. e.g., minimizing the number of test cases, and maximizing cumulative number of detected faults by each test suite.
Results
The empirical results of applying the approach on the Software Under Test (SUT) demonstrate that this approach yields a more efficient test suite (in terms of costs and benefits) compared to the old (manual) test-selection approach, used in the company, and another applicable approach chosen from the literature. With this new approach, regression selection process in the project under study is not ad-hoc anymore. Furthermore, we have been able to eliminate the subjectivity of regression testing and its dependency on expert opinions.
Conclusion
Since the proposed approach has been beneficial in saving the costs of regression testing, it is currently in active use in the company. We believe that other practitioners can apply our approach in their regression-testing contexts too, when applicable. Furthermore, this paper contributes to the body of evidence in regression testing by offering a success story of successful implementation and application of multi-objective regression testing in practice.}
}
@article{KAMPENES200971,
title = {A systematic review of quasi-experiments in software engineering},
journal = {Information and Software Technology},
volume = {51},
number = {1},
pages = {71-82},
year = {2009},
note = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2008.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0950584908000670},
author = {Vigdis By Kampenes and Tore Dybå and Jo E. Hannay and Dag I. {K. Sjøberg}},
keywords = {Quasi-experiments, Randomization, Field experiments, Empirical software engineering, Selection bias, Effect size},
abstract = {Background:
Experiments in which study units are assigned to experimental groups nonrandomly are called quasi-experiments. They allow investigations of cause–effect relations in settings in which randomization is inappropriate, impractical, or too costly.
Problem outline:
The procedure by which the nonrandom assignments are made might result in selection bias and other related internal validity problems. Selection bias is a systematic (not happening by chance) pre-experimental difference between the groups that could influence the results. By detecting the cause of the selection bias, and designing and analyzing the experiments accordingly, the effect of the bias may be reduced or eliminated.
Research method:
To investigate how quasi-experiments are performed in software engineering (SE), we conducted a systematic review of the experiments published in nine major SE journals and three conference proceedings in the decade 1993–2002.
Results:
Among the 113 experiments detected, 35% were quasi-experiments. In addition to field experiments, we found several applications for quasi-experiments in SE. However, there seems to be little awareness of the precise nature of quasi-experiments and the potential for selection bias in them. The term “quasi-experiment” was used in only 10% of the articles reporting quasi-experiments; only half of the quasi-experiments measured a pretest score to control for selection bias, and only 8% reported a threat of selection bias. On average, larger effect sizes were seen in randomized than in quasi-experiments, which might be due to selection bias in the quasi-experiments.
Conclusion:
We conclude that quasi-experimentation is useful in many settings in SE, but their design and analysis must be improved (in ways described in this paper), to ensure that inferences made from this kind of experiment are valid.}
}
@article{SAAD2021106688,
title = {UX work in software startups: A thematic analysis of the literature},
journal = {Information and Software Technology},
volume = {140},
pages = {106688},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106688},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001452},
author = {Jullia Saad and Suéllen Martinelli and Leticia S. Machado and Cleidson R.B. {de Souza} and Alexandre Alvaro and Luciana Zaina},
keywords = {Software startup, User experience, Literature review, Thematic analysis, Software development},
abstract = {Context:
Startups are new and fast-growing innovative businesses. These companies also deal with uncertain market conditions and work under constant time and business pressures. Although User Experience (UX) has been widely adopted in the software industry, this has not been a reality in the context of software startups yet. Several factors might influence whether, which, and how UX is adopted by software startups.
Objective:
The objective of this paper is to investigate in the literature how software startups work with UX and to discover the relationship between software development practices and UX in startups.
Methodology:
Our methodology is composed of three main activities: (1) mapping the literature seeking publications on UX work, software engineering, and startups, which resulted in 21 relevant publications; (2) a thematic analysis based on the output of step 1 (i.e., the relevant literature); and (3) refining the themes found out in step 2 and the design of their relationships to explain the link between UX work and software startups.
Results:
The challenges, opportunities, and practices associated with UX in the context of software startups reported by the literature were organized in a set of themes. As a result, seven themes were defined so as to identify needs and opportunities related to UX work in startups. In addition, we synthesize open questions from the literature and suggest new ones to further research directions about the adoption of UX by software startups.
Conclusion:
Our findings demonstrate that software startups require an approach to UX that is more adherent to the startups’ dynamic and disruptive nature. We also suggest emerging open research questions which should be answered to promote the evolution of UX as applied to software startups.}
}
@article{ZAVALA2019161,
title = {Adaptive monitoring: A systematic mapping},
journal = {Information and Software Technology},
volume = {105},
pages = {161-189},
year = {2019},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S0950584918301861},
author = {Edith Zavala and Xavier Franch and Jordi Marco},
keywords = {Adaptive monitoring, Monitoring reconfiguration, Monitor customization, State of the art, Systematic mapping study, Literature review},
abstract = {Context
Adaptive monitoring is a method used in a variety of domains for responding to changing conditions. It has been applied in different ways, from monitoring systems’ customization to re-composition, in different application domains. However, to the best of our knowledge, there are no studies analyzing how adaptive monitoring differs or resembles among the existing approaches.
Objective
To characterize the current state of the art on adaptive monitoring, specifically to: (a) identify the main concepts in the adaptive monitoring topic; (b) determine the demographic characteristics of the studies published in this topic; (c) identify how adaptive monitoring is conducted and evaluated by the different approaches; (d) identify patterns in the approaches supporting adaptive monitoring.
Method
We have conducted a systematic mapping study of adaptive monitoring approaches following recommended practices. We have applied automatic search and snowballing sampling on different sources and used rigorous selection criteria to retrieve the final set of papers. Moreover, we have used an existing qualitative analysis method for extracting relevant data from studies. Finally, we have applied data mining techniques for identifying patterns in the solutions.
Results
We have evaluated 110 studies organized in 81 approaches that support adaptive monitoring. By analyzing them, we have: (1) surveyed related terms and definitions of adaptive monitoring and proposed a generic one; (2) visualized studies’ demographic data and arranged the studies into approaches; (3) characterized the main approaches’ contributions; (4) determined how approaches conduct the adaptation process and evaluate their solutions.
Conclusions
This cross-domain overview of the current state of the art on adaptive monitoring may be a solid and comprehensive baseline for researchers and practitioners in the field. Especially, it may help in identifying opportunities of research; for instance, the need of proposing generic and flexible software engineering solutions for supporting adaptive monitoring in a variety of systems.}
}
@article{BIESIALSKA2021106448,
title = {Big Data analytics in Agile software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {132},
pages = {106448},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106448},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301981},
author = {Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero},
keywords = {Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review},
abstract = {Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.}
}